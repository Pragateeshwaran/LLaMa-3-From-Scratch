{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 128256\n",
    "        self.dim = 4096\n",
    "        self.n_layers = 32\n",
    "        self.n_heads = 32\n",
    "        self.max_seq_len = 2048\n",
    "        self.norm_eps = 1e-6\n",
    "        self.hidden_dim = 14336\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    shape = [1] * ndim\n",
    "    shape[-2] = freqs_cis.shape[0]\n",
    "    shape[-1] = freqs_cis.shape[1]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    \n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "    \n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    \n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    \n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = config.n_heads\n",
    "        self.n_local_heads = config.n_heads\n",
    "        self.n_local_kv_heads = self.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = config.dim // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            mask = torch.full((1, 1, config.max_seq_len, config.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            output = torch.matmul(scores, xv)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "        self.output.weight = self.embed_tokens.weight\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(config.dim // config.n_heads, config.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor = None):\n",
    "        batch_size, seqlen = tokens.shape\n",
    "        h = self.embed_tokens(tokens)\n",
    "        freqs_cos, freqs_sin = self.freqs_cos[:seqlen], self.freqs_sin[:seqlen]\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if targets is not None:\n",
    "            logits = output[:, :-1, :].contiguous()\n",
    "            targets = targets[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return output, loss\n",
    "        return output, None\n",
    "\n",
    "def compute_total_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "config = ModelConfig()\n",
    "model = LlamaModel(config)\n",
    "total_params = compute_total_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8310231040\n",
      "Output shape: torch.Size([2, 10, 128256])\n",
      "Loss: None\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 128256\n",
    "        self.dim = 4096\n",
    "        self.n_layers = 32\n",
    "        self.n_heads = 32\n",
    "        self.max_seq_len = 2048\n",
    "        self.norm_eps = 1e-6\n",
    "        self.hidden_dim = 14336\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cos, freqs_sin):\n",
    "    xq_r, xq_i = xq.float().reshape(*xq.shape[:-1], -1, 2).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(*xk.shape[:-1], -1, 2).unbind(-1)\n",
    "    \n",
    "    # Ensure proper broadcasting\n",
    "    freqs_cos = freqs_cos.view(1, freqs_cos.shape[0], 1, freqs_cos.shape[1])\n",
    "    freqs_sin = freqs_sin.view(1, freqs_sin.shape[0], 1, freqs_sin.shape[1])\n",
    "    \n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    \n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    \n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = config.n_heads\n",
    "        self.n_local_heads = config.n_heads\n",
    "        self.n_local_kv_heads = self.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = config.dim // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            mask = torch.full((1, 1, config.max_seq_len, config.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            output = torch.matmul(scores, xv)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "        self.output.weight = self.embed_tokens.weight\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(config.dim // config.n_heads, config.max_seq_len * 2)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor = None):\n",
    "        batch_size, seqlen = tokens.shape\n",
    "        h = self.embed_tokens(tokens)\n",
    "        freqs_cos = self.freqs_cos[:seqlen]\n",
    "        freqs_sin = self.freqs_sin[:seqlen]\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if targets is not None:\n",
    "            logits = output[:, :-1, :].contiguous()\n",
    "            targets = targets[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return output, loss\n",
    "        return output, None\n",
    "\n",
    "def compute_total_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "# Example usage\n",
    "config = ModelConfig()\n",
    "model = LlamaModel(config)\n",
    "total_params = compute_total_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Test the model\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "output, loss = model(input_ids)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_tokens.weight': Parameter containing:\n",
       " tensor([[-0.0051, -0.0059,  0.0032,  ...,  0.0007,  0.0024,  0.0058],\n",
       "         [-0.0053,  0.0015,  0.0006,  ...,  0.0050,  0.0060, -0.0038],\n",
       "         [ 0.0046,  0.0066,  0.0064,  ..., -0.0055, -0.0004,  0.0023],\n",
       "         ...,\n",
       "         [-0.0059, -0.0008,  0.0061,  ..., -0.0051,  0.0008,  0.0027],\n",
       "         [ 0.0064,  0.0027, -0.0052,  ..., -0.0015,  0.0047,  0.0064],\n",
       "         [ 0.0029, -0.0041,  0.0012,  ..., -0.0066, -0.0060, -0.0047]],\n",
       "        requires_grad=True),\n",
       " 'layers.0.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0011, -0.0015,  0.0013,  ...,  0.0041, -0.0017,  0.0023],\n",
       "         [ 0.0044, -0.0019,  0.0037,  ..., -0.0002,  0.0041,  0.0010],\n",
       "         [-0.0014, -0.0011, -0.0016,  ...,  0.0036,  0.0047,  0.0003],\n",
       "         ...,\n",
       "         [-0.0036,  0.0021, -0.0022,  ..., -0.0007,  0.0015,  0.0020],\n",
       "         [-0.0009, -0.0049,  0.0020,  ..., -0.0012, -0.0027,  0.0050],\n",
       "         [ 0.0003,  0.0015,  0.0002,  ...,  0.0012,  0.0028, -0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.0.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0013, -0.0035,  0.0027,  ..., -0.0041, -0.0009, -0.0012],\n",
       "         [ 0.0021,  0.0005, -0.0018,  ...,  0.0001, -0.0020, -0.0018],\n",
       "         [ 0.0026, -0.0012,  0.0012,  ...,  0.0017, -0.0018,  0.0035],\n",
       "         ...,\n",
       "         [-0.0004,  0.0006,  0.0010,  ..., -0.0016,  0.0001, -0.0015],\n",
       "         [-0.0010,  0.0055, -0.0019,  ..., -0.0018,  0.0004,  0.0016],\n",
       "         [-0.0026, -0.0022,  0.0014,  ..., -0.0010,  0.0004, -0.0027]],\n",
       "        requires_grad=True),\n",
       " 'layers.0.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-0.0056,  0.0031, -0.0070,  ...,  0.0026, -0.0034,  0.0014],\n",
       "         [ 0.0040,  0.0030,  0.0040,  ..., -0.0022, -0.0053, -0.0003],\n",
       "         [-0.0026,  0.0005,  0.0025,  ...,  0.0006, -0.0013,  0.0017],\n",
       "         ...,\n",
       "         [-0.0014,  0.0006, -0.0031,  ...,  0.0027, -0.0014, -0.0035],\n",
       "         [-0.0016, -0.0021,  0.0026,  ...,  0.0014, -0.0014, -0.0015],\n",
       "         [ 0.0018, -0.0002, -0.0032,  ...,  0.0052, -0.0007, -0.0027]],\n",
       "        requires_grad=True),\n",
       " 'layers.0.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-1.8691e-03, -7.3912e-04,  2.3792e-03,  ..., -1.1952e-02,\n",
       "          -4.6803e-04,  9.0669e-04],\n",
       "         [-2.5782e-03, -4.5166e-03,  2.3199e-03,  ...,  2.2130e-03,\n",
       "           2.9736e-04, -8.8166e-04],\n",
       "         [ 2.4604e-03, -2.2427e-03, -1.0590e-03,  ...,  1.3480e-03,\n",
       "          -2.3545e-03, -3.3233e-03],\n",
       "         ...,\n",
       "         [-2.0944e-03, -2.3869e-04, -2.1583e-03,  ..., -1.8504e-04,\n",
       "           5.2853e-04, -1.2396e-03],\n",
       "         [ 1.5507e-03, -3.8390e-04, -2.6935e-03,  ...,  2.7358e-04,\n",
       "          -1.0490e-03,  3.3916e-03],\n",
       "         [ 3.0741e-04, -6.6853e-04, -1.0364e-03,  ..., -2.7206e-03,\n",
       "          -4.6206e-05,  3.0111e-03]], requires_grad=True),\n",
       " 'layers.0.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 1.9588e-03, -4.5682e-04, -1.0502e-03,  ...,  1.1107e-03,\n",
       "           7.1145e-04,  4.8256e-03],\n",
       "         [-4.1350e-03, -2.1622e-03,  1.1688e-03,  ...,  1.2376e-03,\n",
       "          -1.3873e-03, -1.2140e-03],\n",
       "         [ 1.3406e-03, -4.0024e-03,  4.6600e-04,  ..., -1.9592e-03,\n",
       "          -2.4262e-03,  1.0989e-04],\n",
       "         ...,\n",
       "         [ 1.6343e-03, -2.3312e-03, -1.0313e-03,  ...,  7.4458e-04,\n",
       "          -4.3206e-04,  1.3829e-03],\n",
       "         [ 4.7287e-03,  5.9995e-03, -3.1005e-03,  ...,  1.4507e-03,\n",
       "           1.5622e-03,  3.1200e-03],\n",
       "         [-1.4986e-03,  2.6247e-03, -4.4377e-03,  ..., -6.7491e-05,\n",
       "           8.3920e-04, -3.5032e-03]], requires_grad=True),\n",
       " 'layers.0.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 1.4651e-03,  1.9267e-03,  3.0506e-03,  ...,  2.4136e-03,\n",
       "          -2.9030e-03, -2.4064e-03],\n",
       "         [-1.2832e-03, -1.6283e-03, -2.5782e-03,  ..., -2.9279e-03,\n",
       "          -2.2717e-04, -4.1604e-05],\n",
       "         [-1.2825e-03, -1.1773e-03, -3.3924e-03,  ...,  5.6918e-04,\n",
       "           3.5084e-03, -3.4001e-03],\n",
       "         ...,\n",
       "         [ 2.6307e-03,  4.7914e-04,  2.8029e-03,  ...,  4.2704e-03,\n",
       "           1.4377e-03,  3.5272e-03],\n",
       "         [-3.3315e-03, -4.9491e-03,  1.3739e-03,  ..., -3.8408e-03,\n",
       "           1.1745e-03, -4.1902e-03],\n",
       "         [ 1.7024e-03,  2.5974e-03,  9.0937e-04,  ..., -1.5322e-03,\n",
       "          -4.4884e-03, -2.1442e-03]], requires_grad=True),\n",
       " 'layers.0.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-8.6139e-04, -1.9440e-03,  4.3608e-03,  ..., -2.0028e-03,\n",
       "           2.0641e-03,  6.9926e-04],\n",
       "         [-3.2016e-03,  4.7644e-03,  1.2162e-03,  ...,  3.6439e-03,\n",
       "          -2.9518e-03, -3.3541e-03],\n",
       "         [ 9.0976e-04,  3.7123e-04,  2.2264e-03,  ...,  5.0616e-03,\n",
       "           1.3254e-03, -3.0991e-03],\n",
       "         ...,\n",
       "         [ 5.6933e-04,  2.0457e-03, -8.2290e-04,  ...,  1.7847e-03,\n",
       "          -1.0936e-03,  2.0619e-03],\n",
       "         [ 4.5541e-03, -1.3520e-03,  8.9202e-04,  ..., -5.2539e-03,\n",
       "           6.6087e-05, -1.7960e-03],\n",
       "         [-3.0906e-04, -2.9389e-03,  4.3965e-04,  ...,  3.5124e-03,\n",
       "          -8.1238e-04,  7.2673e-04]], requires_grad=True),\n",
       " 'layers.0.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.0.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.1.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0018,  0.0007, -0.0010,  ..., -0.0021, -0.0007,  0.0014],\n",
       "         [ 0.0005, -0.0010,  0.0005,  ..., -0.0047,  0.0036,  0.0004],\n",
       "         [ 0.0026,  0.0044, -0.0006,  ...,  0.0025, -0.0016, -0.0027],\n",
       "         ...,\n",
       "         [-0.0004,  0.0003, -0.0043,  ..., -0.0005, -0.0034, -0.0031],\n",
       "         [-0.0029,  0.0058, -0.0052,  ...,  0.0015, -0.0040,  0.0002],\n",
       "         [-0.0013,  0.0007, -0.0038,  ...,  0.0037, -0.0036,  0.0015]],\n",
       "        requires_grad=True),\n",
       " 'layers.1.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-4.2561e-03,  4.7014e-04, -3.6581e-03,  ..., -2.1718e-03,\n",
       "          -6.1389e-03, -1.3432e-03],\n",
       "         [-2.2565e-03, -1.6186e-03,  1.1473e-03,  ...,  5.2990e-04,\n",
       "           4.9844e-03, -1.7081e-03],\n",
       "         [ 1.6327e-03,  1.1842e-03,  2.7981e-04,  ..., -1.6460e-03,\n",
       "           2.1999e-03,  7.7307e-05],\n",
       "         ...,\n",
       "         [-8.9165e-04,  1.4438e-03, -1.0930e-03,  ...,  5.5769e-04,\n",
       "          -4.1452e-04, -3.3234e-03],\n",
       "         [ 6.6381e-04,  1.6362e-03,  1.6658e-03,  ..., -2.3233e-03,\n",
       "          -2.9698e-04,  5.0071e-04],\n",
       "         [-9.1188e-04, -1.6852e-03,  1.0020e-03,  ...,  5.9996e-04,\n",
       "          -1.6983e-04,  2.1293e-04]], requires_grad=True),\n",
       " 'layers.1.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-1.1447e-03,  1.9439e-03,  9.2946e-04,  ..., -2.7686e-03,\n",
       "          -3.7842e-03, -3.4193e-03],\n",
       "         [ 9.6923e-04,  1.9897e-03, -3.5088e-03,  ...,  8.1737e-04,\n",
       "          -2.1802e-03,  1.8404e-04],\n",
       "         [ 4.4534e-03,  2.4193e-03, -1.4066e-03,  ..., -4.6614e-03,\n",
       "           1.3342e-03, -7.6504e-05],\n",
       "         ...,\n",
       "         [-2.4523e-03, -1.8555e-03,  7.1841e-04,  ..., -2.1027e-03,\n",
       "          -4.5026e-04,  2.0823e-03],\n",
       "         [-2.0674e-03, -5.0586e-04,  2.8104e-04,  ..., -3.1240e-03,\n",
       "          -1.3811e-03, -6.1023e-03],\n",
       "         [ 9.5734e-04, -1.7311e-03,  3.5809e-04,  ...,  2.4121e-03,\n",
       "           1.1033e-03,  3.1696e-03]], requires_grad=True),\n",
       " 'layers.1.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0032, -0.0036, -0.0023,  ..., -0.0002,  0.0032, -0.0014],\n",
       "         [-0.0005,  0.0065,  0.0024,  ...,  0.0008,  0.0053,  0.0029],\n",
       "         [ 0.0054,  0.0003, -0.0011,  ..., -0.0018, -0.0061, -0.0034],\n",
       "         ...,\n",
       "         [-0.0018,  0.0009, -0.0031,  ..., -0.0014,  0.0032, -0.0045],\n",
       "         [ 0.0020,  0.0020,  0.0028,  ..., -0.0017,  0.0003, -0.0042],\n",
       "         [-0.0037,  0.0001, -0.0020,  ..., -0.0005,  0.0012,  0.0030]],\n",
       "        requires_grad=True),\n",
       " 'layers.1.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 9.0795e-04,  5.5047e-04,  1.9312e-03,  ..., -2.3191e-03,\n",
       "          -4.7248e-03,  2.5904e-03],\n",
       "         [ 2.7607e-03, -3.0283e-03, -3.2678e-03,  ..., -3.0303e-03,\n",
       "           1.2190e-03, -1.2599e-03],\n",
       "         [-2.5104e-03,  2.2530e-03,  9.0602e-04,  ...,  1.1918e-03,\n",
       "          -3.7055e-04,  1.0760e-03],\n",
       "         ...,\n",
       "         [-4.1473e-03,  4.1804e-03, -9.7905e-04,  ..., -1.9006e-03,\n",
       "          -7.9093e-04,  2.9001e-03],\n",
       "         [-1.5390e-03, -4.3731e-04,  8.3772e-04,  ...,  9.7078e-04,\n",
       "          -3.1712e-05,  1.7480e-03],\n",
       "         [-7.3296e-05, -6.1694e-04,  2.0252e-06,  ...,  5.1195e-03,\n",
       "           1.9742e-03, -1.0121e-03]], requires_grad=True),\n",
       " 'layers.1.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-9.7183e-04, -2.9602e-03, -1.2318e-03,  ...,  4.0768e-04,\n",
       "           1.3573e-03,  1.4131e-03],\n",
       "         [ 3.6778e-04, -2.9157e-03, -2.6729e-03,  ..., -3.1190e-03,\n",
       "          -2.6459e-03, -2.9663e-03],\n",
       "         [-2.2047e-03, -2.6434e-04, -4.4844e-05,  ...,  1.3943e-03,\n",
       "          -1.8959e-04,  2.1797e-03],\n",
       "         ...,\n",
       "         [ 1.6492e-03,  2.8585e-03, -2.2200e-04,  ..., -2.3642e-03,\n",
       "           1.6391e-03, -2.2864e-03],\n",
       "         [ 1.8942e-03, -1.3782e-03,  2.4216e-03,  ..., -2.1153e-03,\n",
       "           6.9051e-04, -4.3699e-03],\n",
       "         [ 1.5521e-03,  1.0038e-03,  1.8475e-03,  ...,  1.4205e-03,\n",
       "          -3.5909e-03, -7.2583e-04]], requires_grad=True),\n",
       " 'layers.1.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 3.4006e-03, -1.6677e-03, -1.9100e-03,  ...,  2.6618e-03,\n",
       "           3.5421e-03, -2.8731e-03],\n",
       "         [-2.0394e-03,  2.0455e-03,  1.2096e-03,  ..., -3.5270e-03,\n",
       "           1.6783e-03,  4.0579e-04],\n",
       "         [-3.3110e-03,  1.7939e-03, -1.0706e-05,  ...,  4.1489e-03,\n",
       "           2.4766e-03, -3.7302e-03],\n",
       "         ...,\n",
       "         [-1.0618e-03,  3.1112e-03, -3.7325e-03,  ..., -3.9503e-03,\n",
       "          -4.7792e-03,  5.8149e-03],\n",
       "         [-4.4089e-03,  5.6086e-04, -1.8089e-03,  ...,  3.2011e-03,\n",
       "           1.2015e-03,  2.0509e-03],\n",
       "         [ 9.3939e-05,  2.2481e-03, -5.0129e-03,  ..., -1.3242e-03,\n",
       "          -1.2038e-03, -2.8701e-03]], requires_grad=True),\n",
       " 'layers.1.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.1.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.2.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2299e-03,  1.7672e-03,  2.0154e-03,  ..., -9.6866e-04,\n",
       "          -7.9335e-04, -1.4884e-03],\n",
       "         [-3.1341e-03,  1.4778e-03,  2.6034e-03,  ...,  1.0712e-04,\n",
       "          -2.2234e-03, -2.6802e-04],\n",
       "         [ 1.0024e-04,  6.4284e-04, -3.6432e-04,  ...,  4.7089e-03,\n",
       "           4.0075e-03,  7.2726e-04],\n",
       "         ...,\n",
       "         [-4.7027e-03, -1.3948e-05,  2.8447e-03,  ...,  4.0048e-03,\n",
       "           2.3278e-03, -4.2091e-03],\n",
       "         [-1.1456e-03,  2.0603e-03,  3.9199e-03,  ...,  2.6816e-04,\n",
       "          -2.4680e-03, -7.0808e-04],\n",
       "         [-6.2943e-05, -1.6529e-03, -1.0874e-03,  ..., -2.7011e-03,\n",
       "          -8.6224e-04, -5.2401e-03]], requires_grad=True),\n",
       " 'layers.2.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0025,  0.0031, -0.0057,  ...,  0.0014, -0.0007, -0.0026],\n",
       "         [-0.0029,  0.0047,  0.0012,  ...,  0.0024,  0.0014, -0.0027],\n",
       "         [-0.0023, -0.0010,  0.0006,  ...,  0.0026,  0.0013,  0.0007],\n",
       "         ...,\n",
       "         [-0.0024, -0.0045, -0.0021,  ...,  0.0005, -0.0008,  0.0030],\n",
       "         [ 0.0011,  0.0024,  0.0009,  ..., -0.0029, -0.0024,  0.0047],\n",
       "         [-0.0033,  0.0017, -0.0030,  ..., -0.0011,  0.0003, -0.0032]],\n",
       "        requires_grad=True),\n",
       " 'layers.2.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 5.6540e-03,  1.4986e-03, -2.9606e-03,  ...,  3.7597e-03,\n",
       "          -5.1482e-04, -2.3776e-03],\n",
       "         [-3.0364e-03, -7.3743e-04, -8.7053e-04,  ..., -1.9463e-03,\n",
       "           4.4507e-07, -1.4171e-03],\n",
       "         [ 5.3329e-03, -4.8274e-04, -9.6719e-04,  ...,  5.3363e-04,\n",
       "           1.5135e-03, -3.3220e-04],\n",
       "         ...,\n",
       "         [-3.4376e-04,  4.5208e-03, -1.1956e-03,  ...,  3.0792e-04,\n",
       "           8.8230e-04, -1.3960e-04],\n",
       "         [-3.6021e-04, -1.8877e-03,  3.0884e-03,  ..., -3.2369e-03,\n",
       "           1.2476e-03, -5.7005e-04],\n",
       "         [ 3.2599e-03,  3.8153e-03, -1.3016e-03,  ...,  9.2870e-04,\n",
       "          -1.0906e-03,  1.1603e-03]], requires_grad=True),\n",
       " 'layers.2.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 2.8558e-03,  1.9609e-03, -8.8794e-05,  ..., -1.7811e-03,\n",
       "          -6.8517e-04,  3.6228e-03],\n",
       "         [-2.1519e-03,  1.1975e-03,  1.2106e-03,  ..., -2.5678e-03,\n",
       "           3.1640e-04, -5.6818e-04],\n",
       "         [-3.4617e-03,  4.1681e-03,  3.8634e-03,  ...,  4.2741e-03,\n",
       "          -2.0040e-04, -1.8945e-03],\n",
       "         ...,\n",
       "         [ 7.7103e-03,  2.3368e-03, -2.6346e-03,  ..., -3.0564e-03,\n",
       "          -9.4137e-04,  2.0889e-03],\n",
       "         [ 1.5993e-03, -7.1278e-04, -2.9561e-03,  ..., -2.3751e-03,\n",
       "          -2.0602e-03, -3.2273e-03],\n",
       "         [-6.1863e-04, -2.2164e-03,  9.6068e-05,  ...,  3.6769e-03,\n",
       "           1.0160e-03, -1.5719e-03]], requires_grad=True),\n",
       " 'layers.2.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-2.4000e-03, -9.7249e-04, -3.6631e-03,  ...,  6.5227e-03,\n",
       "          -8.2703e-04, -1.6283e-05],\n",
       "         [ 1.7727e-03, -4.7510e-03,  1.6608e-03,  ..., -1.2515e-03,\n",
       "           4.6412e-04, -4.2090e-03],\n",
       "         [ 2.8376e-03,  2.0732e-03, -2.3790e-04,  ..., -1.4641e-03,\n",
       "           3.0617e-03,  4.0174e-03],\n",
       "         ...,\n",
       "         [-2.0115e-03, -1.1637e-03, -8.0598e-06,  ..., -2.1418e-03,\n",
       "          -2.6221e-03,  9.1058e-04],\n",
       "         [ 2.8120e-03,  1.4756e-03,  3.1599e-03,  ...,  1.4969e-03,\n",
       "          -3.1911e-05,  1.9843e-03],\n",
       "         [-4.5199e-03,  3.1998e-03,  2.5051e-04,  ..., -1.4030e-04,\n",
       "          -2.0424e-03, -3.4488e-03]], requires_grad=True),\n",
       " 'layers.2.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0026, -0.0006, -0.0032,  ...,  0.0056,  0.0005,  0.0001],\n",
       "         [ 0.0057, -0.0004, -0.0008,  ...,  0.0012, -0.0005, -0.0011],\n",
       "         [ 0.0029, -0.0023,  0.0005,  ...,  0.0036,  0.0026,  0.0011],\n",
       "         ...,\n",
       "         [ 0.0015, -0.0009,  0.0004,  ..., -0.0005, -0.0024, -0.0010],\n",
       "         [-0.0008, -0.0009, -0.0035,  ..., -0.0018, -0.0038, -0.0020],\n",
       "         [-0.0025,  0.0015, -0.0027,  ...,  0.0010, -0.0019,  0.0012]],\n",
       "        requires_grad=True),\n",
       " 'layers.2.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.6790e-03, -1.7254e-03,  3.3520e-03,  ..., -1.1930e-03,\n",
       "           8.5260e-05,  9.9928e-04],\n",
       "         [-5.6564e-03,  6.8122e-04,  2.7816e-03,  ..., -8.9559e-04,\n",
       "          -7.9620e-04, -4.7093e-03],\n",
       "         [-1.2150e-03, -1.9595e-03,  4.2487e-03,  ..., -1.6269e-03,\n",
       "          -1.2277e-03, -8.3534e-04],\n",
       "         ...,\n",
       "         [-1.0064e-03,  2.0394e-03,  2.0247e-03,  ...,  1.2440e-03,\n",
       "          -7.3695e-04, -3.2965e-03],\n",
       "         [-6.0996e-03,  1.9502e-03, -2.0465e-03,  ..., -3.4124e-03,\n",
       "          -4.0133e-04, -4.7649e-04],\n",
       "         [ 2.2964e-03,  1.6356e-03,  9.2674e-06,  ..., -3.0313e-03,\n",
       "          -1.7738e-03, -2.0572e-03]], requires_grad=True),\n",
       " 'layers.2.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.2.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.3.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-1.0471e-03,  1.9711e-03, -2.5593e-03,  ..., -1.3737e-03,\n",
       "          -7.2347e-04,  1.7726e-03],\n",
       "         [ 2.2132e-03, -8.6778e-03,  9.8753e-04,  ..., -4.0773e-04,\n",
       "          -1.7751e-04,  4.5866e-04],\n",
       "         [ 1.4913e-03, -1.7439e-04, -2.6659e-03,  ..., -2.8565e-03,\n",
       "          -2.6997e-03, -1.6537e-04],\n",
       "         ...,\n",
       "         [ 3.1553e-03, -3.5917e-03, -3.0736e-03,  ...,  3.2024e-03,\n",
       "           1.0583e-03,  1.7592e-03],\n",
       "         [-1.9401e-04, -1.1828e-04,  2.3722e-03,  ..., -2.4287e-04,\n",
       "           2.3449e-03, -1.0697e-05],\n",
       "         [ 2.4859e-04,  2.7963e-03,  1.0416e-03,  ..., -4.4056e-03,\n",
       "          -1.1929e-03,  4.0409e-05]], requires_grad=True),\n",
       " 'layers.3.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0033, -0.0031,  0.0002,  ...,  0.0005, -0.0027,  0.0002],\n",
       "         [ 0.0004, -0.0014, -0.0002,  ...,  0.0050,  0.0008, -0.0020],\n",
       "         [ 0.0010,  0.0030,  0.0021,  ..., -0.0024,  0.0003, -0.0010],\n",
       "         ...,\n",
       "         [-0.0034, -0.0006,  0.0006,  ...,  0.0026, -0.0043,  0.0037],\n",
       "         [ 0.0004,  0.0004, -0.0020,  ..., -0.0010, -0.0001,  0.0032],\n",
       "         [ 0.0029,  0.0022,  0.0019,  ...,  0.0027,  0.0031, -0.0047]],\n",
       "        requires_grad=True),\n",
       " 'layers.3.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-2.3241e-03, -6.1991e-03,  1.2168e-03,  ...,  1.4620e-03,\n",
       "          -1.1554e-03,  6.7144e-04],\n",
       "         [ 1.8402e-03,  2.5788e-05,  3.1464e-03,  ..., -1.9416e-03,\n",
       "           9.0783e-04,  3.0488e-04],\n",
       "         [-4.9407e-04,  1.1964e-03,  1.0217e-03,  ..., -1.4583e-03,\n",
       "          -4.4416e-04, -8.3773e-04],\n",
       "         ...,\n",
       "         [-9.6225e-04, -4.2592e-03,  1.3474e-03,  ..., -1.0340e-03,\n",
       "           3.5932e-03,  2.3793e-03],\n",
       "         [-1.7889e-03,  5.0833e-03, -1.0344e-03,  ..., -1.8634e-03,\n",
       "           7.0278e-04, -2.8322e-03],\n",
       "         [-1.9874e-03, -3.8790e-03,  4.1195e-04,  ..., -3.2232e-03,\n",
       "           7.2286e-04, -3.4035e-03]], requires_grad=True),\n",
       " 'layers.3.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 2.3589e-03,  2.5634e-03, -2.8068e-03,  ..., -2.5431e-03,\n",
       "           4.9438e-03,  6.3299e-04],\n",
       "         [ 8.0830e-04,  2.3614e-03, -1.4004e-03,  ...,  1.9087e-03,\n",
       "          -3.0188e-03,  2.4478e-03],\n",
       "         [ 4.7617e-04,  1.6620e-03,  4.4618e-04,  ...,  4.4989e-03,\n",
       "          -2.5679e-03, -2.8279e-04],\n",
       "         ...,\n",
       "         [-3.8319e-03,  9.9912e-05, -2.5711e-03,  ...,  4.3069e-03,\n",
       "          -1.7638e-03,  1.3213e-04],\n",
       "         [-2.0588e-03,  3.8408e-03, -3.4831e-04,  ...,  1.7482e-03,\n",
       "          -2.0624e-05, -6.0532e-04],\n",
       "         [-8.7434e-04, -1.8869e-03, -2.7033e-03,  ...,  8.3460e-04,\n",
       "           3.5737e-04,  2.9144e-03]], requires_grad=True),\n",
       " 'layers.3.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-1.6884e-03,  2.5365e-03, -1.2871e-03,  ..., -3.7297e-03,\n",
       "           1.9295e-03,  3.1275e-03],\n",
       "         [ 4.1805e-03, -5.4530e-04,  2.7356e-03,  ...,  2.3774e-03,\n",
       "           1.6151e-04,  9.6711e-05],\n",
       "         [-6.6350e-04, -5.0275e-03,  1.5405e-03,  ...,  1.8032e-03,\n",
       "          -2.2533e-03,  9.6382e-04],\n",
       "         ...,\n",
       "         [-3.4453e-03, -1.7179e-03, -3.5354e-03,  ...,  1.9812e-04,\n",
       "          -3.2596e-03,  3.2229e-03],\n",
       "         [-4.5809e-04, -1.4374e-03,  3.9962e-03,  ...,  3.3350e-03,\n",
       "          -4.8108e-04,  1.6146e-04],\n",
       "         [-1.6475e-05, -1.8600e-03, -2.1229e-03,  ..., -2.5749e-03,\n",
       "           1.4677e-03, -3.4452e-03]], requires_grad=True),\n",
       " 'layers.3.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0022, -0.0027,  0.0036,  ..., -0.0028,  0.0041, -0.0011],\n",
       "         [ 0.0011,  0.0002,  0.0007,  ...,  0.0010, -0.0015, -0.0005],\n",
       "         [-0.0009, -0.0022, -0.0001,  ..., -0.0022,  0.0035,  0.0046],\n",
       "         ...,\n",
       "         [-0.0026,  0.0096, -0.0002,  ..., -0.0011, -0.0048, -0.0015],\n",
       "         [ 0.0020, -0.0020,  0.0021,  ...,  0.0009, -0.0024, -0.0045],\n",
       "         [-0.0027,  0.0023, -0.0009,  ...,  0.0041, -0.0005, -0.0018]],\n",
       "        requires_grad=True),\n",
       " 'layers.3.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0002,  0.0016,  0.0006,  ...,  0.0013,  0.0008,  0.0007],\n",
       "         [-0.0037, -0.0021, -0.0002,  ..., -0.0008,  0.0014,  0.0017],\n",
       "         [-0.0016,  0.0005, -0.0022,  ...,  0.0021,  0.0015,  0.0021],\n",
       "         ...,\n",
       "         [ 0.0022, -0.0010, -0.0017,  ...,  0.0022,  0.0026,  0.0039],\n",
       "         [ 0.0007, -0.0012,  0.0023,  ...,  0.0032, -0.0079, -0.0047],\n",
       "         [ 0.0039, -0.0022,  0.0007,  ...,  0.0023,  0.0041, -0.0014]],\n",
       "        requires_grad=True),\n",
       " 'layers.3.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.3.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.4.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 4.1506e-03,  2.2716e-03,  2.2880e-04,  ..., -2.5156e-04,\n",
       "           5.3128e-03,  3.5557e-04],\n",
       "         [ 5.0856e-03, -6.5910e-04,  1.3836e-03,  ..., -1.1703e-04,\n",
       "          -1.4820e-03, -1.6365e-03],\n",
       "         [ 1.7784e-04, -1.2824e-03,  9.7829e-04,  ..., -2.7190e-03,\n",
       "           2.3143e-04,  8.8083e-05],\n",
       "         ...,\n",
       "         [-4.9223e-03,  2.5920e-03,  7.5964e-04,  ..., -1.5676e-03,\n",
       "          -2.9297e-03, -4.0823e-04],\n",
       "         [ 4.1316e-03, -4.5463e-03,  4.8564e-04,  ...,  4.4868e-03,\n",
       "          -8.0612e-04, -4.7674e-04],\n",
       "         [-1.7354e-03, -3.5278e-03,  5.6566e-03,  ..., -6.5656e-04,\n",
       "           2.3891e-03,  3.9180e-03]], requires_grad=True),\n",
       " 'layers.4.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2514e-04,  3.1477e-03, -1.2261e-03,  ...,  9.5550e-05,\n",
       "          -3.4940e-03,  6.5477e-04],\n",
       "         [ 2.4631e-03,  1.7190e-03, -5.4513e-04,  ...,  5.1189e-03,\n",
       "          -5.2091e-03,  1.8778e-03],\n",
       "         [ 1.2401e-03, -7.6468e-04, -3.0473e-03,  ..., -4.8691e-03,\n",
       "          -2.5999e-03,  6.8217e-03],\n",
       "         ...,\n",
       "         [-3.7096e-03,  8.1314e-04, -2.2929e-03,  ..., -1.0079e-03,\n",
       "           1.8476e-03,  1.2200e-03],\n",
       "         [-1.6261e-03, -3.8090e-03,  1.4493e-04,  ..., -3.4421e-04,\n",
       "           1.7866e-03,  1.2386e-03],\n",
       "         [-7.9271e-04,  2.2865e-03, -1.2278e-03,  ..., -6.9828e-04,\n",
       "           1.2164e-03, -1.2512e-03]], requires_grad=True),\n",
       " 'layers.4.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-9.8871e-04,  2.2273e-03,  1.3020e-03,  ...,  5.2173e-04,\n",
       "          -1.3183e-03, -8.8267e-04],\n",
       "         [ 4.8955e-03, -2.0629e-04,  9.2334e-04,  ..., -2.9135e-04,\n",
       "          -2.1275e-03, -1.4800e-03],\n",
       "         [-2.4012e-03, -1.3908e-03,  2.2473e-03,  ...,  4.6807e-03,\n",
       "           1.1300e-03,  1.9472e-03],\n",
       "         ...,\n",
       "         [-3.3657e-04,  2.3576e-03, -2.0128e-03,  ..., -2.8973e-03,\n",
       "           3.1112e-03,  1.8766e-03],\n",
       "         [-1.2749e-03,  2.0400e-03, -3.5017e-03,  ..., -1.2185e-03,\n",
       "           9.9686e-06, -1.1884e-04],\n",
       "         [ 3.6378e-03,  3.6512e-04, -3.7022e-03,  ..., -3.9236e-03,\n",
       "           9.2096e-04, -2.3018e-03]], requires_grad=True),\n",
       " 'layers.4.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 1.1274e-03,  3.0643e-03,  7.4226e-04,  ...,  1.7629e-03,\n",
       "           1.4127e-03, -4.0488e-03],\n",
       "         [ 2.5069e-03, -5.9511e-04, -7.8652e-03,  ...,  6.4449e-04,\n",
       "           3.2005e-03,  1.4447e-03],\n",
       "         [ 1.0034e-03,  4.5163e-03,  5.3448e-04,  ..., -1.2810e-03,\n",
       "           1.0258e-03, -2.3970e-03],\n",
       "         ...,\n",
       "         [ 2.9339e-03,  2.4272e-03,  3.0425e-03,  ..., -4.6135e-03,\n",
       "           4.8587e-03,  2.5683e-03],\n",
       "         [-2.6585e-03,  1.2840e-03, -3.7632e-04,  ..., -2.0351e-03,\n",
       "           3.1364e-04,  9.1614e-04],\n",
       "         [ 1.7841e-03, -4.7705e-03,  4.5468e-05,  ..., -2.4562e-03,\n",
       "          -1.8616e-03, -2.9609e-03]], requires_grad=True),\n",
       " 'layers.4.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 3.1106e-03, -3.1663e-03,  1.6825e-03,  ..., -1.8544e-03,\n",
       "          -6.0497e-04, -4.3298e-03],\n",
       "         [-1.6076e-03, -2.0454e-03,  1.3685e-04,  ..., -9.9928e-04,\n",
       "          -9.6869e-04,  4.5455e-04],\n",
       "         [-2.6665e-03,  1.6347e-03,  1.1748e-04,  ...,  4.3791e-04,\n",
       "          -3.9936e-04,  1.1642e-03],\n",
       "         ...,\n",
       "         [-6.3869e-04, -4.0105e-03,  3.1263e-03,  ...,  6.4742e-04,\n",
       "          -7.4501e-04,  8.3239e-04],\n",
       "         [ 5.7219e-04, -1.0792e-03, -2.2576e-03,  ..., -2.0291e-03,\n",
       "           2.0479e-03,  9.7318e-05],\n",
       "         [-3.8516e-03,  7.9572e-04,  1.6283e-03,  ..., -1.1054e-03,\n",
       "           1.7579e-03,  2.4512e-03]], requires_grad=True),\n",
       " 'layers.4.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-1.2287e-03,  7.8548e-05,  3.0533e-04,  ..., -1.7990e-03,\n",
       "          -7.4184e-04, -1.1607e-03],\n",
       "         [-7.2732e-04,  4.2065e-03, -1.4333e-03,  ...,  3.4671e-03,\n",
       "          -1.0996e-03, -1.2181e-03],\n",
       "         [ 4.8109e-04, -3.2925e-03, -3.8336e-03,  ..., -9.4127e-04,\n",
       "          -1.2076e-03,  1.7081e-04],\n",
       "         ...,\n",
       "         [-1.7080e-03,  1.3842e-03,  4.3423e-04,  ...,  6.9506e-04,\n",
       "          -8.3059e-04,  1.2227e-03],\n",
       "         [ 1.2807e-03,  1.7254e-03, -1.7428e-04,  ..., -2.1717e-03,\n",
       "           6.1819e-06,  1.2136e-03],\n",
       "         [ 3.7819e-03,  3.8029e-04, -2.1344e-03,  ...,  7.4472e-04,\n",
       "          -9.9708e-04,  6.2335e-03]], requires_grad=True),\n",
       " 'layers.4.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0012,  0.0004,  0.0012,  ..., -0.0024, -0.0006,  0.0015],\n",
       "         [ 0.0003, -0.0029, -0.0030,  ..., -0.0005,  0.0004,  0.0021],\n",
       "         [-0.0052,  0.0065, -0.0022,  ...,  0.0010,  0.0034, -0.0028],\n",
       "         ...,\n",
       "         [-0.0035,  0.0018, -0.0015,  ..., -0.0021,  0.0013, -0.0011],\n",
       "         [-0.0033,  0.0016,  0.0010,  ..., -0.0036,  0.0032,  0.0009],\n",
       "         [ 0.0010,  0.0025,  0.0030,  ...,  0.0008,  0.0003, -0.0023]],\n",
       "        requires_grad=True),\n",
       " 'layers.4.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.4.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.5.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 1.0168e-03, -4.7120e-03, -5.1577e-03,  ..., -1.1882e-03,\n",
       "          -4.0729e-03, -2.3501e-03],\n",
       "         [-6.4558e-04, -4.2560e-03, -1.1429e-03,  ...,  5.7725e-03,\n",
       "           3.4449e-03,  1.3455e-03],\n",
       "         [-5.1739e-04,  4.1084e-03,  1.2365e-03,  ..., -8.2954e-04,\n",
       "           2.4945e-03,  1.5832e-03],\n",
       "         ...,\n",
       "         [ 5.5732e-03, -1.3736e-05, -4.3332e-03,  ...,  1.3122e-03,\n",
       "           3.1354e-04, -7.0599e-03],\n",
       "         [-2.5182e-03, -1.5127e-03,  5.4639e-05,  ..., -1.6954e-03,\n",
       "          -1.0997e-03, -2.3836e-03],\n",
       "         [-3.3912e-03,  5.0319e-03, -2.8036e-03,  ...,  1.3189e-03,\n",
       "          -4.0400e-03, -1.4555e-03]], requires_grad=True),\n",
       " 'layers.5.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-1.5045e-03,  8.4740e-05, -3.0826e-04,  ...,  3.7242e-03,\n",
       "           1.8454e-03,  9.8754e-04],\n",
       "         [-5.9860e-04, -3.9116e-03, -4.1993e-03,  ...,  2.7406e-03,\n",
       "          -4.1986e-03, -3.2402e-03],\n",
       "         [ 1.0330e-03,  2.1595e-03,  1.9207e-03,  ..., -2.7988e-03,\n",
       "           2.3317e-04,  1.6397e-03],\n",
       "         ...,\n",
       "         [-2.1445e-03,  7.1433e-05, -2.6655e-04,  ..., -7.9848e-04,\n",
       "           4.5675e-04,  2.6497e-03],\n",
       "         [ 7.2534e-04, -9.1882e-05, -3.2661e-03,  ...,  1.9418e-03,\n",
       "           2.1559e-03,  6.2664e-03],\n",
       "         [ 2.3913e-03, -1.9990e-03,  2.1314e-03,  ..., -3.7794e-04,\n",
       "          -8.3364e-04,  1.2251e-03]], requires_grad=True),\n",
       " 'layers.5.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-1.0442e-03,  1.7458e-03, -2.3348e-03,  ...,  4.3215e-07,\n",
       "          -9.0642e-04, -2.1336e-03],\n",
       "         [-1.7351e-03,  2.1044e-03, -2.8562e-03,  ..., -1.4453e-04,\n",
       "           2.2207e-03,  1.0160e-03],\n",
       "         [-1.7808e-03, -1.5841e-03, -7.1770e-04,  ...,  5.3332e-03,\n",
       "           1.4410e-03, -2.8780e-03],\n",
       "         ...,\n",
       "         [-1.0252e-03,  8.5206e-04, -1.6914e-03,  ..., -1.2621e-03,\n",
       "           5.3773e-04,  5.9532e-04],\n",
       "         [-1.9143e-03, -1.1642e-03,  6.7078e-04,  ..., -3.3164e-03,\n",
       "          -3.7540e-04,  1.4982e-03],\n",
       "         [-2.7119e-03,  2.2059e-04,  3.3933e-04,  ...,  9.2534e-04,\n",
       "          -1.1677e-03, -9.3559e-04]], requires_grad=True),\n",
       " 'layers.5.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 2.5047e-03,  3.6883e-04,  1.2229e-03,  ..., -1.3218e-05,\n",
       "           2.4345e-04,  8.0013e-04],\n",
       "         [-2.7282e-03, -4.4440e-03, -4.6180e-03,  ...,  2.1562e-03,\n",
       "          -4.6304e-03,  2.4485e-03],\n",
       "         [ 5.3007e-04,  2.7579e-03,  2.5264e-03,  ...,  1.6165e-04,\n",
       "           3.1577e-04,  1.0479e-03],\n",
       "         ...,\n",
       "         [ 2.1122e-03, -9.7958e-04,  1.0676e-03,  ...,  1.0863e-03,\n",
       "           1.1032e-03, -2.5398e-03],\n",
       "         [-6.7142e-03, -4.5996e-05, -4.5002e-03,  ..., -2.5215e-03,\n",
       "          -3.3973e-03,  1.7911e-03],\n",
       "         [ 1.1648e-03,  3.7605e-03, -6.1845e-04,  ..., -1.6460e-03,\n",
       "          -2.1464e-03,  1.2860e-03]], requires_grad=True),\n",
       " 'layers.5.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-9.0398e-05, -9.8853e-04,  1.1119e-03,  ..., -3.1879e-03,\n",
       "          -2.1946e-05, -2.7958e-04],\n",
       "         [ 1.3780e-03, -3.0792e-03,  2.0026e-03,  ...,  2.0753e-04,\n",
       "          -3.8220e-03,  6.4126e-03],\n",
       "         [ 2.3417e-03, -4.4444e-03,  3.0815e-04,  ..., -4.0760e-03,\n",
       "          -2.4217e-04,  1.7546e-03],\n",
       "         ...,\n",
       "         [ 3.6701e-03,  4.1607e-03, -1.5039e-03,  ...,  1.5805e-03,\n",
       "           1.7474e-03,  3.2949e-03],\n",
       "         [ 1.0633e-03,  5.2668e-03, -3.9138e-03,  ...,  5.0842e-03,\n",
       "          -5.2153e-04, -2.1231e-03],\n",
       "         [ 3.6841e-03,  3.0210e-03,  4.3386e-04,  ...,  2.8808e-03,\n",
       "           1.0910e-05,  1.0810e-03]], requires_grad=True),\n",
       " 'layers.5.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0031, -0.0028,  0.0018,  ...,  0.0005,  0.0009, -0.0012],\n",
       "         [-0.0011, -0.0004, -0.0018,  ...,  0.0016, -0.0001,  0.0001],\n",
       "         [ 0.0015,  0.0018,  0.0023,  ..., -0.0005, -0.0007,  0.0014],\n",
       "         ...,\n",
       "         [ 0.0031,  0.0026, -0.0001,  ..., -0.0014, -0.0002,  0.0007],\n",
       "         [ 0.0009, -0.0031,  0.0044,  ..., -0.0027,  0.0019, -0.0016],\n",
       "         [ 0.0031,  0.0024,  0.0024,  ..., -0.0023,  0.0008, -0.0021]],\n",
       "        requires_grad=True),\n",
       " 'layers.5.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0015, -0.0056,  0.0057,  ..., -0.0003, -0.0034, -0.0080],\n",
       "         [-0.0033,  0.0013,  0.0003,  ..., -0.0025, -0.0026, -0.0027],\n",
       "         [ 0.0030, -0.0003,  0.0025,  ..., -0.0009, -0.0027, -0.0010],\n",
       "         ...,\n",
       "         [ 0.0011, -0.0023,  0.0023,  ..., -0.0005, -0.0048,  0.0019],\n",
       "         [-0.0002,  0.0009, -0.0018,  ..., -0.0057,  0.0049, -0.0023],\n",
       "         [-0.0008, -0.0003,  0.0044,  ...,  0.0016,  0.0003,  0.0013]],\n",
       "        requires_grad=True),\n",
       " 'layers.5.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.5.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.6.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 3.1858e-04, -6.0609e-05, -7.5724e-04,  ...,  1.3959e-03,\n",
       "           6.6846e-04,  2.9433e-04],\n",
       "         [ 1.7888e-03, -6.6215e-04, -9.1204e-04,  ...,  5.2116e-04,\n",
       "          -1.1430e-03,  3.2486e-03],\n",
       "         [-1.4228e-04,  8.3558e-04, -4.5061e-03,  ..., -2.3064e-03,\n",
       "          -1.6297e-04,  2.0284e-03],\n",
       "         ...,\n",
       "         [ 4.5241e-03, -2.2451e-04,  3.5764e-03,  ..., -5.1174e-04,\n",
       "           1.6287e-03, -1.7083e-03],\n",
       "         [-3.2820e-03, -3.0197e-03, -2.2841e-03,  ...,  6.0756e-04,\n",
       "          -3.8353e-03, -5.3353e-03],\n",
       "         [ 3.5612e-03,  4.1783e-03,  4.2927e-03,  ...,  5.3337e-03,\n",
       "          -2.5229e-03,  2.3508e-04]], requires_grad=True),\n",
       " 'layers.6.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-3.5649e-04,  8.4266e-04,  5.7057e-03,  ..., -5.5043e-04,\n",
       "          -4.7293e-04, -3.7276e-03],\n",
       "         [ 7.0538e-04,  2.5828e-03,  9.3278e-04,  ..., -5.0411e-05,\n",
       "           3.6624e-04, -2.5466e-03],\n",
       "         [ 3.7717e-04,  4.5982e-03,  3.4516e-03,  ..., -9.4116e-06,\n",
       "           2.0537e-04,  5.8485e-03],\n",
       "         ...,\n",
       "         [-1.5504e-03,  8.7562e-04, -1.4460e-03,  ...,  1.4154e-03,\n",
       "           9.6567e-04, -2.8470e-03],\n",
       "         [ 1.3911e-03, -1.0485e-03,  8.3792e-04,  ..., -3.8551e-04,\n",
       "          -8.9569e-04,  1.5289e-04],\n",
       "         [-2.1934e-03, -1.1295e-03,  2.5016e-03,  ..., -1.2651e-03,\n",
       "          -1.8856e-04, -1.9325e-03]], requires_grad=True),\n",
       " 'layers.6.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-0.0035,  0.0039, -0.0020,  ..., -0.0011,  0.0029,  0.0004],\n",
       "         [-0.0016,  0.0031,  0.0061,  ...,  0.0014, -0.0008, -0.0036],\n",
       "         [-0.0004, -0.0009,  0.0055,  ..., -0.0002,  0.0021, -0.0030],\n",
       "         ...,\n",
       "         [-0.0017, -0.0005, -0.0001,  ..., -0.0007, -0.0010,  0.0034],\n",
       "         [ 0.0026, -0.0021,  0.0026,  ...,  0.0019,  0.0017,  0.0010],\n",
       "         [ 0.0013,  0.0048, -0.0011,  ...,  0.0007,  0.0017, -0.0037]],\n",
       "        requires_grad=True),\n",
       " 'layers.6.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-1.2421e-03, -5.1394e-03, -2.9473e-03,  ..., -5.3677e-03,\n",
       "           2.3191e-03,  1.8479e-03],\n",
       "         [-1.5798e-03,  3.4649e-04, -6.3344e-04,  ...,  1.7337e-03,\n",
       "           8.8921e-04, -3.5690e-03],\n",
       "         [ 8.2538e-04,  1.5789e-03,  3.4786e-03,  ...,  2.2049e-03,\n",
       "          -3.1314e-03, -1.5180e-03],\n",
       "         ...,\n",
       "         [ 8.6279e-05, -1.6222e-03,  1.3573e-03,  ...,  3.9512e-03,\n",
       "           2.3335e-03,  1.2811e-03],\n",
       "         [ 3.9830e-03, -1.7687e-03,  3.3595e-04,  ..., -9.5036e-04,\n",
       "          -3.3451e-03, -1.3972e-03],\n",
       "         [ 1.6327e-03, -1.9445e-03,  5.3200e-03,  ..., -2.1155e-03,\n",
       "           5.7606e-03, -2.9213e-04]], requires_grad=True),\n",
       " 'layers.6.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-2.6680e-03,  8.4565e-04,  1.0427e-03,  ..., -5.8241e-04,\n",
       "          -4.0232e-03, -3.9420e-03],\n",
       "         [ 2.4862e-03,  1.6323e-03,  2.4442e-03,  ..., -4.4879e-04,\n",
       "           2.3295e-03, -1.9164e-03],\n",
       "         [ 4.1251e-03, -1.0940e-03,  5.1846e-03,  ..., -7.6015e-04,\n",
       "           7.2946e-04,  1.8379e-03],\n",
       "         ...,\n",
       "         [ 5.8666e-04, -2.1524e-04,  6.4297e-03,  ..., -2.2720e-03,\n",
       "          -4.2938e-04, -2.9182e-03],\n",
       "         [ 1.6946e-04,  1.7304e-03, -7.6479e-05,  ...,  4.4570e-03,\n",
       "          -3.6286e-03, -1.2400e-03],\n",
       "         [ 6.9167e-05, -2.6355e-03, -7.7777e-04,  ..., -2.3680e-03,\n",
       "          -3.4032e-05, -5.1536e-03]], requires_grad=True),\n",
       " 'layers.6.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2546e-03, -2.8036e-03, -2.3744e-03,  ..., -2.8712e-03,\n",
       "           1.7064e-03, -1.7041e-03],\n",
       "         [-8.9728e-04,  8.3558e-04,  1.1749e-03,  ...,  4.7870e-03,\n",
       "           1.3749e-03, -2.4571e-03],\n",
       "         [ 1.9097e-03,  2.6641e-03,  6.7888e-04,  ...,  1.5699e-03,\n",
       "           2.4566e-03, -6.8222e-06],\n",
       "         ...,\n",
       "         [ 7.8378e-04,  1.6973e-04, -1.0106e-03,  ..., -4.2934e-03,\n",
       "           7.7236e-03, -7.7410e-04],\n",
       "         [-2.4863e-03,  2.6499e-03, -2.3879e-03,  ...,  1.0339e-03,\n",
       "           1.6821e-03,  1.0519e-04],\n",
       "         [-1.7222e-03,  5.0415e-03,  3.2130e-03,  ...,  3.1946e-03,\n",
       "          -6.7787e-05, -1.1889e-03]], requires_grad=True),\n",
       " 'layers.6.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2520e-03, -2.7286e-03,  1.0934e-03,  ..., -2.1369e-03,\n",
       "           1.2821e-03, -4.1721e-04],\n",
       "         [ 2.1497e-03, -7.2862e-04,  2.3788e-03,  ..., -1.5302e-03,\n",
       "           4.6317e-03, -7.3948e-04],\n",
       "         [-8.1499e-04,  3.8663e-03,  2.5227e-03,  ...,  4.9622e-03,\n",
       "           1.7649e-03,  2.1632e-03],\n",
       "         ...,\n",
       "         [-1.4259e-03,  8.7369e-04, -2.7012e-03,  ...,  3.7356e-03,\n",
       "          -2.1124e-03,  1.6034e-06],\n",
       "         [-3.9365e-03, -1.5614e-03, -1.3386e-04,  ..., -3.9140e-04,\n",
       "          -1.0223e-03,  4.8417e-04],\n",
       "         [-2.8224e-05, -2.0793e-03, -8.9773e-04,  ..., -2.3579e-03,\n",
       "           1.2342e-03, -2.3007e-03]], requires_grad=True),\n",
       " 'layers.6.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.6.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.7.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0009, -0.0032,  0.0016,  ...,  0.0062,  0.0032, -0.0038],\n",
       "         [-0.0039,  0.0003,  0.0008,  ...,  0.0009, -0.0007, -0.0016],\n",
       "         [ 0.0015,  0.0001,  0.0019,  ..., -0.0021,  0.0035,  0.0036],\n",
       "         ...,\n",
       "         [ 0.0014, -0.0041, -0.0020,  ...,  0.0041,  0.0033,  0.0006],\n",
       "         [-0.0024, -0.0042,  0.0038,  ...,  0.0004, -0.0012, -0.0006],\n",
       "         [ 0.0038,  0.0036, -0.0013,  ..., -0.0039, -0.0014,  0.0008]],\n",
       "        requires_grad=True),\n",
       " 'layers.7.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 3.3585e-03, -1.1307e-03,  9.2058e-04,  ...,  8.9762e-04,\n",
       "          -7.5855e-05, -1.0115e-03],\n",
       "         [-1.2401e-03,  7.4790e-04, -1.5070e-03,  ...,  2.5962e-04,\n",
       "           2.3169e-03, -4.7807e-03],\n",
       "         [ 2.8353e-04,  3.1749e-03,  7.5417e-04,  ..., -1.9201e-03,\n",
       "           3.8950e-04,  1.6606e-03],\n",
       "         ...,\n",
       "         [ 1.3553e-03,  5.0711e-04,  1.7346e-03,  ..., -3.5646e-03,\n",
       "          -1.2081e-03,  1.3475e-03],\n",
       "         [ 1.4167e-03, -2.3380e-03,  6.5589e-04,  ...,  4.5582e-04,\n",
       "           2.0009e-03,  1.2143e-03],\n",
       "         [-2.7825e-03,  5.2691e-03,  9.5375e-04,  ..., -3.4739e-03,\n",
       "           1.1463e-03, -3.7299e-03]], requires_grad=True),\n",
       " 'layers.7.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-0.0023,  0.0020,  0.0028,  ...,  0.0011,  0.0006, -0.0032],\n",
       "         [-0.0007,  0.0042, -0.0039,  ...,  0.0006,  0.0009,  0.0002],\n",
       "         [-0.0033, -0.0037,  0.0038,  ..., -0.0009, -0.0017,  0.0015],\n",
       "         ...,\n",
       "         [-0.0016,  0.0017,  0.0037,  ...,  0.0005,  0.0028, -0.0026],\n",
       "         [-0.0032, -0.0015,  0.0003,  ...,  0.0012, -0.0002, -0.0007],\n",
       "         [ 0.0021,  0.0004, -0.0007,  ..., -0.0005, -0.0018,  0.0005]],\n",
       "        requires_grad=True),\n",
       " 'layers.7.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-4.0409e-04,  9.0934e-04,  2.8241e-03,  ..., -1.9214e-03,\n",
       "          -4.1953e-04, -1.7637e-03],\n",
       "         [-8.3495e-04,  2.5103e-04,  8.0565e-04,  ...,  1.7332e-03,\n",
       "           2.5875e-03,  2.8751e-03],\n",
       "         [-1.1602e-03,  2.1656e-03, -6.5797e-05,  ...,  5.7390e-04,\n",
       "           3.2276e-03,  2.4812e-03],\n",
       "         ...,\n",
       "         [ 1.6250e-03, -3.3849e-03, -5.1995e-03,  ...,  1.2912e-03,\n",
       "          -4.8020e-04,  1.8932e-03],\n",
       "         [-3.4274e-03, -2.4788e-03,  2.7586e-03,  ...,  5.9330e-03,\n",
       "          -6.7303e-04, -1.1940e-03],\n",
       "         [ 1.2199e-03,  1.5924e-03, -3.3142e-03,  ...,  1.4887e-03,\n",
       "          -5.2493e-03,  2.0424e-03]], requires_grad=True),\n",
       " 'layers.7.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2819e-04,  9.9469e-04, -3.2637e-03,  ..., -2.2525e-03,\n",
       "          -4.0698e-04, -2.3689e-03],\n",
       "         [ 1.7436e-03,  8.6333e-05,  4.8697e-03,  ...,  5.9780e-04,\n",
       "          -4.5421e-04,  1.5784e-03],\n",
       "         [-1.0538e-04, -1.4748e-04,  2.6690e-03,  ...,  1.9056e-03,\n",
       "          -3.0633e-04, -3.2494e-03],\n",
       "         ...,\n",
       "         [ 6.2184e-04, -2.1843e-03, -1.7752e-03,  ...,  2.3181e-04,\n",
       "          -1.5672e-03, -1.8777e-03],\n",
       "         [-7.4636e-04, -5.3362e-03, -4.5783e-03,  ..., -3.3109e-04,\n",
       "           3.6386e-03,  6.0677e-04],\n",
       "         [-4.8122e-03,  8.3116e-03,  1.3369e-03,  ...,  2.0731e-03,\n",
       "          -7.5567e-04, -1.2907e-03]], requires_grad=True),\n",
       " 'layers.7.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-2.1916e-03, -1.7734e-04,  1.8728e-05,  ..., -6.1611e-05,\n",
       "           1.3832e-03, -2.4434e-03],\n",
       "         [ 2.9949e-04,  4.0103e-03,  1.1440e-03,  ...,  1.7243e-03,\n",
       "          -5.5055e-04,  5.3077e-03],\n",
       "         [-9.9907e-05,  4.3364e-03, -5.1651e-04,  ...,  2.5512e-03,\n",
       "          -3.2039e-04, -3.4600e-04],\n",
       "         ...,\n",
       "         [-1.1740e-03,  1.5055e-03, -1.8739e-03,  ..., -1.1807e-03,\n",
       "          -1.3934e-03, -2.3664e-03],\n",
       "         [-1.0697e-03,  3.6055e-03, -4.1203e-03,  ...,  3.7549e-03,\n",
       "           7.6226e-04,  2.3985e-03],\n",
       "         [ 1.3251e-03,  9.4204e-04,  1.9642e-03,  ...,  2.4151e-03,\n",
       "          -2.6758e-03,  2.6713e-03]], requires_grad=True),\n",
       " 'layers.7.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0030, -0.0022, -0.0015,  ...,  0.0002,  0.0024,  0.0009],\n",
       "         [-0.0017,  0.0006,  0.0024,  ..., -0.0007, -0.0019,  0.0036],\n",
       "         [-0.0009, -0.0038, -0.0037,  ..., -0.0038,  0.0011, -0.0045],\n",
       "         ...,\n",
       "         [-0.0049, -0.0021, -0.0029,  ...,  0.0054, -0.0028, -0.0011],\n",
       "         [ 0.0045,  0.0032,  0.0005,  ...,  0.0043, -0.0003,  0.0027],\n",
       "         [-0.0064,  0.0044,  0.0015,  ..., -0.0033, -0.0030,  0.0003]],\n",
       "        requires_grad=True),\n",
       " 'layers.7.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.7.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.8.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 3.4932e-03, -1.3820e-03, -2.9563e-03,  ..., -2.0512e-03,\n",
       "          -6.0049e-04, -9.3854e-04],\n",
       "         [-2.8141e-05, -4.1544e-04,  3.2040e-03,  ...,  6.7705e-04,\n",
       "          -2.7717e-04, -1.2972e-03],\n",
       "         [-3.7300e-03,  2.9628e-03, -5.8849e-04,  ...,  1.0475e-03,\n",
       "           1.7724e-03, -5.8269e-04],\n",
       "         ...,\n",
       "         [ 1.9626e-03,  1.2020e-03,  1.2200e-03,  ..., -3.1177e-04,\n",
       "           3.8553e-03, -1.6159e-03],\n",
       "         [-5.8624e-04, -5.6552e-03, -2.2241e-03,  ..., -2.7864e-03,\n",
       "          -2.6167e-03,  1.7012e-03],\n",
       "         [ 3.1611e-04, -2.2080e-03,  6.2227e-03,  ...,  1.7994e-03,\n",
       "           1.1889e-03,  2.3063e-03]], requires_grad=True),\n",
       " 'layers.8.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0026,  0.0043,  0.0004,  ..., -0.0031,  0.0014,  0.0031],\n",
       "         [ 0.0046,  0.0045,  0.0011,  ...,  0.0016,  0.0023,  0.0011],\n",
       "         [ 0.0041,  0.0009, -0.0015,  ...,  0.0030, -0.0011, -0.0008],\n",
       "         ...,\n",
       "         [ 0.0010, -0.0003, -0.0028,  ..., -0.0015, -0.0024,  0.0045],\n",
       "         [ 0.0035,  0.0018,  0.0041,  ..., -0.0028, -0.0014, -0.0041],\n",
       "         [ 0.0016,  0.0011,  0.0035,  ..., -0.0004,  0.0071, -0.0014]],\n",
       "        requires_grad=True),\n",
       " 'layers.8.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-3.6021e-04, -3.2877e-03,  4.5058e-04,  ...,  5.5331e-04,\n",
       "           3.0216e-04,  4.0288e-03],\n",
       "         [-6.1910e-03, -2.4868e-04, -3.7514e-03,  ..., -2.4288e-03,\n",
       "           1.8820e-05, -3.1329e-03],\n",
       "         [ 1.4642e-03, -1.7411e-03, -1.4212e-03,  ..., -1.9278e-03,\n",
       "           1.0733e-03, -2.9787e-03],\n",
       "         ...,\n",
       "         [ 2.7535e-03,  1.3614e-03,  6.3438e-03,  ..., -1.9000e-03,\n",
       "          -2.9031e-03,  3.6492e-03],\n",
       "         [-1.5578e-03, -4.0234e-03, -2.5602e-03,  ..., -7.2438e-04,\n",
       "          -1.7892e-03,  4.5106e-03],\n",
       "         [ 2.9489e-03, -1.8945e-03,  2.4255e-04,  ...,  1.2699e-03,\n",
       "          -1.6889e-03, -1.6734e-03]], requires_grad=True),\n",
       " 'layers.8.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-4.3456e-05, -2.7075e-03,  2.0347e-04,  ...,  8.2173e-04,\n",
       "           1.7714e-03,  2.3883e-03],\n",
       "         [-1.9038e-03, -1.0360e-03, -4.6095e-03,  ..., -3.1027e-03,\n",
       "          -4.6217e-03,  7.1689e-04],\n",
       "         [-9.4005e-04, -2.2495e-03, -2.1123e-03,  ..., -2.4567e-04,\n",
       "          -4.5565e-03, -1.0361e-03],\n",
       "         ...,\n",
       "         [ 6.0961e-04,  2.5321e-03,  1.6850e-03,  ...,  6.5564e-04,\n",
       "           1.2370e-03,  2.7297e-03],\n",
       "         [ 6.4629e-04, -1.6163e-03,  3.3000e-03,  ..., -2.6804e-03,\n",
       "           2.5749e-03, -1.3110e-03],\n",
       "         [-6.7138e-03, -3.6868e-04, -1.1157e-03,  ...,  3.0199e-03,\n",
       "           2.1010e-03,  1.7397e-03]], requires_grad=True),\n",
       " 'layers.8.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 2.0333e-03, -1.1154e-03, -1.3197e-03,  ..., -2.5407e-03,\n",
       "           9.4658e-04,  8.9005e-04],\n",
       "         [-4.2269e-04,  2.9748e-03,  2.4661e-03,  ...,  1.4786e-04,\n",
       "          -3.3763e-03, -2.8720e-03],\n",
       "         [ 8.4549e-04, -1.4832e-04, -3.8786e-05,  ...,  4.5239e-03,\n",
       "           1.8031e-03, -1.0744e-03],\n",
       "         ...,\n",
       "         [-4.5008e-03,  1.7812e-03,  2.6835e-03,  ...,  2.9527e-07,\n",
       "          -3.7647e-03,  4.7895e-03],\n",
       "         [ 4.0311e-04, -5.9746e-04, -2.7811e-03,  ...,  2.8908e-03,\n",
       "          -1.5125e-03,  1.8972e-04],\n",
       "         [-4.5496e-03,  2.9934e-03, -1.5651e-03,  ..., -2.7638e-04,\n",
       "           3.4321e-03, -3.2919e-03]], requires_grad=True),\n",
       " 'layers.8.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0029, -0.0030,  0.0026,  ..., -0.0024, -0.0044, -0.0033],\n",
       "         [-0.0026, -0.0031,  0.0019,  ..., -0.0033,  0.0004, -0.0011],\n",
       "         [ 0.0007,  0.0036,  0.0022,  ...,  0.0016,  0.0002,  0.0004],\n",
       "         ...,\n",
       "         [-0.0001, -0.0011,  0.0032,  ..., -0.0020, -0.0042,  0.0021],\n",
       "         [-0.0008,  0.0021, -0.0017,  ..., -0.0035, -0.0017,  0.0026],\n",
       "         [-0.0044, -0.0003, -0.0003,  ...,  0.0014,  0.0015,  0.0006]],\n",
       "        requires_grad=True),\n",
       " 'layers.8.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 5.2686e-03, -3.7862e-03,  1.2843e-03,  ...,  3.1265e-03,\n",
       "          -4.2223e-03,  2.0218e-03],\n",
       "         [-2.2024e-03,  3.7678e-03,  3.2638e-03,  ...,  3.0155e-03,\n",
       "           2.4487e-03,  3.8891e-04],\n",
       "         [ 4.6170e-03, -8.0781e-04,  6.9446e-04,  ...,  1.6923e-03,\n",
       "          -1.5331e-03,  3.8752e-03],\n",
       "         ...,\n",
       "         [ 9.3269e-04, -1.4757e-03,  4.5576e-03,  ..., -1.1074e-03,\n",
       "          -2.1381e-03,  5.8320e-04],\n",
       "         [-1.3815e-03,  3.1317e-03,  4.0875e-03,  ...,  3.7707e-03,\n",
       "          -2.5272e-03,  2.4814e-03],\n",
       "         [ 1.1637e-03, -3.7357e-03,  2.7120e-04,  ..., -2.2776e-03,\n",
       "           4.1129e-05,  4.6985e-04]], requires_grad=True),\n",
       " 'layers.8.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.8.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.9.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 9.6276e-04, -2.9979e-03, -2.4247e-03,  ..., -1.0135e-03,\n",
       "          -2.7201e-03,  1.5758e-03],\n",
       "         [-1.8914e-03, -2.3486e-04, -8.9478e-05,  ...,  3.1476e-03,\n",
       "          -3.4251e-04,  1.8413e-03],\n",
       "         [-2.2040e-03,  1.8978e-04, -2.1980e-03,  ..., -1.9685e-03,\n",
       "           2.8796e-03, -1.0020e-03],\n",
       "         ...,\n",
       "         [ 1.6208e-03, -1.7842e-04, -1.6693e-03,  ..., -1.4405e-03,\n",
       "           6.0201e-05,  1.7181e-03],\n",
       "         [ 6.4434e-03,  6.1818e-04,  1.5796e-03,  ..., -2.8036e-04,\n",
       "           3.3051e-03,  3.9566e-03],\n",
       "         [ 1.6451e-03,  4.8379e-04, -4.2154e-04,  ...,  1.9966e-03,\n",
       "          -2.8175e-03, -1.5794e-03]], requires_grad=True),\n",
       " 'layers.9.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-5.4331e-04,  4.1674e-03,  2.3329e-03,  ..., -2.1730e-03,\n",
       "          -2.0896e-03,  4.2221e-04],\n",
       "         [ 1.9897e-03, -5.9784e-04, -2.8808e-03,  ...,  2.5036e-04,\n",
       "          -2.8198e-03,  4.9977e-03],\n",
       "         [-2.7532e-04,  1.1375e-03, -7.9205e-04,  ..., -5.5918e-03,\n",
       "           8.4437e-04,  5.3016e-04],\n",
       "         ...,\n",
       "         [ 5.8705e-04, -4.2024e-03,  4.4002e-06,  ..., -1.8423e-03,\n",
       "          -1.0136e-03,  3.0297e-03],\n",
       "         [ 2.6468e-03,  2.1836e-03,  4.0013e-03,  ...,  1.3712e-03,\n",
       "           1.3747e-03, -2.5740e-03],\n",
       "         [ 3.7920e-03,  3.7389e-04,  3.7823e-03,  ..., -8.6830e-04,\n",
       "           6.0979e-04, -8.0873e-05]], requires_grad=True),\n",
       " 'layers.9.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-9.2773e-04,  2.4450e-03,  1.7675e-03,  ..., -2.9745e-03,\n",
       "           2.3629e-03,  4.7166e-03],\n",
       "         [-2.0266e-03,  6.0248e-04, -1.2724e-03,  ..., -1.8265e-03,\n",
       "          -1.3389e-03,  1.1647e-03],\n",
       "         [-6.9555e-04,  3.1619e-03, -4.0545e-03,  ...,  2.5879e-04,\n",
       "           1.3284e-03, -4.1805e-03],\n",
       "         ...,\n",
       "         [-4.7449e-03, -4.8630e-04, -4.7885e-03,  ..., -2.4512e-03,\n",
       "          -2.6572e-03,  4.1160e-03],\n",
       "         [-3.3494e-03,  1.1198e-03, -2.8279e-04,  ...,  4.8914e-03,\n",
       "          -5.4338e-03,  1.1550e-03],\n",
       "         [-7.9278e-05, -1.9471e-03, -1.7812e-03,  ..., -6.7638e-04,\n",
       "           5.1554e-04,  2.4214e-03]], requires_grad=True),\n",
       " 'layers.9.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 3.3185e-03,  7.3773e-04, -6.3289e-04,  ...,  3.8893e-03,\n",
       "          -2.0814e-03, -8.9698e-04],\n",
       "         [ 1.1601e-03, -3.1016e-04, -3.6596e-03,  ..., -5.5181e-03,\n",
       "          -3.8664e-04,  4.7972e-03],\n",
       "         [-1.0595e-03,  8.2402e-05, -3.9613e-04,  ..., -3.7198e-03,\n",
       "          -8.0676e-04,  8.4528e-04],\n",
       "         ...,\n",
       "         [ 1.4741e-03,  9.7265e-04, -1.3793e-03,  ..., -4.2809e-03,\n",
       "           1.1661e-03, -3.9342e-03],\n",
       "         [ 3.9529e-04, -5.5148e-04, -4.6574e-04,  ..., -3.8824e-04,\n",
       "           2.6394e-03, -4.4372e-03],\n",
       "         [ 1.8761e-03, -1.7301e-03,  6.4775e-04,  ...,  1.0634e-03,\n",
       "           2.3541e-03,  2.3849e-03]], requires_grad=True),\n",
       " 'layers.9.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 1.7416e-03,  6.8938e-04, -3.8313e-03,  ...,  1.5262e-03,\n",
       "           3.6294e-03, -2.7084e-03],\n",
       "         [-3.2831e-03, -6.4709e-05, -1.4421e-03,  ...,  1.3503e-03,\n",
       "          -1.8625e-03,  3.0773e-03],\n",
       "         [-1.7311e-03, -2.5260e-03, -2.1744e-03,  ...,  3.9939e-05,\n",
       "           3.3105e-04,  7.9804e-04],\n",
       "         ...,\n",
       "         [-5.2551e-04,  4.0823e-03,  2.5754e-03,  ...,  2.3886e-03,\n",
       "           1.6950e-03,  3.0082e-03],\n",
       "         [ 3.9889e-03, -3.2574e-04,  1.9939e-04,  ...,  5.5540e-05,\n",
       "           1.9904e-04,  7.1940e-04],\n",
       "         [ 1.0935e-03,  4.4577e-03, -1.6247e-04,  ...,  1.7177e-06,\n",
       "          -1.3865e-03, -4.9690e-03]], requires_grad=True),\n",
       " 'layers.9.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-1.5133e-03,  3.0453e-03,  2.3517e-05,  ...,  2.5883e-03,\n",
       "           6.3596e-04, -2.7611e-03],\n",
       "         [ 3.2649e-03, -8.7990e-05, -5.8548e-04,  ..., -5.8848e-04,\n",
       "          -3.1762e-03, -1.9058e-03],\n",
       "         [-2.1507e-03,  2.4335e-03, -1.1754e-03,  ...,  6.6671e-04,\n",
       "          -4.0744e-03, -6.9437e-04],\n",
       "         ...,\n",
       "         [ 1.0305e-03,  5.5196e-04,  2.0887e-03,  ..., -3.5086e-03,\n",
       "           1.8752e-04,  6.0818e-04],\n",
       "         [-4.1997e-03,  7.8440e-04, -3.1304e-03,  ...,  4.6411e-04,\n",
       "           1.0633e-03, -1.0469e-03],\n",
       "         [-2.8961e-03, -6.1031e-04, -2.7199e-04,  ...,  2.3213e-03,\n",
       "          -1.6993e-03, -1.7671e-03]], requires_grad=True),\n",
       " 'layers.9.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.3863e-03, -2.2233e-03,  3.1664e-03,  ..., -1.9609e-03,\n",
       "          -2.1622e-03, -1.8846e-03],\n",
       "         [-1.3561e-05,  1.2489e-03,  3.8833e-03,  ...,  3.0237e-04,\n",
       "           3.2535e-04,  1.3786e-04],\n",
       "         [ 3.4056e-03,  3.8176e-05,  1.7618e-03,  ..., -1.3465e-03,\n",
       "           1.5120e-03, -1.9366e-03],\n",
       "         ...,\n",
       "         [-5.2014e-03,  1.5338e-03,  2.3434e-03,  ...,  2.3157e-03,\n",
       "          -4.4292e-04,  1.4532e-03],\n",
       "         [ 2.3220e-03, -1.0423e-03,  3.5964e-03,  ...,  7.2184e-04,\n",
       "          -3.5265e-03, -6.2952e-03],\n",
       "         [-3.5592e-03,  1.5141e-03,  1.7141e-03,  ..., -1.8451e-03,\n",
       "           2.3958e-03, -6.6697e-03]], requires_grad=True),\n",
       " 'layers.9.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.9.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.10.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-4.9138e-03,  1.9877e-03, -1.3662e-03,  ..., -7.6622e-04,\n",
       "           1.5985e-03,  1.5798e-03],\n",
       "         [-5.1012e-03,  4.1993e-03, -3.4503e-05,  ..., -5.9148e-03,\n",
       "          -3.0009e-03,  1.8736e-03],\n",
       "         [-6.5311e-04, -2.5133e-03, -1.8086e-06,  ..., -1.0454e-03,\n",
       "          -6.6139e-04,  4.8684e-03],\n",
       "         ...,\n",
       "         [ 3.2992e-03, -2.1135e-03, -5.5716e-05,  ...,  2.0345e-03,\n",
       "           2.7447e-03,  2.8833e-03],\n",
       "         [-5.2840e-03, -5.6128e-04, -1.5280e-03,  ..., -2.2666e-03,\n",
       "          -6.9258e-03,  6.0519e-04],\n",
       "         [-1.9960e-03,  6.1914e-04, -2.6842e-03,  ...,  2.3578e-04,\n",
       "           3.0495e-03,  6.7364e-04]], requires_grad=True),\n",
       " 'layers.10.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0004,  0.0006, -0.0002,  ...,  0.0004,  0.0025, -0.0014],\n",
       "         [ 0.0038,  0.0019,  0.0010,  ..., -0.0030,  0.0038, -0.0009],\n",
       "         [ 0.0026,  0.0032, -0.0002,  ..., -0.0002, -0.0041, -0.0009],\n",
       "         ...,\n",
       "         [ 0.0017,  0.0015,  0.0003,  ...,  0.0035,  0.0004,  0.0007],\n",
       "         [-0.0028,  0.0024,  0.0004,  ..., -0.0009, -0.0039,  0.0032],\n",
       "         [-0.0022, -0.0012, -0.0042,  ..., -0.0035, -0.0002,  0.0021]],\n",
       "        requires_grad=True),\n",
       " 'layers.10.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-1.0370e-03, -6.0725e-04,  8.2384e-04,  ..., -4.9340e-07,\n",
       "           9.8398e-05,  2.0691e-04],\n",
       "         [ 1.6398e-03,  3.1132e-03,  2.4471e-03,  ...,  4.2170e-03,\n",
       "           1.4450e-03,  7.6612e-04],\n",
       "         [-4.7304e-04,  1.6530e-03,  4.0772e-03,  ..., -2.5743e-03,\n",
       "          -5.3530e-03, -2.1798e-03],\n",
       "         ...,\n",
       "         [ 2.8139e-03, -7.4134e-04,  6.1331e-04,  ..., -3.4691e-05,\n",
       "          -9.2315e-04,  5.1292e-04],\n",
       "         [ 3.8400e-04,  1.2620e-03,  5.2207e-04,  ...,  2.4734e-03,\n",
       "           3.2442e-03,  1.1399e-03],\n",
       "         [ 3.4236e-04,  1.3818e-03,  5.3804e-03,  ...,  6.7555e-03,\n",
       "           9.5724e-04, -3.5952e-03]], requires_grad=True),\n",
       " 'layers.10.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0006,  0.0019, -0.0028,  ..., -0.0043,  0.0014, -0.0007],\n",
       "         [ 0.0009, -0.0035, -0.0001,  ..., -0.0018, -0.0027,  0.0047],\n",
       "         [ 0.0032,  0.0011, -0.0008,  ..., -0.0050, -0.0020, -0.0008],\n",
       "         ...,\n",
       "         [-0.0043, -0.0015,  0.0011,  ...,  0.0036, -0.0036, -0.0008],\n",
       "         [ 0.0021,  0.0014, -0.0008,  ..., -0.0030,  0.0006,  0.0001],\n",
       "         [-0.0005,  0.0020, -0.0058,  ...,  0.0003,  0.0027, -0.0043]],\n",
       "        requires_grad=True),\n",
       " 'layers.10.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-3.9407e-03,  4.2150e-03, -4.2342e-03,  ..., -2.5192e-03,\n",
       "           2.9707e-03,  2.5630e-03],\n",
       "         [ 5.3023e-04, -2.5330e-03,  4.5732e-04,  ..., -3.1711e-03,\n",
       "          -1.6184e-03,  4.1582e-03],\n",
       "         [ 1.1349e-03, -4.5124e-03,  1.6826e-03,  ...,  2.7977e-03,\n",
       "           4.1669e-04,  9.2839e-04],\n",
       "         ...,\n",
       "         [-7.7705e-04,  1.8306e-03, -2.0586e-03,  ..., -4.3630e-04,\n",
       "          -3.2862e-03,  2.7390e-03],\n",
       "         [ 1.6821e-03,  2.5945e-03,  1.0143e-03,  ..., -5.6687e-04,\n",
       "          -7.0967e-05,  2.7401e-03],\n",
       "         [ 1.6094e-03, -5.7867e-04, -1.6850e-03,  ...,  1.8051e-03,\n",
       "           3.8015e-03, -4.9573e-03]], requires_grad=True),\n",
       " 'layers.10.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0013,  0.0012, -0.0033,  ...,  0.0014,  0.0050,  0.0031],\n",
       "         [ 0.0033,  0.0071,  0.0002,  ...,  0.0011, -0.0029, -0.0015],\n",
       "         [-0.0023,  0.0027,  0.0021,  ..., -0.0067,  0.0005,  0.0019],\n",
       "         ...,\n",
       "         [ 0.0002,  0.0021, -0.0014,  ...,  0.0023, -0.0010, -0.0037],\n",
       "         [-0.0017,  0.0026,  0.0024,  ...,  0.0017,  0.0017,  0.0036],\n",
       "         [ 0.0012, -0.0030, -0.0019,  ...,  0.0023,  0.0018,  0.0012]],\n",
       "        requires_grad=True),\n",
       " 'layers.10.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0021,  0.0023,  0.0001,  ...,  0.0027,  0.0026,  0.0013],\n",
       "         [-0.0029,  0.0023, -0.0050,  ..., -0.0013,  0.0021, -0.0013],\n",
       "         [-0.0024, -0.0027, -0.0010,  ..., -0.0008, -0.0012, -0.0057],\n",
       "         ...,\n",
       "         [-0.0017,  0.0025,  0.0022,  ..., -0.0002,  0.0012, -0.0015],\n",
       "         [ 0.0016, -0.0031,  0.0002,  ..., -0.0003,  0.0006,  0.0019],\n",
       "         [ 0.0001, -0.0010,  0.0023,  ..., -0.0023, -0.0002, -0.0016]],\n",
       "        requires_grad=True),\n",
       " 'layers.10.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.10.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.11.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 3.7161e-03, -2.0462e-03, -8.1036e-05,  ..., -3.5911e-03,\n",
       "           4.8772e-03,  1.0057e-03],\n",
       "         [-2.2378e-03,  1.9106e-03, -1.8212e-03,  ..., -4.7544e-03,\n",
       "          -8.5798e-05, -5.2276e-04],\n",
       "         [ 2.0530e-04,  3.2563e-03,  9.1118e-04,  ..., -2.5545e-04,\n",
       "          -2.6438e-03, -4.0605e-03],\n",
       "         ...,\n",
       "         [-5.3375e-04,  2.6664e-03, -3.5858e-03,  ..., -1.2990e-03,\n",
       "          -1.8959e-03, -1.9484e-03],\n",
       "         [ 8.3174e-04, -3.4930e-03,  2.1846e-03,  ..., -9.4290e-04,\n",
       "          -3.1471e-03,  4.7576e-05],\n",
       "         [ 3.8308e-04, -1.0284e-03,  1.8251e-03,  ...,  8.4358e-04,\n",
       "          -1.9510e-03, -7.1402e-04]], requires_grad=True),\n",
       " 'layers.11.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 1.2885e-03, -1.7742e-03, -9.3602e-04,  ..., -1.6896e-03,\n",
       "           3.3104e-04,  2.1369e-03],\n",
       "         [-1.4505e-03, -6.0139e-03, -2.4877e-03,  ...,  2.1590e-03,\n",
       "           1.9727e-03,  1.2926e-04],\n",
       "         [ 2.2294e-03, -3.4172e-03,  6.6792e-04,  ..., -1.8411e-03,\n",
       "           3.6797e-03,  5.1625e-04],\n",
       "         ...,\n",
       "         [ 2.2871e-03,  2.3341e-05, -2.1858e-03,  ..., -1.6865e-03,\n",
       "           7.2837e-04, -2.8161e-04],\n",
       "         [-1.5404e-03, -5.5754e-04,  6.1266e-03,  ...,  1.8388e-03,\n",
       "          -1.6346e-03,  1.2068e-03],\n",
       "         [ 1.2532e-03,  4.3606e-03,  2.4915e-04,  ..., -3.4340e-03,\n",
       "           1.9471e-03, -6.4400e-04]], requires_grad=True),\n",
       " 'layers.11.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0024, -0.0047,  0.0008,  ..., -0.0029, -0.0040, -0.0025],\n",
       "         [-0.0048, -0.0024,  0.0009,  ..., -0.0010, -0.0007, -0.0038],\n",
       "         [-0.0023,  0.0008, -0.0033,  ...,  0.0070,  0.0016, -0.0030],\n",
       "         ...,\n",
       "         [ 0.0035,  0.0010,  0.0026,  ...,  0.0005,  0.0040, -0.0067],\n",
       "         [-0.0027,  0.0029, -0.0019,  ...,  0.0021,  0.0002, -0.0024],\n",
       "         [-0.0010,  0.0019, -0.0007,  ..., -0.0014,  0.0008,  0.0010]],\n",
       "        requires_grad=True),\n",
       " 'layers.11.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0016,  0.0002,  0.0006,  ..., -0.0017,  0.0026, -0.0012],\n",
       "         [ 0.0039, -0.0025, -0.0014,  ...,  0.0044, -0.0024, -0.0018],\n",
       "         [ 0.0007, -0.0019, -0.0006,  ...,  0.0023, -0.0021,  0.0015],\n",
       "         ...,\n",
       "         [-0.0074,  0.0014, -0.0027,  ..., -0.0007, -0.0063, -0.0020],\n",
       "         [-0.0010, -0.0010, -0.0016,  ...,  0.0028, -0.0033, -0.0013],\n",
       "         [-0.0014, -0.0021, -0.0011,  ...,  0.0027, -0.0002, -0.0005]],\n",
       "        requires_grad=True),\n",
       " 'layers.11.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-0.0012, -0.0003,  0.0014,  ..., -0.0024, -0.0019,  0.0021],\n",
       "         [ 0.0025, -0.0054, -0.0013,  ...,  0.0018, -0.0018,  0.0009],\n",
       "         [-0.0015,  0.0033, -0.0015,  ..., -0.0007, -0.0008, -0.0008],\n",
       "         ...,\n",
       "         [-0.0033,  0.0018, -0.0041,  ..., -0.0019, -0.0042,  0.0015],\n",
       "         [ 0.0020, -0.0036,  0.0043,  ..., -0.0027,  0.0035, -0.0026],\n",
       "         [-0.0015, -0.0023, -0.0002,  ..., -0.0002, -0.0031, -0.0006]],\n",
       "        requires_grad=True),\n",
       " 'layers.11.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0033, -0.0007, -0.0023,  ..., -0.0042, -0.0002,  0.0004],\n",
       "         [ 0.0026,  0.0019, -0.0010,  ...,  0.0024,  0.0026, -0.0006],\n",
       "         [ 0.0020,  0.0011,  0.0004,  ..., -0.0086,  0.0008, -0.0071],\n",
       "         ...,\n",
       "         [ 0.0034,  0.0009, -0.0018,  ...,  0.0034, -0.0018,  0.0020],\n",
       "         [ 0.0008, -0.0019,  0.0019,  ...,  0.0026,  0.0039,  0.0015],\n",
       "         [ 0.0054,  0.0041, -0.0038,  ...,  0.0013,  0.0005,  0.0026]],\n",
       "        requires_grad=True),\n",
       " 'layers.11.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0035, -0.0038, -0.0017,  ..., -0.0008, -0.0038, -0.0037],\n",
       "         [-0.0011, -0.0010,  0.0008,  ..., -0.0023, -0.0024, -0.0033],\n",
       "         [-0.0027, -0.0008, -0.0015,  ..., -0.0017,  0.0006,  0.0016],\n",
       "         ...,\n",
       "         [ 0.0011,  0.0032,  0.0020,  ..., -0.0001, -0.0021, -0.0038],\n",
       "         [-0.0026,  0.0009, -0.0006,  ...,  0.0031, -0.0016, -0.0005],\n",
       "         [-0.0023, -0.0009, -0.0008,  ..., -0.0026, -0.0017,  0.0025]],\n",
       "        requires_grad=True),\n",
       " 'layers.11.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.11.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.12.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0087, -0.0024, -0.0049,  ..., -0.0026,  0.0028, -0.0005],\n",
       "         [ 0.0013,  0.0018,  0.0006,  ...,  0.0070, -0.0003,  0.0024],\n",
       "         [ 0.0001,  0.0020,  0.0026,  ..., -0.0010,  0.0005, -0.0021],\n",
       "         ...,\n",
       "         [-0.0007, -0.0015,  0.0030,  ..., -0.0003,  0.0019,  0.0012],\n",
       "         [-0.0036,  0.0021, -0.0017,  ..., -0.0025,  0.0014,  0.0020],\n",
       "         [ 0.0037, -0.0010, -0.0028,  ...,  0.0011, -0.0030,  0.0022]],\n",
       "        requires_grad=True),\n",
       " 'layers.12.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0020,  0.0011,  0.0003,  ..., -0.0004, -0.0002, -0.0021],\n",
       "         [ 0.0022,  0.0035,  0.0034,  ..., -0.0024, -0.0036,  0.0023],\n",
       "         [-0.0007,  0.0029,  0.0029,  ...,  0.0042, -0.0011, -0.0016],\n",
       "         ...,\n",
       "         [ 0.0029,  0.0021,  0.0007,  ..., -0.0012,  0.0025,  0.0006],\n",
       "         [ 0.0053, -0.0023,  0.0027,  ..., -0.0037, -0.0009,  0.0023],\n",
       "         [-0.0010, -0.0003,  0.0016,  ..., -0.0013,  0.0039, -0.0031]],\n",
       "        requires_grad=True),\n",
       " 'layers.12.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-1.8822e-03,  1.2823e-03,  3.1329e-03,  ..., -1.6134e-03,\n",
       "           6.9992e-04, -5.1143e-03],\n",
       "         [ 1.6421e-03,  4.1525e-03,  7.9553e-04,  ..., -7.1282e-04,\n",
       "          -5.1467e-04, -4.2086e-03],\n",
       "         [-3.5760e-03, -5.8743e-04, -1.9949e-03,  ...,  1.1390e-03,\n",
       "          -1.4026e-04, -4.9289e-04],\n",
       "         ...,\n",
       "         [ 4.3009e-03,  7.4670e-04, -2.6506e-03,  ..., -1.6557e-03,\n",
       "           3.0796e-03, -2.2251e-03],\n",
       "         [-9.1766e-04, -3.4823e-03, -3.3762e-03,  ...,  8.5784e-04,\n",
       "           7.2412e-05,  3.5603e-04],\n",
       "         [-2.9772e-03,  1.0744e-03,  1.6108e-03,  ...,  1.5656e-04,\n",
       "           4.5278e-03, -8.9494e-04]], requires_grad=True),\n",
       " 'layers.12.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-1.6635e-03, -1.3297e-03,  2.7621e-04,  ...,  9.7950e-04,\n",
       "           3.4391e-04, -2.8584e-03],\n",
       "         [-1.1630e-03, -1.6249e-03, -4.3796e-03,  ..., -2.0903e-03,\n",
       "          -2.1033e-03,  3.0578e-03],\n",
       "         [ 2.6445e-05,  8.3571e-03, -2.1253e-03,  ..., -1.7802e-03,\n",
       "           1.8916e-03, -2.2499e-03],\n",
       "         ...,\n",
       "         [-3.8477e-03,  1.0475e-03,  5.3070e-04,  ...,  2.2698e-03,\n",
       "          -7.8169e-03,  4.3364e-03],\n",
       "         [-1.3448e-03, -6.3856e-05,  6.9169e-03,  ..., -4.6080e-05,\n",
       "          -2.8621e-03,  3.4203e-03],\n",
       "         [-6.6162e-03, -3.1744e-04,  1.0665e-03,  ..., -4.3385e-03,\n",
       "          -1.9517e-03,  6.4329e-04]], requires_grad=True),\n",
       " 'layers.12.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-8.6912e-04, -1.1139e-03, -8.3961e-04,  ...,  4.1558e-03,\n",
       "           7.8115e-04, -3.0520e-04],\n",
       "         [-3.0443e-03, -3.5524e-03, -6.1288e-04,  ..., -5.0712e-05,\n",
       "           5.8297e-04,  1.1789e-03],\n",
       "         [ 1.2315e-03, -4.0954e-04, -1.4357e-03,  ..., -3.1036e-04,\n",
       "          -2.9468e-03,  5.2708e-03],\n",
       "         ...,\n",
       "         [-3.9789e-03,  1.1316e-03, -6.1489e-05,  ...,  2.5756e-03,\n",
       "           2.6804e-03, -4.8040e-03],\n",
       "         [ 1.0019e-04, -4.2053e-04, -4.0715e-04,  ..., -2.2800e-03,\n",
       "           9.8920e-04, -2.0646e-03],\n",
       "         [ 3.5004e-04, -2.5067e-04,  2.6768e-04,  ..., -1.1105e-03,\n",
       "           2.1534e-03,  8.8381e-04]], requires_grad=True),\n",
       " 'layers.12.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0036, -0.0013, -0.0039,  ..., -0.0050,  0.0031,  0.0009],\n",
       "         [ 0.0003,  0.0005,  0.0014,  ..., -0.0029, -0.0011,  0.0026],\n",
       "         [ 0.0003,  0.0011,  0.0008,  ..., -0.0009,  0.0004,  0.0014],\n",
       "         ...,\n",
       "         [-0.0043, -0.0003, -0.0007,  ...,  0.0011,  0.0010, -0.0003],\n",
       "         [-0.0025, -0.0032, -0.0026,  ...,  0.0004, -0.0012,  0.0013],\n",
       "         [ 0.0004,  0.0028,  0.0026,  ...,  0.0018, -0.0013,  0.0023]],\n",
       "        requires_grad=True),\n",
       " 'layers.12.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0020, -0.0049, -0.0002,  ...,  0.0004,  0.0023, -0.0004],\n",
       "         [ 0.0037,  0.0044, -0.0022,  ..., -0.0036,  0.0015, -0.0008],\n",
       "         [-0.0023,  0.0032, -0.0003,  ..., -0.0047,  0.0019,  0.0005],\n",
       "         ...,\n",
       "         [-0.0037,  0.0018,  0.0015,  ..., -0.0005, -0.0021, -0.0009],\n",
       "         [-0.0022, -0.0041, -0.0031,  ...,  0.0017,  0.0019,  0.0019],\n",
       "         [-0.0006, -0.0014, -0.0021,  ..., -0.0017,  0.0010,  0.0008]],\n",
       "        requires_grad=True),\n",
       " 'layers.12.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.12.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.13.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0006,  0.0013,  0.0029,  ...,  0.0018, -0.0011,  0.0020],\n",
       "         [-0.0009, -0.0024, -0.0018,  ...,  0.0017, -0.0001,  0.0014],\n",
       "         [ 0.0017,  0.0033,  0.0026,  ...,  0.0017, -0.0021,  0.0002],\n",
       "         ...,\n",
       "         [-0.0012,  0.0009,  0.0015,  ..., -0.0008,  0.0044, -0.0011],\n",
       "         [ 0.0006,  0.0025, -0.0034,  ...,  0.0007,  0.0024, -0.0003],\n",
       "         [-0.0025,  0.0024, -0.0011,  ...,  0.0002, -0.0014,  0.0008]],\n",
       "        requires_grad=True),\n",
       " 'layers.13.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0009, -0.0006, -0.0030,  ..., -0.0047, -0.0022,  0.0024],\n",
       "         [ 0.0030, -0.0035,  0.0010,  ..., -0.0014,  0.0020,  0.0014],\n",
       "         [-0.0002,  0.0009, -0.0014,  ..., -0.0013, -0.0004, -0.0007],\n",
       "         ...,\n",
       "         [ 0.0006, -0.0008,  0.0013,  ..., -0.0012,  0.0005, -0.0009],\n",
       "         [ 0.0012,  0.0021, -0.0058,  ...,  0.0003, -0.0003,  0.0035],\n",
       "         [-0.0013, -0.0007, -0.0007,  ..., -0.0043, -0.0007, -0.0003]],\n",
       "        requires_grad=True),\n",
       " 'layers.13.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-8.6065e-04,  5.2581e-03,  2.0233e-03,  ...,  2.1894e-03,\n",
       "          -4.0451e-03, -2.4729e-03],\n",
       "         [-1.9522e-03,  2.4632e-03, -1.3001e-03,  ...,  1.6522e-03,\n",
       "          -1.1900e-03,  1.4832e-03],\n",
       "         [ 1.0556e-03,  5.0134e-04, -1.7084e-05,  ..., -1.8018e-03,\n",
       "           1.5488e-03,  1.5247e-03],\n",
       "         ...,\n",
       "         [ 1.2080e-03, -3.5729e-03, -3.8973e-03,  ...,  3.2793e-04,\n",
       "           2.9084e-03,  3.8941e-03],\n",
       "         [ 6.9161e-03,  5.4005e-03, -7.5189e-03,  ...,  4.0517e-03,\n",
       "          -3.8229e-04, -2.4011e-03],\n",
       "         [ 4.5979e-04,  2.4359e-03, -2.0262e-03,  ..., -1.1605e-04,\n",
       "          -2.4038e-04,  3.0676e-03]], requires_grad=True),\n",
       " 'layers.13.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0024,  0.0046,  0.0023,  ..., -0.0014, -0.0024,  0.0021],\n",
       "         [ 0.0037,  0.0059, -0.0012,  ...,  0.0044, -0.0011,  0.0027],\n",
       "         [-0.0042,  0.0031, -0.0027,  ..., -0.0027,  0.0064, -0.0042],\n",
       "         ...,\n",
       "         [ 0.0026,  0.0028,  0.0027,  ...,  0.0017, -0.0004,  0.0016],\n",
       "         [ 0.0045,  0.0038,  0.0019,  ...,  0.0007,  0.0030, -0.0023],\n",
       "         [-0.0004, -0.0036,  0.0012,  ..., -0.0017,  0.0029,  0.0054]],\n",
       "        requires_grad=True),\n",
       " 'layers.13.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2340e-03, -4.7075e-04,  8.7522e-04,  ..., -4.4320e-03,\n",
       "          -2.2444e-03,  2.8786e-03],\n",
       "         [ 7.9911e-04,  3.1061e-03,  2.8647e-03,  ...,  2.3101e-03,\n",
       "          -6.1128e-06, -1.1709e-03],\n",
       "         [-9.9541e-06, -7.5550e-03, -1.6064e-03,  ...,  1.8163e-03,\n",
       "           7.0219e-03, -2.3938e-03],\n",
       "         ...,\n",
       "         [ 5.0624e-03, -5.9736e-04,  3.3249e-04,  ..., -8.2663e-04,\n",
       "          -7.9943e-04, -1.4939e-03],\n",
       "         [ 1.0428e-03, -7.4883e-04, -6.4359e-04,  ...,  1.0632e-03,\n",
       "           3.8413e-03,  3.6085e-03],\n",
       "         [-2.0173e-03,  3.0045e-03,  3.6573e-03,  ...,  1.3345e-03,\n",
       "           5.7249e-04,  1.2143e-03]], requires_grad=True),\n",
       " 'layers.13.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0060,  0.0007,  0.0015,  ..., -0.0007,  0.0052,  0.0036],\n",
       "         [ 0.0015,  0.0024, -0.0002,  ...,  0.0018, -0.0036, -0.0047],\n",
       "         [ 0.0008,  0.0017, -0.0007,  ...,  0.0004,  0.0022,  0.0018],\n",
       "         ...,\n",
       "         [ 0.0011,  0.0022,  0.0011,  ..., -0.0007,  0.0027,  0.0018],\n",
       "         [ 0.0036, -0.0003,  0.0009,  ...,  0.0016,  0.0013,  0.0026],\n",
       "         [ 0.0018,  0.0027, -0.0007,  ...,  0.0033, -0.0003, -0.0016]],\n",
       "        requires_grad=True),\n",
       " 'layers.13.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-2.4829e-03,  6.1563e-03, -8.3652e-04,  ...,  1.9455e-04,\n",
       "           8.5494e-05, -5.2473e-04],\n",
       "         [ 3.8900e-04,  2.9546e-03,  1.3874e-03,  ..., -1.0469e-03,\n",
       "           3.6897e-03,  1.5079e-03],\n",
       "         [ 3.4191e-03, -1.5855e-03,  1.4563e-03,  ...,  2.7270e-03,\n",
       "           1.8185e-03,  1.3947e-03],\n",
       "         ...,\n",
       "         [ 2.7620e-03,  2.6513e-03,  2.0906e-03,  ..., -1.4419e-03,\n",
       "           4.2981e-04,  3.6889e-04],\n",
       "         [ 4.7618e-03, -7.7611e-04, -1.2816e-03,  ..., -3.9520e-03,\n",
       "           4.4919e-04, -1.9333e-03],\n",
       "         [-5.3544e-03, -8.5450e-04, -3.0900e-03,  ..., -2.8142e-03,\n",
       "           1.2683e-03, -3.7363e-03]], requires_grad=True),\n",
       " 'layers.13.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.13.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.14.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 1.5578e-03, -8.9081e-04, -1.1991e-03,  ...,  3.5505e-03,\n",
       "           5.5161e-04, -1.2905e-04],\n",
       "         [-3.3709e-03, -1.8532e-03,  1.5622e-03,  ..., -1.0652e-04,\n",
       "          -3.8900e-04, -4.7821e-04],\n",
       "         [ 6.4836e-04,  9.8796e-04,  1.4520e-03,  ..., -2.2847e-03,\n",
       "           4.3100e-03,  1.2156e-03],\n",
       "         ...,\n",
       "         [ 6.5888e-05,  1.2201e-03,  3.3030e-03,  ..., -2.2526e-03,\n",
       "           5.1934e-03, -4.6228e-03],\n",
       "         [-8.7756e-04,  7.8990e-04,  3.6517e-03,  ...,  1.4661e-03,\n",
       "          -2.8052e-03, -6.7688e-04],\n",
       "         [-2.8877e-04,  1.8054e-03,  5.5915e-04,  ..., -2.3039e-03,\n",
       "          -4.3151e-04,  4.2796e-03]], requires_grad=True),\n",
       " 'layers.14.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-5.9887e-04, -6.1524e-04, -6.8424e-04,  ...,  1.3174e-03,\n",
       "          -1.1788e-03,  1.2734e-03],\n",
       "         [-9.8669e-04,  5.1273e-04,  8.2626e-04,  ..., -4.3713e-04,\n",
       "           1.9138e-03, -1.2852e-03],\n",
       "         [ 6.8350e-04,  5.0249e-03,  2.0475e-04,  ...,  4.6282e-04,\n",
       "          -1.4003e-03, -3.4211e-04],\n",
       "         ...,\n",
       "         [ 5.0839e-04, -1.6699e-03, -1.8435e-03,  ...,  4.1903e-03,\n",
       "           1.1617e-03,  8.6630e-04],\n",
       "         [ 1.3895e-03,  2.4307e-03,  2.4759e-04,  ..., -1.6121e-03,\n",
       "           1.6873e-03,  1.4292e-03],\n",
       "         [-2.5605e-03, -7.8777e-05,  4.7043e-03,  ..., -2.1087e-03,\n",
       "          -6.0004e-04,  6.6333e-04]], requires_grad=True),\n",
       " 'layers.14.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-3.7135e-03,  2.6356e-03,  1.0015e-03,  ..., -3.3369e-03,\n",
       "          -5.3010e-04,  1.0652e-03],\n",
       "         [-3.2934e-03,  2.7459e-04,  2.2393e-03,  ..., -1.7848e-03,\n",
       "           5.4341e-04, -5.1216e-03],\n",
       "         [ 3.5379e-03, -2.7625e-03, -4.6586e-04,  ...,  3.7231e-03,\n",
       "           3.7235e-03, -2.4621e-03],\n",
       "         ...,\n",
       "         [ 5.7569e-04,  3.6115e-03,  3.2523e-03,  ...,  4.5127e-03,\n",
       "          -8.2390e-03, -1.6714e-03],\n",
       "         [ 1.1345e-03,  2.1680e-03, -9.5588e-04,  ...,  1.5084e-03,\n",
       "           2.4564e-03,  2.3488e-04],\n",
       "         [-7.7781e-05,  2.0752e-03,  7.6550e-05,  ...,  4.6327e-03,\n",
       "          -3.5036e-03, -5.6994e-03]], requires_grad=True),\n",
       " 'layers.14.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 1.5916e-03,  3.8493e-05,  2.2012e-03,  ...,  1.8825e-03,\n",
       "          -3.1924e-03, -3.0731e-03],\n",
       "         [-1.8169e-03,  1.1219e-03, -2.2526e-03,  ..., -9.2823e-04,\n",
       "          -2.5423e-03,  1.9246e-03],\n",
       "         [-2.5813e-03, -8.9518e-04,  1.0465e-03,  ..., -5.9538e-05,\n",
       "           3.7641e-03,  3.8443e-04],\n",
       "         ...,\n",
       "         [-6.2063e-04,  2.5607e-04,  4.6980e-05,  ..., -1.9972e-04,\n",
       "          -3.6402e-03, -8.5323e-04],\n",
       "         [ 4.8401e-03,  1.4166e-03, -2.7214e-03,  ...,  6.9192e-04,\n",
       "          -1.5194e-03,  2.1904e-03],\n",
       "         [ 2.2765e-03,  1.9490e-03, -8.7319e-03,  ...,  2.5682e-04,\n",
       "           9.5400e-04,  2.6104e-03]], requires_grad=True),\n",
       " 'layers.14.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0025, -0.0022, -0.0007,  ...,  0.0017, -0.0040, -0.0017],\n",
       "         [-0.0015,  0.0042, -0.0025,  ..., -0.0017,  0.0018, -0.0017],\n",
       "         [ 0.0007,  0.0003,  0.0014,  ...,  0.0036,  0.0036, -0.0057],\n",
       "         ...,\n",
       "         [-0.0017,  0.0002, -0.0005,  ...,  0.0029, -0.0049, -0.0045],\n",
       "         [ 0.0004, -0.0030,  0.0001,  ...,  0.0006, -0.0003, -0.0017],\n",
       "         [-0.0046, -0.0020, -0.0027,  ...,  0.0036,  0.0033, -0.0005]],\n",
       "        requires_grad=True),\n",
       " 'layers.14.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0002, -0.0010,  0.0025,  ..., -0.0007,  0.0034, -0.0020],\n",
       "         [ 0.0054, -0.0058, -0.0019,  ...,  0.0006, -0.0013,  0.0034],\n",
       "         [-0.0004,  0.0041, -0.0027,  ..., -0.0021, -0.0041, -0.0038],\n",
       "         ...,\n",
       "         [ 0.0014,  0.0033,  0.0010,  ..., -0.0030, -0.0003,  0.0003],\n",
       "         [ 0.0042,  0.0009, -0.0003,  ...,  0.0007,  0.0006, -0.0023],\n",
       "         [ 0.0004,  0.0032, -0.0022,  ...,  0.0020, -0.0034, -0.0032]],\n",
       "        requires_grad=True),\n",
       " 'layers.14.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.7771e-03, -1.5498e-03, -2.6241e-04,  ...,  1.8304e-03,\n",
       "          -1.4625e-03, -8.0716e-04],\n",
       "         [-5.8652e-04, -3.5700e-03, -1.6736e-03,  ..., -3.9966e-04,\n",
       "           1.1369e-03,  1.8377e-03],\n",
       "         [ 4.4596e-04,  1.8342e-03, -3.4853e-03,  ...,  2.1094e-03,\n",
       "           5.0751e-03,  3.0620e-03],\n",
       "         ...,\n",
       "         [ 3.9433e-03,  1.0441e-03,  1.9780e-03,  ...,  1.2962e-03,\n",
       "          -1.9747e-03,  7.0656e-04],\n",
       "         [ 3.5318e-04,  2.4146e-03, -2.2531e-03,  ..., -7.1602e-05,\n",
       "           7.6997e-05, -1.2214e-04],\n",
       "         [-7.7108e-04,  6.4350e-04,  1.8953e-03,  ...,  3.8666e-04,\n",
       "          -2.2043e-03,  1.1943e-03]], requires_grad=True),\n",
       " 'layers.14.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.14.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.15.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0011,  0.0013, -0.0032,  ..., -0.0020,  0.0012,  0.0017],\n",
       "         [ 0.0006, -0.0014, -0.0022,  ...,  0.0039, -0.0053, -0.0007],\n",
       "         [-0.0009, -0.0005, -0.0036,  ...,  0.0025,  0.0037, -0.0003],\n",
       "         ...,\n",
       "         [-0.0020,  0.0022, -0.0026,  ...,  0.0042,  0.0021, -0.0026],\n",
       "         [-0.0028,  0.0026,  0.0011,  ...,  0.0025,  0.0002, -0.0002],\n",
       "         [ 0.0010, -0.0028, -0.0004,  ..., -0.0026, -0.0007, -0.0008]],\n",
       "        requires_grad=True),\n",
       " 'layers.15.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 5.1543e-03,  2.4107e-03,  2.2489e-04,  ...,  2.6908e-03,\n",
       "           1.3011e-03,  5.6449e-04],\n",
       "         [-1.4260e-05,  2.4575e-03,  1.8047e-03,  ...,  9.6474e-04,\n",
       "           9.4560e-04, -7.1423e-04],\n",
       "         [ 2.0164e-03,  1.0281e-03,  1.3670e-03,  ...,  1.3321e-03,\n",
       "           6.1948e-04, -1.6090e-03],\n",
       "         ...,\n",
       "         [ 2.5358e-03,  4.7523e-03, -4.2216e-04,  ...,  1.1211e-03,\n",
       "          -7.4260e-04,  4.3402e-03],\n",
       "         [-2.4989e-03,  2.1594e-04, -6.4063e-04,  ...,  1.2288e-03,\n",
       "           4.6124e-03, -8.7399e-04],\n",
       "         [-1.3902e-03,  1.7980e-03, -3.3743e-03,  ...,  8.5416e-04,\n",
       "           5.1245e-03, -1.1327e-05]], requires_grad=True),\n",
       " 'layers.15.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 9.2445e-04, -1.7319e-03, -3.2959e-03,  ...,  1.6676e-03,\n",
       "          -1.7101e-03,  9.2696e-04],\n",
       "         [-1.1371e-03,  8.2958e-04,  3.9152e-03,  ..., -4.1030e-04,\n",
       "           2.9836e-04,  2.2405e-03],\n",
       "         [-2.1557e-03, -1.2901e-03,  7.2519e-04,  ...,  1.3683e-03,\n",
       "           8.8984e-04, -1.5600e-03],\n",
       "         ...,\n",
       "         [-1.7476e-03, -3.0619e-03, -5.3246e-04,  ...,  9.2502e-04,\n",
       "           2.7039e-03, -2.6082e-04],\n",
       "         [-3.0416e-03, -5.6189e-04,  4.7485e-03,  ...,  1.1064e-03,\n",
       "           2.6921e-04,  6.0009e-03],\n",
       "         [ 1.1411e-03,  3.8593e-03, -3.6167e-03,  ...,  1.8510e-05,\n",
       "          -3.4549e-03,  4.0137e-04]], requires_grad=True),\n",
       " 'layers.15.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 3.9200e-03, -4.1033e-03, -1.1933e-03,  ..., -4.5672e-04,\n",
       "           3.2505e-06,  4.5876e-03],\n",
       "         [ 3.9288e-03, -1.8156e-03,  6.3453e-04,  ...,  2.9254e-03,\n",
       "          -3.0690e-04, -5.2086e-03],\n",
       "         [-6.2334e-03, -1.9536e-03,  3.8378e-03,  ...,  1.6001e-03,\n",
       "          -1.9296e-04, -6.1120e-04],\n",
       "         ...,\n",
       "         [-3.3731e-03,  3.3045e-03,  7.9001e-04,  ..., -4.4518e-03,\n",
       "          -8.5602e-04, -1.2668e-03],\n",
       "         [ 1.3500e-03, -3.6954e-03,  6.3477e-04,  ...,  1.5140e-03,\n",
       "           1.2257e-03,  6.0108e-04],\n",
       "         [-3.0134e-03, -1.2020e-03, -5.6100e-03,  ...,  1.9280e-03,\n",
       "          -6.7687e-04,  2.7606e-03]], requires_grad=True),\n",
       " 'layers.15.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-0.0050,  0.0010,  0.0004,  ...,  0.0040, -0.0017,  0.0015],\n",
       "         [-0.0008, -0.0012,  0.0017,  ...,  0.0006,  0.0006, -0.0054],\n",
       "         [ 0.0010, -0.0009,  0.0018,  ...,  0.0011,  0.0042, -0.0012],\n",
       "         ...,\n",
       "         [-0.0006, -0.0019,  0.0025,  ..., -0.0038, -0.0003, -0.0041],\n",
       "         [-0.0017, -0.0017,  0.0027,  ..., -0.0007, -0.0008, -0.0015],\n",
       "         [-0.0030, -0.0018, -0.0054,  ..., -0.0020, -0.0006,  0.0011]],\n",
       "        requires_grad=True),\n",
       " 'layers.15.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 3.8568e-04, -1.7850e-03, -1.2805e-03,  ...,  4.3564e-03,\n",
       "          -3.5177e-04, -2.2536e-03],\n",
       "         [-1.0909e-03, -4.2209e-03,  3.2041e-03,  ..., -3.6967e-04,\n",
       "          -1.9064e-03,  1.0326e-03],\n",
       "         [ 3.7131e-04,  7.1685e-05,  1.6011e-03,  ...,  2.4083e-03,\n",
       "           2.2486e-03,  1.1705e-03],\n",
       "         ...,\n",
       "         [-3.1579e-03,  1.9587e-03,  1.3050e-03,  ...,  1.5699e-03,\n",
       "           4.9152e-04,  4.4967e-04],\n",
       "         [-6.3789e-04, -4.7787e-03, -5.3696e-04,  ..., -2.7133e-03,\n",
       "          -8.1290e-04,  2.0078e-03],\n",
       "         [-4.7406e-03, -2.2517e-03, -1.5681e-03,  ...,  3.5363e-04,\n",
       "          -4.1820e-03,  5.8025e-03]], requires_grad=True),\n",
       " 'layers.15.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0009,  0.0042,  0.0023,  ...,  0.0018, -0.0013,  0.0023],\n",
       "         [-0.0004,  0.0003, -0.0001,  ...,  0.0015,  0.0011,  0.0020],\n",
       "         [-0.0038, -0.0001, -0.0016,  ...,  0.0025,  0.0037, -0.0002],\n",
       "         ...,\n",
       "         [ 0.0015, -0.0038,  0.0069,  ..., -0.0019,  0.0019, -0.0033],\n",
       "         [-0.0030,  0.0036, -0.0002,  ...,  0.0030, -0.0034,  0.0010],\n",
       "         [ 0.0002,  0.0026, -0.0002,  ...,  0.0035, -0.0018, -0.0023]],\n",
       "        requires_grad=True),\n",
       " 'layers.15.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.15.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.16.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-3.1263e-03, -1.2825e-03,  2.1209e-03,  ..., -1.4570e-03,\n",
       "          -3.3395e-03,  6.6339e-04],\n",
       "         [ 3.3945e-03, -1.3316e-03,  9.0097e-05,  ..., -1.0716e-03,\n",
       "           3.0967e-03, -7.8376e-04],\n",
       "         [-4.6292e-03, -5.5326e-04, -1.3920e-03,  ..., -2.7082e-03,\n",
       "           3.2203e-03,  2.7721e-03],\n",
       "         ...,\n",
       "         [ 5.5858e-04, -2.3832e-03,  2.5791e-04,  ...,  4.6047e-03,\n",
       "           1.4405e-03, -4.8457e-04],\n",
       "         [-1.8595e-03,  4.1117e-05,  2.1576e-03,  ..., -2.0359e-03,\n",
       "           5.5867e-04, -4.7836e-05],\n",
       "         [ 9.8310e-04, -1.7995e-03, -2.5539e-03,  ...,  2.2017e-03,\n",
       "           1.2691e-03, -4.9824e-03]], requires_grad=True),\n",
       " 'layers.16.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-1.5540e-03, -6.6878e-04, -9.0608e-04,  ...,  4.0252e-03,\n",
       "           5.5955e-04, -1.1394e-03],\n",
       "         [ 4.3128e-04,  1.0976e-03, -1.7244e-04,  ..., -6.3083e-05,\n",
       "          -7.3166e-04,  1.6108e-03],\n",
       "         [-1.7995e-03, -2.7120e-03,  3.9179e-03,  ...,  2.1141e-03,\n",
       "          -5.3406e-03,  1.2144e-03],\n",
       "         ...,\n",
       "         [ 1.6825e-03, -3.9150e-03, -4.0450e-03,  ..., -1.7822e-03,\n",
       "          -3.7584e-03, -3.4343e-03],\n",
       "         [-2.0355e-03, -2.9305e-03,  1.1334e-03,  ...,  1.2197e-03,\n",
       "          -7.7080e-04, -3.5238e-03],\n",
       "         [-1.2976e-03, -6.5825e-04,  2.2009e-03,  ..., -4.8890e-03,\n",
       "           7.9533e-04, -4.2921e-04]], requires_grad=True),\n",
       " 'layers.16.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0009,  0.0002,  0.0009,  ..., -0.0006, -0.0045, -0.0051],\n",
       "         [-0.0005, -0.0022, -0.0022,  ...,  0.0036,  0.0018, -0.0019],\n",
       "         [ 0.0010, -0.0006,  0.0016,  ...,  0.0007,  0.0028, -0.0040],\n",
       "         ...,\n",
       "         [ 0.0023, -0.0026, -0.0019,  ...,  0.0030, -0.0034,  0.0025],\n",
       "         [ 0.0008, -0.0007, -0.0002,  ...,  0.0049, -0.0008,  0.0006],\n",
       "         [ 0.0002,  0.0010,  0.0025,  ...,  0.0004,  0.0028, -0.0003]],\n",
       "        requires_grad=True),\n",
       " 'layers.16.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-2.4726e-03, -3.3931e-04, -7.8423e-03,  ..., -3.6932e-03,\n",
       "           2.4105e-03, -6.7569e-04],\n",
       "         [-6.4182e-03,  2.9121e-03,  8.7937e-04,  ..., -5.6601e-03,\n",
       "          -6.9508e-04, -6.5701e-04],\n",
       "         [-1.7604e-03,  2.6487e-03, -4.8448e-03,  ...,  2.9792e-03,\n",
       "          -1.0867e-03,  4.9195e-03],\n",
       "         ...,\n",
       "         [-5.4648e-03,  2.1269e-05, -4.6954e-04,  ..., -2.0876e-03,\n",
       "           8.3188e-04,  3.0714e-03],\n",
       "         [ 2.5100e-04, -1.3219e-03,  2.7401e-03,  ..., -1.0034e-03,\n",
       "          -2.4546e-03,  3.0453e-03],\n",
       "         [ 2.6537e-03, -1.0114e-03, -6.9629e-04,  ..., -3.3642e-03,\n",
       "          -3.0723e-03, -1.0477e-03]], requires_grad=True),\n",
       " 'layers.16.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-2.5518e-03,  5.7167e-04, -9.1859e-04,  ...,  3.0367e-03,\n",
       "          -2.4425e-03,  9.3418e-04],\n",
       "         [-1.8496e-03,  1.5407e-03, -1.6648e-03,  ..., -2.9574e-03,\n",
       "           3.8501e-03,  1.8078e-03],\n",
       "         [ 2.0661e-03,  3.4910e-03,  3.2916e-04,  ...,  1.1299e-03,\n",
       "          -9.4137e-04, -3.1617e-03],\n",
       "         ...,\n",
       "         [ 1.6708e-03, -2.1612e-03, -9.8766e-05,  ...,  9.7082e-04,\n",
       "          -4.7184e-04,  2.4055e-03],\n",
       "         [-1.0192e-03, -3.7614e-03, -1.1546e-03,  ...,  1.3011e-03,\n",
       "           4.1447e-04, -6.4960e-04],\n",
       "         [ 1.7656e-03,  9.5584e-04,  7.1760e-04,  ..., -4.0827e-03,\n",
       "          -1.6155e-03,  5.3970e-04]], requires_grad=True),\n",
       " 'layers.16.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 2.2985e-03,  2.4280e-03, -2.0068e-03,  ..., -1.0900e-03,\n",
       "           5.7159e-03, -3.6477e-03],\n",
       "         [ 5.8505e-03,  3.3461e-05,  6.4445e-04,  ...,  3.9983e-05,\n",
       "          -1.8473e-03,  6.9080e-03],\n",
       "         [ 2.2921e-03,  2.3564e-04, -8.5620e-04,  ...,  1.1541e-03,\n",
       "          -1.4588e-03,  1.8540e-03],\n",
       "         ...,\n",
       "         [-3.4568e-03, -3.8649e-03,  1.4251e-03,  ...,  1.9057e-03,\n",
       "           4.5886e-03,  2.0978e-03],\n",
       "         [ 3.9165e-03,  2.6871e-03, -4.8530e-03,  ...,  1.4427e-03,\n",
       "          -8.1147e-04, -2.4085e-03],\n",
       "         [-3.6816e-03, -1.1686e-03, -2.9676e-03,  ..., -1.9679e-04,\n",
       "           8.4908e-04,  1.2515e-03]], requires_grad=True),\n",
       " 'layers.16.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 1.7202e-03,  2.8332e-03, -4.3701e-03,  ..., -1.8420e-03,\n",
       "           4.0391e-03, -6.7334e-04],\n",
       "         [-2.0201e-03,  8.6420e-04,  4.9573e-03,  ..., -1.4709e-03,\n",
       "          -1.4020e-03, -4.3840e-03],\n",
       "         [-3.2789e-03, -2.7091e-03, -3.1555e-03,  ...,  3.6649e-04,\n",
       "           4.7064e-03,  5.5604e-06],\n",
       "         ...,\n",
       "         [ 2.8985e-03,  2.2624e-03,  4.6510e-03,  ...,  2.1995e-03,\n",
       "          -3.4840e-04,  2.4844e-03],\n",
       "         [ 3.3343e-03,  4.9183e-04, -1.6469e-03,  ...,  1.4231e-03,\n",
       "           2.2104e-05, -2.2305e-03],\n",
       "         [ 9.1108e-04, -2.7911e-04, -1.1352e-03,  ..., -1.8012e-03,\n",
       "          -3.0621e-03, -3.9265e-03]], requires_grad=True),\n",
       " 'layers.16.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.16.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.17.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-1.8656e-04, -3.1102e-03, -2.9337e-03,  ...,  2.5287e-04,\n",
       "           4.9456e-04,  2.4729e-04],\n",
       "         [-2.1670e-03, -1.9307e-04, -1.8268e-04,  ...,  1.6626e-03,\n",
       "           5.7795e-04, -1.6650e-03],\n",
       "         [ 8.9841e-04, -1.8290e-03, -8.0839e-04,  ..., -8.0341e-04,\n",
       "           1.5394e-03,  1.7763e-03],\n",
       "         ...,\n",
       "         [-1.9068e-03, -2.8204e-03, -3.2675e-04,  ..., -6.9321e-04,\n",
       "           3.7868e-03, -2.6511e-06],\n",
       "         [ 1.0546e-03,  2.2746e-03, -1.1480e-03,  ..., -2.8545e-03,\n",
       "           3.1969e-04,  1.3459e-03],\n",
       "         [ 3.7177e-03,  3.4917e-03,  7.4173e-04,  ...,  3.3316e-04,\n",
       "           1.7236e-03,  9.7007e-04]], requires_grad=True),\n",
       " 'layers.17.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0071, -0.0005,  0.0020,  ..., -0.0050,  0.0007,  0.0021],\n",
       "         [ 0.0040, -0.0022,  0.0025,  ..., -0.0014, -0.0004, -0.0012],\n",
       "         [ 0.0016,  0.0049,  0.0034,  ...,  0.0008, -0.0003, -0.0008],\n",
       "         ...,\n",
       "         [ 0.0014, -0.0013, -0.0024,  ...,  0.0020,  0.0036, -0.0032],\n",
       "         [-0.0024, -0.0033,  0.0010,  ..., -0.0003,  0.0017, -0.0048],\n",
       "         [-0.0008,  0.0044,  0.0003,  ...,  0.0010,  0.0002,  0.0009]],\n",
       "        requires_grad=True),\n",
       " 'layers.17.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 3.1224e-04,  3.0194e-04,  9.3122e-04,  ...,  8.1774e-04,\n",
       "           1.1172e-03, -2.8324e-03],\n",
       "         [-1.0160e-03, -4.2620e-03, -2.0661e-03,  ...,  4.0517e-03,\n",
       "           5.3308e-03, -2.5985e-03],\n",
       "         [ 1.3554e-03, -4.5167e-04,  1.5536e-03,  ...,  2.5625e-03,\n",
       "          -2.5308e-03, -3.1692e-03],\n",
       "         ...,\n",
       "         [ 3.3142e-04, -7.6825e-04,  2.6795e-03,  ...,  3.0366e-03,\n",
       "           1.4890e-03, -2.8513e-03],\n",
       "         [ 1.3786e-03, -5.7562e-03, -1.2661e-03,  ...,  1.2835e-04,\n",
       "           1.3049e-03,  3.5543e-03],\n",
       "         [ 1.1232e-03,  5.0286e-03, -1.8246e-03,  ..., -3.4210e-05,\n",
       "          -9.2196e-05,  3.8230e-04]], requires_grad=True),\n",
       " 'layers.17.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0006, -0.0011, -0.0003,  ..., -0.0030,  0.0024,  0.0010],\n",
       "         [-0.0013,  0.0050,  0.0040,  ..., -0.0014, -0.0040,  0.0001],\n",
       "         [-0.0031, -0.0008,  0.0030,  ...,  0.0002,  0.0004,  0.0011],\n",
       "         ...,\n",
       "         [ 0.0014,  0.0025, -0.0003,  ...,  0.0027, -0.0028, -0.0045],\n",
       "         [-0.0039, -0.0016,  0.0019,  ...,  0.0006,  0.0012, -0.0004],\n",
       "         [ 0.0026, -0.0002, -0.0008,  ...,  0.0037,  0.0002,  0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.17.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 1.7336e-03,  7.6111e-05, -2.7721e-03,  ...,  1.7486e-03,\n",
       "           1.1983e-03, -1.6676e-03],\n",
       "         [-1.6221e-03,  2.1203e-04,  3.4881e-03,  ..., -2.0664e-03,\n",
       "           4.2218e-03,  2.5902e-03],\n",
       "         [-5.2437e-04,  2.5338e-03, -2.0854e-03,  ...,  2.5910e-03,\n",
       "           1.1052e-03, -3.9301e-03],\n",
       "         ...,\n",
       "         [ 2.5706e-03, -3.3611e-04,  1.6764e-03,  ..., -5.4246e-03,\n",
       "           2.7254e-04,  1.8955e-03],\n",
       "         [-1.6184e-03, -1.7028e-03, -5.2483e-04,  ..., -2.2759e-03,\n",
       "           2.6923e-03,  2.5185e-03],\n",
       "         [-5.3839e-04, -8.3031e-04,  1.3172e-03,  ...,  1.2845e-03,\n",
       "           4.0326e-03,  2.9829e-03]], requires_grad=True),\n",
       " 'layers.17.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 1.2504e-03,  8.3291e-06, -1.2479e-03,  ..., -1.5208e-04,\n",
       "           1.4735e-03,  3.3595e-03],\n",
       "         [ 2.6626e-03,  1.3151e-03, -1.0519e-03,  ..., -8.4600e-04,\n",
       "          -2.0145e-03,  4.0216e-03],\n",
       "         [-1.5862e-03, -1.1410e-04, -7.3636e-03,  ..., -2.5566e-03,\n",
       "          -3.9911e-03,  4.7979e-04],\n",
       "         ...,\n",
       "         [-1.4220e-03,  5.0031e-04, -1.2737e-03,  ..., -6.1431e-03,\n",
       "          -1.0789e-03,  8.7253e-04],\n",
       "         [ 4.5572e-03,  2.2258e-03, -1.8727e-04,  ..., -1.0967e-04,\n",
       "           2.8373e-03,  2.0259e-03],\n",
       "         [-5.2108e-03, -1.5668e-03, -1.3604e-03,  ..., -1.8637e-03,\n",
       "           8.7444e-04, -1.0804e-03]], requires_grad=True),\n",
       " 'layers.17.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.9871e-03,  3.6985e-03,  5.3406e-03,  ..., -1.5594e-03,\n",
       "           1.6833e-03,  4.4714e-05],\n",
       "         [-4.2095e-03,  1.9201e-03, -1.7549e-03,  ..., -4.9118e-04,\n",
       "          -2.0139e-03, -2.4136e-04],\n",
       "         [-2.1250e-03,  1.6855e-03, -1.1921e-03,  ..., -2.5488e-03,\n",
       "          -2.6027e-03,  5.7241e-04],\n",
       "         ...,\n",
       "         [ 6.4498e-03,  3.1912e-03,  5.7161e-04,  ..., -1.3316e-03,\n",
       "          -1.9846e-03, -1.7971e-03],\n",
       "         [-2.6882e-03,  2.2646e-03,  1.6854e-03,  ..., -5.0274e-03,\n",
       "           2.5268e-03, -3.8030e-03],\n",
       "         [-5.2777e-04,  2.1290e-03,  1.0103e-03,  ...,  1.0731e-03,\n",
       "          -4.2863e-03, -7.3136e-04]], requires_grad=True),\n",
       " 'layers.17.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.17.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.18.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-1.1941e-03, -6.6071e-04, -2.7527e-03,  ...,  3.4835e-04,\n",
       "          -1.1970e-03,  6.4215e-03],\n",
       "         [-1.2304e-04, -2.8102e-03, -5.8037e-04,  ...,  3.2950e-03,\n",
       "           6.0031e-04, -2.3146e-03],\n",
       "         [-2.6252e-03,  6.2159e-04, -1.0912e-03,  ...,  2.4661e-03,\n",
       "          -2.3648e-03,  8.3026e-05],\n",
       "         ...,\n",
       "         [-2.9141e-04, -3.5088e-03, -1.6274e-03,  ...,  5.5779e-03,\n",
       "           5.9623e-04,  2.4335e-03],\n",
       "         [-2.9689e-05,  8.4101e-04,  1.9266e-03,  ...,  2.6130e-03,\n",
       "          -1.2615e-03,  1.2865e-03],\n",
       "         [-5.0242e-03, -3.8675e-04,  8.5876e-04,  ..., -1.7519e-03,\n",
       "          -1.3563e-03,  6.9809e-04]], requires_grad=True),\n",
       " 'layers.18.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0007, -0.0002, -0.0015,  ...,  0.0022, -0.0003, -0.0020],\n",
       "         [-0.0041, -0.0003, -0.0018,  ..., -0.0017, -0.0023, -0.0018],\n",
       "         [ 0.0020, -0.0017,  0.0007,  ..., -0.0012,  0.0017,  0.0003],\n",
       "         ...,\n",
       "         [-0.0021,  0.0016,  0.0015,  ...,  0.0002, -0.0036, -0.0019],\n",
       "         [ 0.0025, -0.0002,  0.0018,  ...,  0.0022, -0.0009, -0.0004],\n",
       "         [-0.0009, -0.0004, -0.0002,  ..., -0.0049,  0.0012, -0.0003]],\n",
       "        requires_grad=True),\n",
       " 'layers.18.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 6.8862e-04, -2.3867e-03,  7.4570e-04,  ...,  6.7261e-03,\n",
       "          -6.6888e-05, -4.2358e-04],\n",
       "         [-9.0334e-04, -2.1066e-03, -5.5766e-04,  ..., -2.0313e-04,\n",
       "          -7.1215e-04, -1.0325e-04],\n",
       "         [ 8.5027e-04,  1.2255e-03,  3.3502e-03,  ..., -7.0253e-04,\n",
       "           1.1537e-04, -2.3939e-03],\n",
       "         ...,\n",
       "         [ 4.7195e-04, -2.4844e-03,  1.1269e-03,  ..., -2.7636e-03,\n",
       "           3.7585e-03, -1.7895e-04],\n",
       "         [-2.1082e-03,  1.5728e-03, -6.0458e-03,  ...,  3.7015e-03,\n",
       "           1.5213e-03, -1.6950e-03],\n",
       "         [ 1.3139e-04,  4.0724e-04,  2.7831e-04,  ...,  5.4324e-04,\n",
       "          -7.6864e-04, -1.3821e-03]], requires_grad=True),\n",
       " 'layers.18.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-4.8108e-03, -2.9686e-03, -1.2193e-03,  ..., -7.7143e-04,\n",
       "           1.5290e-04,  2.1887e-03],\n",
       "         [ 5.9784e-04, -7.6866e-04,  5.4371e-04,  ..., -2.4060e-03,\n",
       "          -1.6089e-05,  4.9666e-04],\n",
       "         [ 7.8817e-04,  5.6465e-05, -3.9215e-03,  ...,  1.1516e-03,\n",
       "          -1.8130e-03,  4.9724e-03],\n",
       "         ...,\n",
       "         [ 1.5829e-03, -9.2254e-05, -4.9231e-03,  ..., -2.1821e-03,\n",
       "          -5.9283e-03, -1.7907e-03],\n",
       "         [-2.4765e-03,  2.0766e-03, -1.6578e-03,  ...,  4.4848e-04,\n",
       "           3.3117e-03,  1.6298e-04],\n",
       "         [ 2.0613e-03,  2.4005e-03,  9.0622e-04,  ..., -1.4243e-03,\n",
       "          -2.4817e-04, -3.9591e-03]], requires_grad=True),\n",
       " 'layers.18.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 3.4987e-03, -2.3633e-03, -7.0689e-04,  ..., -5.7021e-03,\n",
       "           2.1399e-03,  9.0547e-05],\n",
       "         [ 4.7037e-04,  1.3179e-04, -1.0256e-03,  ...,  2.2898e-03,\n",
       "          -2.4586e-03, -9.7481e-03],\n",
       "         [-2.0460e-03, -2.0767e-03, -1.0590e-03,  ..., -3.1632e-03,\n",
       "           2.9181e-03, -1.3480e-03],\n",
       "         ...,\n",
       "         [ 1.1869e-03,  1.8953e-03, -9.5549e-04,  ...,  2.6727e-03,\n",
       "           3.1292e-03,  1.5890e-03],\n",
       "         [ 2.8417e-04, -4.8714e-03,  3.3012e-03,  ..., -1.5488e-03,\n",
       "           4.2543e-03,  9.2824e-04],\n",
       "         [ 2.0759e-03,  2.5857e-04, -2.6500e-03,  ...,  1.5060e-04,\n",
       "           1.2294e-03,  3.9399e-03]], requires_grad=True),\n",
       " 'layers.18.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-4.6542e-04,  4.1071e-03, -1.6043e-04,  ...,  1.3620e-03,\n",
       "          -1.8883e-04,  1.0115e-03],\n",
       "         [-2.9936e-03, -2.2684e-03, -1.2150e-04,  ...,  7.9323e-04,\n",
       "          -4.0876e-04, -1.0958e-03],\n",
       "         [-4.4842e-03,  1.7020e-03, -8.6166e-04,  ..., -1.4007e-03,\n",
       "          -1.7369e-03,  1.7661e-03],\n",
       "         ...,\n",
       "         [-6.6011e-03, -3.8498e-04, -1.9819e-03,  ..., -4.5550e-03,\n",
       "          -1.2053e-04, -1.3819e-03],\n",
       "         [ 3.7380e-03, -1.4733e-03, -5.7062e-04,  ..., -3.1047e-03,\n",
       "          -1.0310e-03,  3.9287e-04],\n",
       "         [ 7.8315e-04,  4.9417e-05,  7.9433e-04,  ..., -1.6999e-03,\n",
       "          -9.3091e-05, -6.6324e-04]], requires_grad=True),\n",
       " 'layers.18.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0027,  0.0010,  0.0012,  ..., -0.0018, -0.0007, -0.0005],\n",
       "         [ 0.0030,  0.0052,  0.0016,  ..., -0.0014,  0.0014,  0.0002],\n",
       "         [ 0.0015,  0.0007,  0.0019,  ...,  0.0009,  0.0025,  0.0052],\n",
       "         ...,\n",
       "         [ 0.0010,  0.0008, -0.0042,  ..., -0.0031, -0.0007,  0.0024],\n",
       "         [-0.0001,  0.0002, -0.0023,  ..., -0.0040, -0.0018,  0.0013],\n",
       "         [ 0.0005,  0.0004,  0.0022,  ..., -0.0034, -0.0010, -0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.18.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.18.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.19.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0033,  0.0028, -0.0011,  ...,  0.0014, -0.0015, -0.0020],\n",
       "         [ 0.0014,  0.0060,  0.0040,  ..., -0.0034, -0.0013, -0.0013],\n",
       "         [-0.0029,  0.0012, -0.0038,  ...,  0.0014,  0.0007, -0.0038],\n",
       "         ...,\n",
       "         [ 0.0011, -0.0005, -0.0019,  ..., -0.0006, -0.0036, -0.0021],\n",
       "         [-0.0015, -0.0008,  0.0007,  ...,  0.0072,  0.0002, -0.0007],\n",
       "         [ 0.0037, -0.0001,  0.0021,  ..., -0.0029, -0.0029,  0.0015]],\n",
       "        requires_grad=True),\n",
       " 'layers.19.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0018, -0.0060,  0.0008,  ...,  0.0036,  0.0025, -0.0005],\n",
       "         [ 0.0031,  0.0015, -0.0006,  ...,  0.0029,  0.0014, -0.0003],\n",
       "         [-0.0015, -0.0022,  0.0005,  ..., -0.0010,  0.0022,  0.0030],\n",
       "         ...,\n",
       "         [ 0.0020,  0.0019, -0.0016,  ..., -0.0024, -0.0022,  0.0024],\n",
       "         [-0.0009,  0.0025, -0.0005,  ...,  0.0009,  0.0029, -0.0024],\n",
       "         [-0.0023, -0.0044, -0.0008,  ..., -0.0007, -0.0008,  0.0036]],\n",
       "        requires_grad=True),\n",
       " 'layers.19.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-3.6432e-04,  5.9376e-04,  7.5737e-03,  ..., -9.3771e-05,\n",
       "           9.6867e-04,  3.0677e-03],\n",
       "         [ 6.4434e-04, -1.1604e-04,  6.9803e-04,  ...,  4.5270e-03,\n",
       "           1.5849e-03, -2.2895e-03],\n",
       "         [-2.7085e-04, -9.5600e-04,  5.7173e-05,  ..., -1.1137e-03,\n",
       "           7.8599e-06, -2.1433e-03],\n",
       "         ...,\n",
       "         [ 1.7731e-03, -8.9498e-04, -3.4654e-03,  ..., -3.0208e-03,\n",
       "           2.4617e-03,  5.8530e-04],\n",
       "         [ 3.0269e-03,  8.0973e-05, -1.0088e-03,  ...,  1.9609e-03,\n",
       "           2.2161e-03,  1.9082e-04],\n",
       "         [ 1.7667e-04,  6.0996e-04,  1.1973e-05,  ..., -5.8322e-04,\n",
       "          -4.6507e-03, -3.9580e-03]], requires_grad=True),\n",
       " 'layers.19.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-2.1603e-03,  1.9498e-03, -3.8057e-03,  ...,  3.2437e-03,\n",
       "          -9.5521e-04, -1.7132e-04],\n",
       "         [ 4.4036e-03,  2.5508e-03,  6.1400e-04,  ..., -2.6105e-03,\n",
       "           1.4400e-03, -3.3767e-05],\n",
       "         [ 1.8683e-03, -6.1902e-04,  8.5406e-04,  ...,  8.5608e-04,\n",
       "          -7.1579e-04,  3.1942e-04],\n",
       "         ...,\n",
       "         [ 6.7090e-04, -8.2729e-04,  7.5685e-04,  ...,  2.5718e-03,\n",
       "          -3.4206e-03,  7.8921e-04],\n",
       "         [ 2.1728e-03, -9.5414e-04,  1.6371e-03,  ...,  1.9478e-03,\n",
       "          -3.2025e-03,  8.8105e-04],\n",
       "         [ 1.3077e-03,  2.1335e-03, -5.2042e-04,  ..., -5.5297e-04,\n",
       "          -4.3865e-04,  1.7322e-03]], requires_grad=True),\n",
       " 'layers.19.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 6.0253e-04, -9.9492e-04,  5.8727e-04,  ...,  6.5180e-04,\n",
       "          -7.0200e-04,  9.8712e-05],\n",
       "         [ 1.7711e-03,  1.7921e-03, -2.8485e-03,  ..., -6.9529e-04,\n",
       "          -6.0116e-04,  3.4724e-03],\n",
       "         [-8.9826e-04,  1.0089e-03, -1.6725e-03,  ..., -8.5407e-04,\n",
       "          -9.8556e-04, -2.5390e-03],\n",
       "         ...,\n",
       "         [-4.4782e-03, -4.7166e-03, -3.4180e-03,  ..., -2.3057e-03,\n",
       "          -1.7923e-03, -3.8712e-04],\n",
       "         [ 2.6033e-03, -2.5040e-03,  1.6219e-03,  ..., -1.2452e-03,\n",
       "          -2.9780e-03,  2.2949e-03],\n",
       "         [-5.6709e-04,  5.5067e-03,  2.0595e-03,  ...,  4.5806e-03,\n",
       "           1.8137e-03,  2.2846e-04]], requires_grad=True),\n",
       " 'layers.19.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0017, -0.0035, -0.0013,  ..., -0.0026,  0.0014, -0.0011],\n",
       "         [-0.0003,  0.0033, -0.0003,  ...,  0.0013, -0.0028, -0.0016],\n",
       "         [-0.0043, -0.0007,  0.0021,  ..., -0.0044, -0.0016, -0.0007],\n",
       "         ...,\n",
       "         [ 0.0012,  0.0011, -0.0015,  ...,  0.0005, -0.0018, -0.0014],\n",
       "         [ 0.0036, -0.0002,  0.0028,  ...,  0.0024, -0.0014, -0.0002],\n",
       "         [-0.0043,  0.0004,  0.0007,  ..., -0.0005, -0.0039,  0.0034]],\n",
       "        requires_grad=True),\n",
       " 'layers.19.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0023, -0.0041, -0.0006,  ...,  0.0014,  0.0037,  0.0019],\n",
       "         [ 0.0021,  0.0023,  0.0033,  ...,  0.0033, -0.0004,  0.0009],\n",
       "         [ 0.0011, -0.0059,  0.0015,  ..., -0.0016,  0.0002,  0.0013],\n",
       "         ...,\n",
       "         [-0.0025,  0.0009, -0.0040,  ..., -0.0012, -0.0011,  0.0011],\n",
       "         [ 0.0016,  0.0037, -0.0009,  ..., -0.0034, -0.0012, -0.0012],\n",
       "         [-0.0006,  0.0005, -0.0024,  ..., -0.0006, -0.0001, -0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.19.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.19.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.20.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0024,  0.0029, -0.0009,  ...,  0.0047, -0.0028, -0.0001],\n",
       "         [ 0.0016,  0.0030,  0.0015,  ..., -0.0003, -0.0001, -0.0034],\n",
       "         [-0.0002, -0.0007,  0.0021,  ...,  0.0032,  0.0018,  0.0009],\n",
       "         ...,\n",
       "         [ 0.0035,  0.0009, -0.0010,  ..., -0.0046,  0.0016,  0.0005],\n",
       "         [ 0.0017,  0.0007, -0.0041,  ...,  0.0006, -0.0046, -0.0008],\n",
       "         [ 0.0042, -0.0015,  0.0006,  ..., -0.0029, -0.0010,  0.0046]],\n",
       "        requires_grad=True),\n",
       " 'layers.20.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-8.7094e-03, -2.6539e-04,  2.8961e-05,  ...,  9.0085e-04,\n",
       "           1.4110e-05,  4.5349e-04],\n",
       "         [ 1.3336e-03, -1.7741e-03, -7.7415e-05,  ...,  2.8973e-03,\n",
       "           2.3174e-03, -2.7051e-03],\n",
       "         [-1.7951e-03, -1.3695e-03, -5.6106e-04,  ...,  2.3520e-03,\n",
       "          -5.8891e-05,  7.6323e-03],\n",
       "         ...,\n",
       "         [ 1.0682e-03, -1.6614e-03,  2.8278e-04,  ...,  2.0088e-03,\n",
       "          -2.2169e-03, -6.4347e-04],\n",
       "         [ 6.2859e-03,  2.1831e-03, -2.3380e-03,  ...,  9.3351e-04,\n",
       "           1.6556e-03, -1.4156e-04],\n",
       "         [ 5.0093e-03,  2.1328e-04,  1.7390e-03,  ...,  4.6805e-04,\n",
       "          -1.0130e-03,  3.3724e-04]], requires_grad=True),\n",
       " 'layers.20.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-0.0019, -0.0031, -0.0030,  ..., -0.0019,  0.0015,  0.0035],\n",
       "         [ 0.0010,  0.0005, -0.0005,  ..., -0.0022,  0.0014,  0.0019],\n",
       "         [ 0.0005,  0.0024, -0.0006,  ...,  0.0027, -0.0001,  0.0032],\n",
       "         ...,\n",
       "         [-0.0018,  0.0033,  0.0053,  ...,  0.0044, -0.0019, -0.0026],\n",
       "         [ 0.0044, -0.0042,  0.0047,  ..., -0.0014, -0.0026, -0.0015],\n",
       "         [ 0.0022,  0.0003,  0.0014,  ...,  0.0022, -0.0008, -0.0023]],\n",
       "        requires_grad=True),\n",
       " 'layers.20.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0026,  0.0023,  0.0043,  ...,  0.0030, -0.0008, -0.0012],\n",
       "         [-0.0003,  0.0054,  0.0007,  ..., -0.0005,  0.0020, -0.0003],\n",
       "         [ 0.0016, -0.0021, -0.0023,  ..., -0.0003,  0.0007,  0.0014],\n",
       "         ...,\n",
       "         [-0.0019, -0.0049, -0.0016,  ..., -0.0004,  0.0036,  0.0030],\n",
       "         [ 0.0025,  0.0004, -0.0007,  ...,  0.0054, -0.0027, -0.0004],\n",
       "         [-0.0007, -0.0031, -0.0007,  ...,  0.0004,  0.0007,  0.0013]],\n",
       "        requires_grad=True),\n",
       " 'layers.20.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0040,  0.0005, -0.0030,  ...,  0.0017,  0.0007, -0.0030],\n",
       "         [-0.0055,  0.0002, -0.0019,  ...,  0.0003,  0.0046,  0.0040],\n",
       "         [-0.0023, -0.0006, -0.0003,  ...,  0.0014,  0.0011, -0.0013],\n",
       "         ...,\n",
       "         [ 0.0024,  0.0036,  0.0017,  ...,  0.0016, -0.0023,  0.0031],\n",
       "         [ 0.0026,  0.0004, -0.0011,  ..., -0.0036, -0.0031, -0.0025],\n",
       "         [-0.0013,  0.0013, -0.0018,  ..., -0.0016, -0.0003, -0.0019]],\n",
       "        requires_grad=True),\n",
       " 'layers.20.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 1.3804e-03, -1.0624e-03, -3.5411e-03,  ...,  1.2878e-04,\n",
       "          -4.0291e-03,  1.6314e-03],\n",
       "         [-4.1824e-03, -3.8909e-03, -1.9340e-03,  ...,  8.8013e-04,\n",
       "           2.9237e-03, -2.5152e-03],\n",
       "         [-2.3507e-03,  8.7966e-04,  4.7064e-05,  ..., -2.2091e-03,\n",
       "           6.6532e-03, -4.5705e-03],\n",
       "         ...,\n",
       "         [-1.9299e-03,  2.8477e-03,  1.5540e-03,  ..., -5.5129e-03,\n",
       "          -3.7981e-03,  1.6794e-03],\n",
       "         [-1.8367e-04,  2.2983e-03, -1.1063e-03,  ...,  4.3337e-03,\n",
       "           5.2956e-03,  5.2743e-04],\n",
       "         [-4.5641e-04,  3.4245e-03,  3.0886e-03,  ..., -3.2077e-03,\n",
       "           1.7172e-03,  6.7553e-03]], requires_grad=True),\n",
       " 'layers.20.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0005,  0.0008, -0.0017,  ..., -0.0012, -0.0035,  0.0023],\n",
       "         [-0.0034,  0.0012,  0.0010,  ...,  0.0027,  0.0005, -0.0021],\n",
       "         [-0.0012, -0.0043,  0.0020,  ...,  0.0004, -0.0038, -0.0018],\n",
       "         ...,\n",
       "         [-0.0005,  0.0013, -0.0034,  ...,  0.0022, -0.0014,  0.0036],\n",
       "         [-0.0008, -0.0018, -0.0030,  ..., -0.0025,  0.0022, -0.0041],\n",
       "         [ 0.0008,  0.0048,  0.0002,  ..., -0.0004, -0.0010, -0.0004]],\n",
       "        requires_grad=True),\n",
       " 'layers.20.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.20.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.21.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 2.6056e-03, -1.4418e-03, -2.2513e-03,  ...,  6.8473e-04,\n",
       "           5.5210e-03, -2.2528e-03],\n",
       "         [ 9.6573e-04, -1.0430e-04,  6.5531e-04,  ...,  5.6839e-04,\n",
       "           3.9582e-03, -3.0995e-04],\n",
       "         [-3.2839e-03,  3.3588e-03,  4.3867e-03,  ..., -3.9285e-03,\n",
       "           3.1411e-03,  4.3720e-03],\n",
       "         ...,\n",
       "         [ 2.9944e-03, -1.5458e-03, -1.2899e-04,  ..., -1.1652e-03,\n",
       "          -1.8090e-03,  4.3730e-03],\n",
       "         [ 3.5755e-03,  3.6597e-03, -3.0902e-03,  ..., -2.6940e-03,\n",
       "          -2.3853e-03, -8.1016e-05],\n",
       "         [ 4.6645e-03, -8.8689e-04,  2.1614e-03,  ..., -4.0677e-03,\n",
       "          -4.7478e-03, -2.4101e-03]], requires_grad=True),\n",
       " 'layers.21.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0032,  0.0017, -0.0038,  ...,  0.0022, -0.0019,  0.0018],\n",
       "         [-0.0002,  0.0040, -0.0014,  ...,  0.0011,  0.0015,  0.0048],\n",
       "         [ 0.0018, -0.0027,  0.0020,  ..., -0.0015,  0.0006,  0.0045],\n",
       "         ...,\n",
       "         [ 0.0012,  0.0028,  0.0024,  ..., -0.0024,  0.0036, -0.0048],\n",
       "         [-0.0030,  0.0014,  0.0065,  ..., -0.0001,  0.0023,  0.0007],\n",
       "         [ 0.0009,  0.0021, -0.0033,  ..., -0.0014, -0.0022,  0.0023]],\n",
       "        requires_grad=True),\n",
       " 'layers.21.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 1.1586e-03, -6.4966e-05,  2.1713e-03,  ...,  9.7620e-05,\n",
       "          -8.1818e-04,  1.7268e-03],\n",
       "         [ 1.6296e-04, -5.5662e-03,  2.8731e-03,  ...,  4.5803e-04,\n",
       "          -4.8642e-03,  1.4318e-03],\n",
       "         [-1.4006e-03, -2.3839e-03, -2.0193e-03,  ...,  3.8960e-03,\n",
       "          -4.3358e-04,  1.7493e-03],\n",
       "         ...,\n",
       "         [-1.0127e-03, -1.1813e-03,  2.6798e-04,  ...,  6.9236e-04,\n",
       "           1.1523e-03,  2.7184e-03],\n",
       "         [-4.0857e-03, -9.7118e-05,  9.5605e-04,  ..., -1.3183e-03,\n",
       "           1.0231e-03,  5.1430e-03],\n",
       "         [-5.9477e-05,  3.9409e-04,  1.0536e-03,  ..., -1.0276e-04,\n",
       "          -8.8792e-04, -3.0655e-03]], requires_grad=True),\n",
       " 'layers.21.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-7.6324e-04, -2.1206e-03, -3.4569e-04,  ...,  5.0061e-04,\n",
       "           1.4429e-03,  1.9326e-03],\n",
       "         [-5.9578e-03, -1.2392e-03, -3.4695e-04,  ..., -2.8422e-03,\n",
       "           1.2929e-05, -3.5610e-03],\n",
       "         [-2.7640e-03, -4.6859e-04, -5.2603e-03,  ...,  3.6195e-03,\n",
       "           1.8916e-03, -1.8453e-03],\n",
       "         ...,\n",
       "         [-4.7076e-03, -3.1959e-03, -2.3313e-03,  ...,  4.0657e-04,\n",
       "           2.9818e-03,  4.7502e-03],\n",
       "         [-1.0596e-04, -2.7085e-03, -3.1931e-03,  ...,  1.9858e-03,\n",
       "           7.3566e-03, -2.3787e-04],\n",
       "         [ 3.4975e-03,  2.4080e-03,  3.8663e-04,  ..., -1.7913e-03,\n",
       "          -1.0385e-03, -6.5463e-05]], requires_grad=True),\n",
       " 'layers.21.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-1.6214e-03,  2.2720e-03,  2.1008e-03,  ..., -8.8783e-04,\n",
       "           1.5649e-04, -2.9783e-03],\n",
       "         [ 2.7580e-03, -1.1895e-03,  1.6850e-03,  ...,  8.4229e-04,\n",
       "          -8.6330e-04, -1.5073e-04],\n",
       "         [ 6.3351e-04,  3.1845e-03,  9.4368e-04,  ...,  1.7785e-03,\n",
       "          -5.6787e-03, -1.3070e-03],\n",
       "         ...,\n",
       "         [ 3.5030e-03,  2.6929e-04,  3.6522e-03,  ..., -1.5499e-03,\n",
       "           4.8886e-04, -1.4397e-03],\n",
       "         [ 5.0339e-03,  6.8768e-04,  1.2580e-03,  ...,  7.5331e-04,\n",
       "          -1.9098e-03,  3.4028e-03],\n",
       "         [ 3.1799e-05, -6.3336e-04,  3.9734e-03,  ...,  6.3608e-04,\n",
       "           3.2769e-03, -1.5148e-03]], requires_grad=True),\n",
       " 'layers.21.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0017,  0.0041, -0.0022,  ...,  0.0005,  0.0014, -0.0006],\n",
       "         [ 0.0008, -0.0009, -0.0004,  ..., -0.0002, -0.0020, -0.0012],\n",
       "         [-0.0007, -0.0033, -0.0002,  ...,  0.0028, -0.0045, -0.0024],\n",
       "         ...,\n",
       "         [ 0.0005,  0.0042, -0.0005,  ..., -0.0062, -0.0007,  0.0024],\n",
       "         [-0.0016, -0.0047,  0.0013,  ...,  0.0032,  0.0038,  0.0005],\n",
       "         [ 0.0027,  0.0002,  0.0010,  ...,  0.0022, -0.0018,  0.0020]],\n",
       "        requires_grad=True),\n",
       " 'layers.21.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 9.7885e-04,  2.1579e-03,  2.4373e-03,  ..., -4.8117e-03,\n",
       "          -2.9092e-03,  3.2128e-04],\n",
       "         [ 5.9535e-03,  4.2093e-03,  6.3697e-04,  ...,  8.4010e-04,\n",
       "           3.0318e-03, -3.2044e-03],\n",
       "         [ 3.5570e-03,  2.2823e-03, -3.2708e-03,  ...,  1.8442e-04,\n",
       "           3.5301e-03,  4.5992e-05],\n",
       "         ...,\n",
       "         [-1.8445e-03,  2.0679e-03,  6.4893e-04,  ..., -2.7558e-03,\n",
       "          -8.8577e-04, -1.7999e-03],\n",
       "         [-1.1735e-03, -1.0346e-03, -2.8830e-03,  ..., -2.1832e-03,\n",
       "           1.5211e-03, -4.0564e-03],\n",
       "         [-5.1383e-04, -1.4377e-03,  2.8779e-03,  ..., -3.0944e-04,\n",
       "          -1.4740e-03,  2.9728e-03]], requires_grad=True),\n",
       " 'layers.21.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.21.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.22.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-5.3503e-03, -1.8780e-03,  5.8409e-04,  ..., -3.3419e-03,\n",
       "           3.5126e-03,  9.3520e-04],\n",
       "         [-1.5008e-03,  2.0758e-03,  4.4165e-03,  ..., -2.0804e-03,\n",
       "          -2.9405e-03, -1.3261e-04],\n",
       "         [ 6.8474e-05,  1.4632e-03, -1.5720e-03,  ...,  2.9788e-03,\n",
       "           3.3448e-03,  1.1847e-03],\n",
       "         ...,\n",
       "         [ 1.6628e-03, -2.3978e-03, -3.2062e-04,  ...,  2.5425e-03,\n",
       "           2.5851e-03,  1.2143e-03],\n",
       "         [-1.8970e-03, -1.4436e-03, -4.9444e-04,  ..., -1.0228e-03,\n",
       "          -3.4184e-06,  2.6845e-03],\n",
       "         [ 1.8712e-03,  3.8281e-03,  8.7175e-04,  ...,  3.2186e-03,\n",
       "           4.3934e-03,  2.9308e-04]], requires_grad=True),\n",
       " 'layers.22.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0010,  0.0047, -0.0033,  ..., -0.0020, -0.0029, -0.0056],\n",
       "         [-0.0037,  0.0048,  0.0029,  ..., -0.0025, -0.0003, -0.0002],\n",
       "         [-0.0011, -0.0010,  0.0031,  ...,  0.0008, -0.0027,  0.0043],\n",
       "         ...,\n",
       "         [ 0.0005, -0.0011, -0.0042,  ..., -0.0002, -0.0031,  0.0006],\n",
       "         [-0.0053,  0.0062,  0.0022,  ..., -0.0043, -0.0025,  0.0016],\n",
       "         [ 0.0007,  0.0026,  0.0032,  ..., -0.0010, -0.0002, -0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.22.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-4.3662e-03,  1.2563e-03, -5.5859e-03,  ..., -3.0820e-04,\n",
       "          -2.2264e-03,  2.2962e-03],\n",
       "         [ 8.6127e-04,  4.6276e-03,  4.1096e-03,  ..., -6.3180e-04,\n",
       "          -5.3114e-04,  3.1943e-03],\n",
       "         [ 2.4273e-03,  1.1393e-03, -2.3731e-04,  ..., -2.0181e-03,\n",
       "           7.7430e-04, -2.0325e-03],\n",
       "         ...,\n",
       "         [ 1.7185e-03, -2.0621e-03,  7.0475e-04,  ...,  1.8053e-03,\n",
       "           2.2031e-03, -3.5692e-03],\n",
       "         [ 3.3725e-03,  6.5766e-04, -1.7468e-03,  ...,  1.5415e-03,\n",
       "           2.1632e-03,  4.2281e-04],\n",
       "         [-1.1014e-03,  3.9519e-05,  1.4694e-03,  ...,  1.3941e-03,\n",
       "           1.0542e-03, -4.5012e-03]], requires_grad=True),\n",
       " 'layers.22.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 7.6266e-05,  1.0811e-03,  2.2594e-03,  ...,  4.8166e-03,\n",
       "           3.0856e-03,  4.9168e-03],\n",
       "         [ 6.5898e-04,  5.2584e-04,  9.4217e-04,  ..., -1.6508e-03,\n",
       "          -2.2147e-03, -2.1442e-03],\n",
       "         [ 1.3917e-03, -5.7826e-04,  4.1412e-03,  ...,  1.9019e-03,\n",
       "          -2.7018e-03,  2.9385e-04],\n",
       "         ...,\n",
       "         [ 6.5851e-03,  1.3364e-03,  7.2792e-04,  ...,  5.8188e-03,\n",
       "           1.3215e-03, -8.8964e-04],\n",
       "         [ 1.0789e-03, -2.4039e-03,  6.5749e-04,  ...,  4.7573e-04,\n",
       "          -1.0698e-03,  1.8671e-04],\n",
       "         [ 6.6304e-04,  3.9572e-04, -4.1559e-04,  ...,  1.0169e-04,\n",
       "          -6.2579e-04,  1.4033e-03]], requires_grad=True),\n",
       " 'layers.22.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-6.1469e-04, -2.8446e-03,  3.7470e-03,  ...,  1.0824e-03,\n",
       "          -6.0020e-06, -3.1432e-03],\n",
       "         [ 2.6203e-04,  4.0000e-03, -1.7421e-03,  ..., -4.5250e-04,\n",
       "          -1.4612e-03,  6.0817e-04],\n",
       "         [ 2.5570e-04, -3.0516e-03, -3.2111e-03,  ...,  1.2531e-03,\n",
       "          -3.4369e-04, -1.3212e-03],\n",
       "         ...,\n",
       "         [-1.8370e-03, -1.9181e-04,  4.1342e-03,  ..., -1.8944e-04,\n",
       "          -2.0020e-03,  9.3514e-04],\n",
       "         [-5.0433e-03,  5.3723e-04,  1.8402e-03,  ...,  1.1295e-03,\n",
       "          -9.9490e-04, -2.5336e-04],\n",
       "         [ 3.9308e-03,  3.2677e-03,  2.2061e-03,  ..., -4.1450e-04,\n",
       "          -2.8921e-03,  7.4348e-04]], requires_grad=True),\n",
       " 'layers.22.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-3.2464e-04, -9.3887e-04, -4.1993e-03,  ...,  2.4868e-03,\n",
       "          -2.7616e-03,  1.5800e-03],\n",
       "         [ 3.8621e-04, -1.7225e-04,  2.4336e-03,  ...,  2.5740e-03,\n",
       "           7.3453e-04,  1.7320e-03],\n",
       "         [-1.8341e-03, -3.4704e-03, -2.5302e-03,  ..., -6.6417e-04,\n",
       "          -4.7501e-03, -2.1347e-03],\n",
       "         ...,\n",
       "         [ 4.5680e-03, -4.7127e-03,  2.4165e-03,  ...,  5.6313e-04,\n",
       "          -1.4316e-03,  8.3374e-04],\n",
       "         [ 4.4076e-03,  4.4657e-03,  2.7005e-03,  ..., -4.8387e-04,\n",
       "          -2.3309e-03, -3.5214e-05],\n",
       "         [-2.1486e-03, -7.0828e-03,  3.0711e-03,  ..., -2.6319e-03,\n",
       "           3.3856e-04, -3.6370e-03]], requires_grad=True),\n",
       " 'layers.22.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 1.1081e-03, -4.9796e-04,  3.6109e-03,  ...,  8.6854e-04,\n",
       "          -4.2725e-03,  4.8363e-03],\n",
       "         [-1.0702e-03, -3.6446e-05,  1.6962e-04,  ...,  5.7823e-03,\n",
       "           2.1842e-03,  3.2529e-03],\n",
       "         [-1.0667e-03, -2.8595e-03,  2.2215e-03,  ...,  3.2982e-03,\n",
       "           1.8682e-03,  5.4990e-04],\n",
       "         ...,\n",
       "         [ 6.8013e-03, -2.4726e-03,  9.9206e-04,  ..., -2.0956e-03,\n",
       "          -3.7609e-03,  2.4911e-03],\n",
       "         [-1.4942e-03, -1.0017e-03,  1.1018e-03,  ..., -5.0520e-03,\n",
       "           2.9889e-03, -4.1572e-03],\n",
       "         [ 1.3507e-03, -1.0923e-04,  1.5071e-03,  ...,  2.1299e-03,\n",
       "           4.5904e-04, -1.7322e-03]], requires_grad=True),\n",
       " 'layers.22.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.22.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.23.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0033, -0.0014,  0.0003,  ...,  0.0028,  0.0017, -0.0004],\n",
       "         [-0.0003, -0.0010, -0.0025,  ..., -0.0024, -0.0012,  0.0031],\n",
       "         [-0.0042,  0.0007,  0.0045,  ..., -0.0003, -0.0005, -0.0014],\n",
       "         ...,\n",
       "         [-0.0018, -0.0003,  0.0027,  ..., -0.0088, -0.0035, -0.0003],\n",
       "         [-0.0016,  0.0032, -0.0016,  ..., -0.0008, -0.0028,  0.0006],\n",
       "         [-0.0039,  0.0002, -0.0024,  ..., -0.0036, -0.0002, -0.0017]],\n",
       "        requires_grad=True),\n",
       " 'layers.23.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 3.5431e-04,  1.7430e-03, -3.4158e-03,  ...,  1.4768e-03,\n",
       "          -1.0371e-03,  2.8079e-03],\n",
       "         [ 3.1127e-03, -1.3733e-03, -2.4696e-03,  ...,  1.9383e-03,\n",
       "           2.4135e-03,  5.9706e-04],\n",
       "         [-1.2043e-03,  1.0323e-03,  3.2730e-04,  ..., -1.7047e-03,\n",
       "           1.7351e-03,  2.1101e-03],\n",
       "         ...,\n",
       "         [-2.4931e-03,  1.6236e-03,  5.6229e-03,  ...,  4.8969e-04,\n",
       "           3.9585e-03,  1.5642e-03],\n",
       "         [-4.2746e-05, -4.0950e-03, -3.3928e-03,  ...,  2.4050e-03,\n",
       "          -8.7666e-04,  9.5707e-04],\n",
       "         [ 1.8317e-03, -1.5381e-03,  3.6571e-03,  ..., -3.1301e-03,\n",
       "          -1.5411e-03,  1.2046e-03]], requires_grad=True),\n",
       " 'layers.23.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-1.7013e-03,  2.6973e-03, -1.1627e-04,  ..., -6.6889e-04,\n",
       "          -4.3057e-03,  1.0464e-03],\n",
       "         [-6.3760e-05,  2.9478e-04, -1.9067e-03,  ..., -2.3945e-03,\n",
       "          -5.9765e-03, -1.5653e-03],\n",
       "         [ 4.3665e-03,  5.9430e-03,  1.3072e-03,  ..., -1.5311e-03,\n",
       "          -2.6117e-03,  2.5935e-03],\n",
       "         ...,\n",
       "         [ 2.2934e-03, -7.1634e-04, -2.5721e-03,  ...,  9.0480e-04,\n",
       "           2.1047e-03, -1.3527e-03],\n",
       "         [ 5.0339e-05, -1.0555e-03, -2.4831e-03,  ..., -1.5002e-04,\n",
       "          -1.2644e-03,  2.2568e-03],\n",
       "         [ 9.1237e-04, -7.0225e-03, -1.1208e-03,  ..., -3.5544e-04,\n",
       "           4.7678e-03,  5.3788e-03]], requires_grad=True),\n",
       " 'layers.23.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-6.8113e-04, -4.8290e-04,  6.7642e-04,  ...,  4.5876e-03,\n",
       "          -6.1221e-04, -1.2635e-03],\n",
       "         [ 8.1488e-04, -4.3414e-04, -1.0267e-03,  ...,  1.6316e-03,\n",
       "          -4.7814e-03,  1.7873e-03],\n",
       "         [-6.5046e-05,  3.4185e-03,  1.8080e-03,  ...,  2.1626e-03,\n",
       "           1.9551e-03, -3.0687e-03],\n",
       "         ...,\n",
       "         [ 3.2528e-03, -1.0456e-03,  2.6323e-03,  ...,  1.1527e-03,\n",
       "          -2.3974e-03,  5.0211e-04],\n",
       "         [ 2.0495e-03, -6.4179e-03,  2.1479e-03,  ...,  3.0491e-03,\n",
       "           2.9324e-03, -1.1315e-03],\n",
       "         [-1.8522e-03, -8.5018e-04,  4.1781e-04,  ..., -4.1944e-03,\n",
       "           1.4339e-03, -4.7437e-03]], requires_grad=True),\n",
       " 'layers.23.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-1.2911e-03, -1.6659e-03,  1.0571e-04,  ...,  1.7475e-04,\n",
       "          -3.9911e-03, -8.8950e-05],\n",
       "         [ 1.7517e-03, -1.0449e-03, -5.1557e-04,  ..., -2.8625e-03,\n",
       "          -2.4801e-03, -4.1325e-03],\n",
       "         [-8.4963e-03,  1.4711e-03,  1.5057e-03,  ..., -2.0333e-03,\n",
       "           2.4517e-04,  3.4236e-03],\n",
       "         ...,\n",
       "         [-2.8586e-03, -4.0282e-04, -1.6057e-03,  ..., -3.4765e-04,\n",
       "           2.5339e-03, -5.1095e-04],\n",
       "         [ 3.3094e-03,  2.2183e-03,  3.2891e-04,  ...,  8.9179e-04,\n",
       "          -5.0098e-04,  1.6287e-03],\n",
       "         [-7.3777e-05, -5.8127e-04,  4.5683e-04,  ...,  3.5480e-04,\n",
       "          -3.8682e-04, -4.7101e-05]], requires_grad=True),\n",
       " 'layers.23.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 1.8248e-03,  4.4629e-03, -8.8008e-03,  ...,  1.6854e-03,\n",
       "          -2.8109e-03, -1.7034e-03],\n",
       "         [-2.4555e-03,  4.1932e-06, -6.2919e-04,  ..., -3.5214e-03,\n",
       "          -2.8156e-03,  2.7133e-03],\n",
       "         [-9.4434e-05, -2.3496e-03,  1.0175e-03,  ...,  4.5098e-04,\n",
       "          -3.2670e-03, -1.4501e-03],\n",
       "         ...,\n",
       "         [-1.8316e-03,  2.5183e-03,  3.6808e-04,  ..., -6.5082e-05,\n",
       "          -2.2346e-03, -1.6534e-03],\n",
       "         [ 1.2812e-03, -9.5569e-03, -3.5204e-03,  ...,  5.5235e-03,\n",
       "          -1.4109e-04, -2.6673e-03],\n",
       "         [ 1.9450e-03,  3.1474e-03,  4.6459e-03,  ...,  4.0301e-03,\n",
       "          -4.1824e-03, -3.3846e-03]], requires_grad=True),\n",
       " 'layers.23.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 9.8734e-04,  1.6606e-03, -4.1672e-03,  ..., -3.6096e-03,\n",
       "           3.6970e-04,  7.3031e-05],\n",
       "         [ 3.3922e-03, -3.7859e-03,  3.5335e-05,  ...,  2.4032e-03,\n",
       "           2.6614e-03, -9.1085e-04],\n",
       "         [ 2.6724e-03, -3.6400e-04,  3.2464e-03,  ..., -1.8495e-03,\n",
       "           2.3983e-04,  2.5429e-03],\n",
       "         ...,\n",
       "         [-3.3509e-03,  2.0034e-03,  1.1794e-03,  ...,  1.0065e-03,\n",
       "          -1.4607e-03,  7.7218e-04],\n",
       "         [-4.1566e-04, -4.1751e-03,  3.9038e-03,  ..., -5.2811e-04,\n",
       "          -7.7636e-05, -2.0546e-03],\n",
       "         [-1.9597e-03,  4.2418e-05, -7.5615e-04,  ..., -4.6876e-03,\n",
       "          -1.5835e-04,  3.3967e-04]], requires_grad=True),\n",
       " 'layers.23.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.23.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.24.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-2.7098e-03,  4.4285e-03,  5.7125e-04,  ...,  2.3821e-04,\n",
       "           1.4242e-03,  1.8523e-03],\n",
       "         [ 1.9464e-03, -5.6802e-05,  3.7048e-05,  ...,  3.0633e-03,\n",
       "          -2.0856e-03, -4.3100e-04],\n",
       "         [-3.4624e-03, -3.4204e-03, -2.9179e-03,  ..., -1.5337e-03,\n",
       "          -3.4286e-03,  7.4579e-05],\n",
       "         ...,\n",
       "         [ 1.6843e-03,  7.0171e-04,  4.4372e-04,  ..., -3.4688e-03,\n",
       "          -3.1417e-03, -1.5486e-03],\n",
       "         [ 5.0529e-03,  4.5231e-03,  1.2451e-03,  ...,  2.2588e-03,\n",
       "           9.5248e-04,  9.3624e-04],\n",
       "         [-6.0159e-04, -1.1941e-04, -3.3969e-03,  ..., -3.8873e-03,\n",
       "           5.4810e-04,  9.3458e-04]], requires_grad=True),\n",
       " 'layers.24.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-4.3385e-03, -5.7075e-04,  3.2730e-03,  ...,  4.5593e-03,\n",
       "          -3.7753e-03, -4.9420e-03],\n",
       "         [-5.1635e-03, -2.4447e-03,  2.1570e-03,  ..., -2.0328e-04,\n",
       "           9.2703e-04,  5.8149e-04],\n",
       "         [-3.6293e-03,  1.8603e-03,  1.8252e-04,  ...,  2.3280e-03,\n",
       "          -3.0690e-03,  5.8544e-04],\n",
       "         ...,\n",
       "         [ 9.0674e-04, -5.3823e-03, -9.7249e-04,  ..., -2.9629e-03,\n",
       "          -2.1305e-03, -2.3853e-04],\n",
       "         [-6.1610e-05,  2.6839e-03, -2.9423e-03,  ..., -4.9966e-04,\n",
       "          -1.8627e-03,  4.9991e-03],\n",
       "         [-7.7801e-04,  1.6170e-04, -1.4376e-03,  ...,  2.4754e-03,\n",
       "           2.4235e-03,  8.2921e-04]], requires_grad=True),\n",
       " 'layers.24.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-3.5755e-03, -9.0580e-04, -1.3004e-03,  ...,  4.1091e-04,\n",
       "          -3.5787e-04, -2.2321e-03],\n",
       "         [ 1.2732e-03,  1.0390e-03, -2.7218e-06,  ..., -2.4065e-04,\n",
       "           2.1276e-03, -2.9398e-03],\n",
       "         [-1.1330e-03, -6.3610e-04, -1.9468e-03,  ..., -4.8845e-03,\n",
       "           1.3167e-03,  4.5149e-03],\n",
       "         ...,\n",
       "         [-3.9018e-03,  3.5394e-03,  2.9875e-03,  ..., -9.4955e-04,\n",
       "           1.4777e-03,  7.9256e-03],\n",
       "         [-1.5399e-03, -1.3383e-03, -1.9124e-03,  ..., -1.3560e-03,\n",
       "          -4.4335e-03, -3.8082e-03],\n",
       "         [-2.7989e-03, -5.0491e-03,  1.4021e-03,  ..., -1.2779e-03,\n",
       "           2.9073e-03, -3.3867e-03]], requires_grad=True),\n",
       " 'layers.24.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0019,  0.0006, -0.0042,  ...,  0.0024,  0.0009,  0.0039],\n",
       "         [ 0.0005,  0.0032, -0.0003,  ..., -0.0002,  0.0039, -0.0021],\n",
       "         [ 0.0013,  0.0063,  0.0039,  ..., -0.0005, -0.0019, -0.0003],\n",
       "         ...,\n",
       "         [-0.0019,  0.0004, -0.0039,  ...,  0.0058,  0.0025,  0.0062],\n",
       "         [-0.0003,  0.0007, -0.0002,  ...,  0.0009,  0.0035, -0.0021],\n",
       "         [-0.0015,  0.0054,  0.0003,  ...,  0.0024,  0.0057, -0.0011]],\n",
       "        requires_grad=True),\n",
       " 'layers.24.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-1.6454e-03,  2.2123e-03, -4.6895e-04,  ..., -1.4810e-03,\n",
       "           6.9386e-04, -4.1074e-03],\n",
       "         [-3.7601e-03, -5.6854e-04, -3.4417e-03,  ...,  1.0293e-03,\n",
       "          -1.7134e-03,  2.7873e-03],\n",
       "         [-2.4269e-03, -1.2744e-03,  1.5522e-03,  ..., -7.4984e-04,\n",
       "          -4.2743e-03,  1.1808e-03],\n",
       "         ...,\n",
       "         [-5.7026e-04,  2.9550e-03,  6.1516e-04,  ...,  3.4799e-03,\n",
       "          -2.4910e-03,  2.5684e-03],\n",
       "         [-1.8006e-03,  5.6636e-04, -5.7912e-06,  ...,  3.0199e-03,\n",
       "           1.6166e-03,  2.8495e-03],\n",
       "         [-2.3753e-03, -1.1845e-03,  3.0737e-03,  ...,  1.7140e-04,\n",
       "          -1.1370e-03, -1.0577e-04]], requires_grad=True),\n",
       " 'layers.24.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 1.4173e-03, -3.7106e-04, -5.7318e-03,  ...,  1.7841e-03,\n",
       "          -4.4874e-03,  1.7440e-03],\n",
       "         [-3.4532e-03, -4.9378e-03, -1.0718e-03,  ..., -8.8949e-06,\n",
       "           2.7790e-03,  5.1232e-03],\n",
       "         [ 1.0863e-03, -5.4988e-03, -4.2203e-03,  ...,  7.4513e-04,\n",
       "           7.3134e-04, -2.4904e-03],\n",
       "         ...,\n",
       "         [-9.6209e-04, -1.2182e-03, -1.7525e-03,  ..., -4.9432e-03,\n",
       "           4.0475e-03,  2.3160e-03],\n",
       "         [ 1.5865e-03, -2.7146e-03, -2.3882e-03,  ...,  1.5738e-03,\n",
       "          -1.5127e-03,  3.6741e-03],\n",
       "         [ 4.0760e-04,  1.5737e-04, -8.8562e-04,  ...,  1.0744e-03,\n",
       "           2.3682e-03, -2.7345e-03]], requires_grad=True),\n",
       " 'layers.24.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0052,  0.0061, -0.0020,  ...,  0.0046,  0.0015, -0.0037],\n",
       "         [ 0.0029, -0.0016,  0.0027,  ..., -0.0014, -0.0003,  0.0006],\n",
       "         [-0.0030,  0.0004, -0.0009,  ..., -0.0029, -0.0011,  0.0003],\n",
       "         ...,\n",
       "         [ 0.0032,  0.0053, -0.0043,  ...,  0.0033, -0.0024,  0.0003],\n",
       "         [ 0.0031, -0.0025,  0.0009,  ..., -0.0043,  0.0009, -0.0007],\n",
       "         [-0.0060, -0.0004, -0.0007,  ...,  0.0032, -0.0017,  0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.24.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.24.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.25.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0008,  0.0013, -0.0001,  ...,  0.0072,  0.0010, -0.0002],\n",
       "         [-0.0013, -0.0005, -0.0028,  ...,  0.0012,  0.0002, -0.0008],\n",
       "         [-0.0023, -0.0010, -0.0045,  ..., -0.0016,  0.0025,  0.0050],\n",
       "         ...,\n",
       "         [-0.0006, -0.0024, -0.0021,  ...,  0.0018,  0.0014, -0.0043],\n",
       "         [-0.0009, -0.0003, -0.0015,  ..., -0.0017, -0.0005,  0.0022],\n",
       "         [ 0.0010, -0.0002, -0.0058,  ..., -0.0014, -0.0043,  0.0013]],\n",
       "        requires_grad=True),\n",
       " 'layers.25.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 3.2809e-03,  3.8834e-03, -2.1750e-03,  ..., -1.7189e-03,\n",
       "           1.6157e-03, -4.3993e-03],\n",
       "         [ 2.5474e-03,  1.3851e-03,  2.6082e-03,  ...,  4.8190e-03,\n",
       "           9.9211e-04, -5.1031e-03],\n",
       "         [-3.6178e-05,  1.6767e-03,  2.4136e-03,  ...,  3.5336e-03,\n",
       "           2.6133e-04, -2.0021e-03],\n",
       "         ...,\n",
       "         [ 4.1809e-04, -2.3483e-03,  2.7114e-03,  ..., -1.4049e-04,\n",
       "           3.2092e-03,  2.4493e-03],\n",
       "         [ 6.6292e-04,  1.3791e-03,  3.5554e-03,  ..., -4.4430e-03,\n",
       "           4.4038e-03,  2.7362e-03],\n",
       "         [-3.1865e-03, -6.7564e-03, -2.3997e-03,  ...,  4.9231e-03,\n",
       "           1.4098e-03,  3.3987e-03]], requires_grad=True),\n",
       " 'layers.25.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-0.0040,  0.0024, -0.0056,  ...,  0.0008, -0.0021, -0.0046],\n",
       "         [-0.0045, -0.0042, -0.0016,  ...,  0.0031, -0.0061, -0.0063],\n",
       "         [-0.0010, -0.0011, -0.0004,  ...,  0.0039, -0.0007, -0.0014],\n",
       "         ...,\n",
       "         [-0.0016,  0.0008, -0.0035,  ...,  0.0035, -0.0014,  0.0027],\n",
       "         [ 0.0003,  0.0012,  0.0019,  ..., -0.0039,  0.0024,  0.0005],\n",
       "         [ 0.0006, -0.0004,  0.0038,  ...,  0.0016,  0.0020, -0.0002]],\n",
       "        requires_grad=True),\n",
       " 'layers.25.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0020, -0.0025,  0.0008,  ..., -0.0028, -0.0053,  0.0010],\n",
       "         [-0.0019, -0.0042,  0.0024,  ..., -0.0005, -0.0007,  0.0013],\n",
       "         [-0.0036,  0.0030,  0.0003,  ...,  0.0039,  0.0021, -0.0003],\n",
       "         ...,\n",
       "         [-0.0012,  0.0015,  0.0054,  ..., -0.0015,  0.0044,  0.0027],\n",
       "         [-0.0005, -0.0040, -0.0014,  ...,  0.0030, -0.0037, -0.0013],\n",
       "         [-0.0041,  0.0027,  0.0017,  ...,  0.0035,  0.0014,  0.0005]],\n",
       "        requires_grad=True),\n",
       " 'layers.25.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0007,  0.0018, -0.0023,  ...,  0.0022, -0.0042,  0.0006],\n",
       "         [ 0.0012, -0.0015, -0.0001,  ...,  0.0008,  0.0011,  0.0028],\n",
       "         [ 0.0008,  0.0008, -0.0024,  ..., -0.0011,  0.0022,  0.0017],\n",
       "         ...,\n",
       "         [-0.0019,  0.0028, -0.0068,  ..., -0.0023, -0.0016,  0.0015],\n",
       "         [-0.0053, -0.0027,  0.0006,  ..., -0.0031,  0.0013, -0.0006],\n",
       "         [-0.0034, -0.0048, -0.0032,  ..., -0.0008, -0.0009,  0.0018]],\n",
       "        requires_grad=True),\n",
       " 'layers.25.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-4.2633e-03, -3.0386e-03, -1.6146e-03,  ..., -5.9796e-03,\n",
       "           1.2835e-04, -4.1679e-04],\n",
       "         [-5.3526e-04, -4.8698e-03, -1.8856e-03,  ..., -6.0364e-03,\n",
       "           7.9550e-04, -2.6074e-04],\n",
       "         [ 1.0172e-03, -3.2497e-04, -2.0716e-03,  ...,  4.4671e-04,\n",
       "          -1.1107e-03,  1.4612e-03],\n",
       "         ...,\n",
       "         [ 2.2011e-03, -7.2433e-04, -1.0495e-03,  ...,  1.6658e-03,\n",
       "          -3.5277e-04, -4.4934e-03],\n",
       "         [ 1.1031e-03, -4.1795e-05, -1.7875e-03,  ...,  7.0385e-04,\n",
       "          -1.5472e-03, -4.1288e-03],\n",
       "         [-2.2496e-03,  2.0302e-03,  3.2528e-03,  ..., -1.0535e-03,\n",
       "           2.8026e-03,  4.2514e-03]], requires_grad=True),\n",
       " 'layers.25.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.1805e-03,  6.9609e-04, -1.4475e-03,  ...,  3.9179e-04,\n",
       "           9.4857e-04,  9.2328e-04],\n",
       "         [ 3.7372e-04, -1.7605e-03,  1.0337e-03,  ..., -4.7476e-04,\n",
       "           7.8675e-04,  2.2199e-03],\n",
       "         [-5.6042e-04, -1.4414e-03, -2.4121e-05,  ...,  2.1218e-05,\n",
       "          -3.1379e-03,  1.0287e-03],\n",
       "         ...,\n",
       "         [-4.5785e-03,  5.0312e-04,  2.1283e-03,  ..., -3.5411e-03,\n",
       "           4.7957e-03,  8.2939e-04],\n",
       "         [-1.7973e-03,  1.0004e-03,  5.8257e-03,  ...,  5.6637e-03,\n",
       "           5.2385e-04,  1.9531e-03],\n",
       "         [-2.2971e-03,  2.2105e-03, -1.2512e-04,  ..., -1.2680e-03,\n",
       "          -1.4952e-03,  4.0433e-04]], requires_grad=True),\n",
       " 'layers.25.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.25.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.26.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-2.4521e-04,  3.2282e-03, -1.4888e-03,  ..., -1.2981e-03,\n",
       "           1.4651e-03, -8.4751e-04],\n",
       "         [-1.6156e-03,  2.4898e-03, -1.3660e-03,  ...,  2.4901e-03,\n",
       "           1.9320e-03, -6.9074e-04],\n",
       "         [-2.9635e-03,  7.1026e-04,  7.9300e-04,  ...,  8.9079e-04,\n",
       "          -2.4839e-03,  5.3412e-04],\n",
       "         ...,\n",
       "         [ 4.7162e-03,  3.6948e-03,  2.8401e-03,  ..., -1.2474e-04,\n",
       "          -4.7252e-03, -5.0372e-03],\n",
       "         [ 9.3783e-05, -3.8729e-03, -6.6302e-04,  ...,  3.9889e-03,\n",
       "          -2.3474e-03,  4.9137e-03],\n",
       "         [ 9.6351e-04,  2.8260e-03,  5.0384e-03,  ...,  2.4086e-03,\n",
       "           1.4215e-03,  4.9570e-03]], requires_grad=True),\n",
       " 'layers.26.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-2.8439e-03,  3.7109e-03,  5.2885e-03,  ..., -3.8122e-03,\n",
       "           4.1778e-03, -6.2968e-04],\n",
       "         [-5.9755e-03,  2.9375e-03, -1.2630e-03,  ..., -2.1136e-03,\n",
       "          -1.1311e-04, -1.6716e-03],\n",
       "         [-4.3781e-04,  1.1425e-03,  1.4064e-03,  ...,  3.2200e-03,\n",
       "           2.7249e-04, -6.7745e-03],\n",
       "         ...,\n",
       "         [ 1.5954e-05, -2.9858e-03,  1.2774e-03,  ..., -3.4492e-03,\n",
       "          -1.3427e-03, -4.7898e-03],\n",
       "         [ 5.7692e-04,  2.1929e-03,  2.6081e-03,  ..., -2.2119e-04,\n",
       "          -2.0228e-03,  1.5652e-03],\n",
       "         [-8.0948e-04,  2.0537e-03, -5.0951e-03,  ...,  3.6524e-04,\n",
       "           2.9282e-03, -1.4917e-03]], requires_grad=True),\n",
       " 'layers.26.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-7.2748e-04, -2.1795e-03, -1.0707e-05,  ..., -3.8965e-03,\n",
       "           3.1301e-03, -2.6417e-03],\n",
       "         [ 1.0714e-03, -1.6756e-03,  9.0326e-04,  ..., -1.1858e-03,\n",
       "          -8.3884e-04,  3.0327e-03],\n",
       "         [-1.9690e-03, -1.9016e-03, -1.2082e-03,  ..., -2.6817e-03,\n",
       "           4.4994e-03, -1.9508e-03],\n",
       "         ...,\n",
       "         [-7.2485e-04,  2.8514e-03, -3.5316e-03,  ..., -4.4257e-04,\n",
       "           1.4071e-03, -4.5821e-04],\n",
       "         [-3.0307e-03,  2.2598e-04, -3.3802e-04,  ..., -1.7549e-03,\n",
       "          -7.3149e-04,  1.6579e-03],\n",
       "         [ 5.3174e-04, -1.6632e-03,  1.1022e-03,  ...,  7.5392e-04,\n",
       "          -4.6769e-04,  3.9945e-03]], requires_grad=True),\n",
       " 'layers.26.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-2.3313e-03, -3.2336e-03,  1.9798e-03,  ...,  2.7699e-04,\n",
       "          -8.0784e-04, -6.7297e-04],\n",
       "         [-9.7730e-04,  1.0780e-03, -1.9087e-03,  ..., -1.0290e-03,\n",
       "           5.6912e-03, -1.7317e-03],\n",
       "         [ 3.0919e-03, -4.2863e-04,  5.0743e-04,  ..., -1.3133e-03,\n",
       "           3.2449e-04,  1.5820e-03],\n",
       "         ...,\n",
       "         [-1.5591e-03,  2.5271e-05,  1.3352e-04,  ...,  2.1540e-04,\n",
       "          -4.7758e-03, -1.0766e-03],\n",
       "         [-8.6219e-04, -1.5467e-03, -1.8846e-04,  ...,  3.1995e-04,\n",
       "          -1.5362e-03,  5.2303e-03],\n",
       "         [ 2.3102e-03, -4.2948e-03, -9.7476e-04,  ...,  4.6402e-03,\n",
       "           2.6109e-03,  1.5997e-03]], requires_grad=True),\n",
       " 'layers.26.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[ 3.1627e-03,  1.2322e-03, -5.0030e-04,  ...,  1.9457e-03,\n",
       "           1.4426e-03, -1.2527e-03],\n",
       "         [-2.8148e-03,  2.8525e-03,  1.3871e-03,  ..., -1.3015e-03,\n",
       "           2.7845e-03,  1.1743e-03],\n",
       "         [ 1.5824e-03, -8.2690e-05, -1.4050e-03,  ...,  1.4700e-03,\n",
       "          -1.9606e-03,  2.8228e-03],\n",
       "         ...,\n",
       "         [-5.8353e-04,  1.4598e-03, -1.5344e-03,  ..., -4.6698e-04,\n",
       "           1.2195e-03, -5.5118e-03],\n",
       "         [-1.1205e-03, -1.0211e-03, -1.4518e-03,  ..., -3.2162e-04,\n",
       "          -1.1342e-03,  3.6581e-04],\n",
       "         [ 2.0289e-03, -6.3486e-04,  3.5379e-03,  ...,  2.3340e-03,\n",
       "           1.8674e-03,  2.2516e-03]], requires_grad=True),\n",
       " 'layers.26.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0033, -0.0012,  0.0017,  ..., -0.0018,  0.0003,  0.0032],\n",
       "         [-0.0036, -0.0052,  0.0036,  ...,  0.0003, -0.0015,  0.0052],\n",
       "         [-0.0009, -0.0013,  0.0040,  ...,  0.0021, -0.0035,  0.0030],\n",
       "         ...,\n",
       "         [ 0.0054,  0.0022, -0.0020,  ...,  0.0008, -0.0018, -0.0029],\n",
       "         [-0.0012,  0.0009, -0.0017,  ...,  0.0028,  0.0021, -0.0025],\n",
       "         [ 0.0030, -0.0069,  0.0022,  ...,  0.0016, -0.0009, -0.0014]],\n",
       "        requires_grad=True),\n",
       " 'layers.26.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 2.0674e-03,  1.9768e-03, -5.3532e-03,  ..., -6.7328e-04,\n",
       "           2.4102e-03,  3.2179e-03],\n",
       "         [-8.2877e-04, -1.4894e-03,  1.0781e-03,  ..., -6.4547e-04,\n",
       "          -2.4545e-03, -4.7576e-03],\n",
       "         [ 7.7844e-04, -2.4269e-03, -1.1287e-03,  ...,  8.4031e-04,\n",
       "           7.0987e-04,  1.3894e-03],\n",
       "         ...,\n",
       "         [-1.0623e-03, -3.0456e-05, -1.5375e-03,  ..., -2.4563e-04,\n",
       "           1.3556e-03, -4.1313e-03],\n",
       "         [ 1.0260e-03, -7.3826e-05, -2.5184e-03,  ..., -1.4867e-03,\n",
       "          -2.5589e-04,  1.8910e-03],\n",
       "         [-1.6049e-03,  1.7027e-04,  2.1193e-03,  ..., -2.2486e-03,\n",
       "           3.8683e-03, -1.9844e-03]], requires_grad=True),\n",
       " 'layers.26.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.26.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.27.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 3.7290e-04, -1.5328e-03, -1.8576e-03,  ...,  4.8377e-03,\n",
       "          -1.2338e-03, -3.4145e-03],\n",
       "         [ 7.4695e-04,  8.0999e-05,  2.1299e-03,  ...,  2.8669e-03,\n",
       "          -2.3301e-05,  1.0241e-03],\n",
       "         [ 3.1236e-03, -2.6855e-03, -2.9022e-03,  ..., -8.3713e-04,\n",
       "          -3.0874e-03, -3.0131e-03],\n",
       "         ...,\n",
       "         [ 2.5862e-03,  2.7261e-03, -1.4522e-03,  ..., -2.3329e-03,\n",
       "          -1.7132e-03,  4.3149e-03],\n",
       "         [-1.8625e-03,  1.5653e-03, -2.9245e-03,  ...,  2.4928e-03,\n",
       "          -3.0913e-03,  2.4351e-03],\n",
       "         [-1.8966e-04, -3.5321e-03,  4.3984e-03,  ..., -1.8441e-03,\n",
       "           1.5968e-03, -1.5914e-03]], requires_grad=True),\n",
       " 'layers.27.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-0.0023, -0.0002, -0.0036,  ..., -0.0035,  0.0013, -0.0003],\n",
       "         [ 0.0011,  0.0009, -0.0002,  ..., -0.0008, -0.0020, -0.0038],\n",
       "         [-0.0052,  0.0041, -0.0010,  ..., -0.0003, -0.0027,  0.0005],\n",
       "         ...,\n",
       "         [ 0.0046, -0.0012,  0.0015,  ...,  0.0007, -0.0019,  0.0036],\n",
       "         [ 0.0024,  0.0014,  0.0012,  ..., -0.0011,  0.0046, -0.0013],\n",
       "         [ 0.0015,  0.0023,  0.0010,  ..., -0.0001,  0.0028, -0.0013]],\n",
       "        requires_grad=True),\n",
       " 'layers.27.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-2.7328e-03,  6.3283e-03,  2.8265e-03,  ...,  3.8433e-04,\n",
       "          -2.1731e-03, -2.1429e-03],\n",
       "         [ 2.5654e-03,  1.0584e-03,  9.8517e-05,  ..., -3.2399e-04,\n",
       "          -6.3141e-04, -3.0698e-03],\n",
       "         [-4.2025e-03, -2.4742e-03, -5.0634e-03,  ..., -1.0117e-03,\n",
       "          -8.2059e-04,  6.8707e-04],\n",
       "         ...,\n",
       "         [ 1.3300e-03,  2.7526e-03, -4.9023e-03,  ...,  1.7428e-03,\n",
       "           3.8865e-03,  1.4434e-03],\n",
       "         [-2.5738e-03, -3.0680e-03, -1.1252e-03,  ...,  2.5725e-03,\n",
       "          -1.6211e-04,  2.0359e-03],\n",
       "         [-2.4313e-03, -9.3779e-04,  1.1641e-03,  ..., -2.2226e-03,\n",
       "           1.3998e-03, -8.5509e-04]], requires_grad=True),\n",
       " 'layers.27.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 1.6567e-03, -1.8951e-03,  2.0811e-04,  ..., -4.8060e-03,\n",
       "          -1.1183e-03, -3.0854e-03],\n",
       "         [ 7.6213e-04,  1.2304e-03,  5.1853e-03,  ...,  1.6011e-03,\n",
       "           1.9414e-03,  3.6087e-03],\n",
       "         [ 6.5584e-04,  5.2470e-04, -1.0252e-03,  ..., -3.0028e-03,\n",
       "           4.5204e-04, -6.0945e-04],\n",
       "         ...,\n",
       "         [-2.1136e-03, -1.2094e-04,  6.6378e-04,  ...,  2.3558e-03,\n",
       "          -1.7477e-03, -2.9508e-03],\n",
       "         [-1.3708e-03, -1.7879e-03,  2.2775e-03,  ...,  1.0330e-03,\n",
       "           6.9924e-03,  3.0829e-03],\n",
       "         [-3.6032e-03,  4.0800e-04,  7.2693e-05,  ..., -5.9109e-04,\n",
       "          -4.6027e-03,  4.7913e-03]], requires_grad=True),\n",
       " 'layers.27.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-4.5814e-04, -1.3592e-03, -2.1137e-03,  ...,  1.6550e-03,\n",
       "           1.8731e-03, -3.5249e-03],\n",
       "         [-1.0136e-03, -1.3739e-03,  2.1981e-04,  ...,  3.7443e-03,\n",
       "           1.4804e-03, -4.7771e-04],\n",
       "         [ 1.4212e-03,  3.5300e-03, -2.7848e-03,  ..., -6.0054e-05,\n",
       "           3.4099e-03,  5.3462e-04],\n",
       "         ...,\n",
       "         [-7.2546e-04, -1.1780e-03, -1.1360e-03,  ...,  1.1242e-04,\n",
       "          -8.1821e-04, -8.3560e-04],\n",
       "         [ 2.0792e-03, -2.0081e-03, -3.1030e-03,  ..., -4.5865e-03,\n",
       "          -5.6312e-04,  4.1346e-06],\n",
       "         [-2.4012e-03,  4.4157e-03,  2.4788e-03,  ..., -2.5738e-03,\n",
       "          -3.7307e-04, -1.7500e-03]], requires_grad=True),\n",
       " 'layers.27.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-0.0017,  0.0017, -0.0067,  ..., -0.0023,  0.0009,  0.0027],\n",
       "         [ 0.0034, -0.0030, -0.0012,  ..., -0.0004,  0.0039,  0.0058],\n",
       "         [-0.0018,  0.0002,  0.0001,  ..., -0.0073,  0.0033,  0.0035],\n",
       "         ...,\n",
       "         [ 0.0033, -0.0010, -0.0012,  ...,  0.0001, -0.0010, -0.0019],\n",
       "         [ 0.0025, -0.0039, -0.0018,  ..., -0.0020,  0.0005, -0.0008],\n",
       "         [ 0.0018,  0.0023,  0.0013,  ..., -0.0015,  0.0004,  0.0088]],\n",
       "        requires_grad=True),\n",
       " 'layers.27.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-1.0241e-03, -5.1711e-04, -9.8485e-04,  ..., -2.1029e-03,\n",
       "           1.8124e-03, -5.5811e-03],\n",
       "         [ 4.5137e-05,  3.4933e-03,  2.0520e-03,  ..., -3.4919e-03,\n",
       "           5.1313e-05, -1.6206e-03],\n",
       "         [-1.4238e-03,  2.5165e-03,  8.3875e-04,  ..., -3.4787e-03,\n",
       "           2.8876e-04, -1.1237e-03],\n",
       "         ...,\n",
       "         [-2.6654e-03,  4.9709e-04,  2.2505e-03,  ...,  1.4622e-04,\n",
       "          -1.0374e-03,  1.9254e-03],\n",
       "         [-4.5090e-04,  1.5205e-03, -1.0969e-03,  ...,  4.0127e-04,\n",
       "          -4.3171e-04, -3.1238e-03],\n",
       "         [ 4.0270e-03, -2.3337e-03, -1.9906e-04,  ...,  1.3050e-05,\n",
       "          -6.5191e-04,  6.6038e-04]], requires_grad=True),\n",
       " 'layers.27.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.27.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.28.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 7.9252e-04, -3.2162e-04,  3.4079e-03,  ..., -2.3415e-03,\n",
       "           3.4323e-03,  1.3811e-03],\n",
       "         [ 3.5283e-03,  2.4826e-03, -3.9272e-04,  ...,  1.4456e-03,\n",
       "          -2.3832e-05,  2.9566e-03],\n",
       "         [-2.0015e-03,  2.8400e-03, -2.1759e-03,  ...,  5.0584e-04,\n",
       "           8.0200e-04, -2.8305e-03],\n",
       "         ...,\n",
       "         [-2.6152e-03, -1.0619e-03,  4.7134e-04,  ...,  2.6074e-03,\n",
       "          -2.5857e-03,  2.7864e-03],\n",
       "         [-1.9659e-03, -5.8797e-03, -2.3220e-03,  ...,  2.5019e-03,\n",
       "           1.0658e-04,  4.2846e-03],\n",
       "         [-3.0384e-03,  5.9571e-03, -3.1806e-03,  ...,  4.6224e-04,\n",
       "          -7.1472e-04,  2.7522e-04]], requires_grad=True),\n",
       " 'layers.28.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[ 2.6791e-03, -7.7300e-04,  3.7315e-03,  ..., -5.2729e-04,\n",
       "          -2.0087e-03, -6.4932e-03],\n",
       "         [-5.6394e-05, -3.3018e-03,  3.8812e-04,  ..., -4.2558e-03,\n",
       "           1.9291e-03, -2.9677e-04],\n",
       "         [ 5.8341e-03,  1.4730e-03,  1.3338e-03,  ...,  5.9087e-03,\n",
       "           1.2850e-03,  1.0143e-03],\n",
       "         ...,\n",
       "         [ 1.2834e-05, -7.1107e-04, -1.8842e-03,  ..., -1.2219e-03,\n",
       "          -1.9668e-04, -2.8112e-04],\n",
       "         [ 2.7331e-03,  3.8025e-03, -5.4306e-03,  ...,  1.1596e-03,\n",
       "           1.9278e-03,  3.6571e-03],\n",
       "         [-2.2618e-03,  2.7129e-03, -2.7069e-03,  ..., -4.5759e-04,\n",
       "           4.3684e-04, -2.7707e-03]], requires_grad=True),\n",
       " 'layers.28.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-8.6736e-04,  1.8424e-03,  6.7130e-04,  ...,  6.1120e-04,\n",
       "           2.6042e-04, -5.8127e-04],\n",
       "         [ 3.8188e-03, -4.3482e-03, -1.9411e-03,  ..., -1.9265e-03,\n",
       "          -2.3861e-03, -1.6112e-03],\n",
       "         [-4.5826e-04,  1.4689e-04, -1.0461e-03,  ..., -8.4368e-04,\n",
       "           2.7212e-03, -1.3426e-03],\n",
       "         ...,\n",
       "         [ 6.0804e-04, -4.9557e-03, -9.9605e-06,  ..., -8.7499e-04,\n",
       "          -1.9040e-03, -1.3459e-04],\n",
       "         [-8.6653e-04,  8.0862e-04, -3.5304e-03,  ..., -4.4490e-03,\n",
       "           1.4480e-03,  2.2182e-03],\n",
       "         [-2.3833e-03, -9.6117e-04,  1.7094e-03,  ..., -3.8974e-04,\n",
       "          -2.2801e-03, -8.2058e-04]], requires_grad=True),\n",
       " 'layers.28.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-1.9967e-03,  7.2173e-04, -9.8435e-04,  ..., -6.2922e-05,\n",
       "           8.9013e-04, -5.0651e-03],\n",
       "         [-1.0127e-03, -2.5141e-03,  3.4322e-03,  ..., -2.7921e-03,\n",
       "           1.7088e-03, -1.9412e-03],\n",
       "         [-3.5626e-03, -2.4234e-03,  1.0595e-03,  ...,  1.5843e-03,\n",
       "          -2.4295e-03,  6.0709e-04],\n",
       "         ...,\n",
       "         [ 3.5073e-04, -2.2037e-03,  2.0605e-03,  ..., -1.6396e-03,\n",
       "          -4.6745e-04, -1.9380e-04],\n",
       "         [-2.9022e-04,  1.2081e-03, -2.4456e-03,  ...,  3.1304e-03,\n",
       "          -7.1296e-03, -3.3864e-04],\n",
       "         [ 3.4753e-03,  1.3340e-03,  1.3620e-03,  ...,  4.0059e-03,\n",
       "          -1.5410e-03,  2.5463e-03]], requires_grad=True),\n",
       " 'layers.28.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-0.0013,  0.0011, -0.0051,  ..., -0.0002,  0.0021,  0.0010],\n",
       "         [-0.0027, -0.0025,  0.0038,  ..., -0.0003,  0.0028,  0.0007],\n",
       "         [ 0.0025,  0.0046, -0.0044,  ...,  0.0023, -0.0019,  0.0066],\n",
       "         ...,\n",
       "         [-0.0019, -0.0003,  0.0020,  ...,  0.0036, -0.0018,  0.0004],\n",
       "         [ 0.0013,  0.0017, -0.0031,  ..., -0.0026, -0.0011, -0.0011],\n",
       "         [-0.0006, -0.0019,  0.0010,  ..., -0.0038, -0.0023,  0.0035]],\n",
       "        requires_grad=True),\n",
       " 'layers.28.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 7.7497e-04,  2.8387e-03,  1.2012e-03,  ..., -4.9631e-04,\n",
       "          -5.0964e-04,  1.8271e-03],\n",
       "         [ 4.7140e-03,  1.7145e-03,  5.3909e-04,  ...,  7.8446e-04,\n",
       "           1.5531e-04, -2.2437e-03],\n",
       "         [-1.4585e-04, -7.8033e-03, -2.2460e-03,  ...,  2.3877e-03,\n",
       "          -4.7828e-04,  8.3235e-04],\n",
       "         ...,\n",
       "         [ 1.3514e-03,  2.2294e-03,  1.3140e-03,  ...,  4.5475e-03,\n",
       "           6.3035e-05, -4.6622e-03],\n",
       "         [-1.6406e-03,  5.4533e-04,  5.4532e-04,  ..., -1.0035e-03,\n",
       "          -3.0715e-04,  1.4866e-03],\n",
       "         [-1.0916e-03,  1.8070e-03, -9.4134e-06,  ...,  4.6273e-04,\n",
       "          -9.3403e-04, -1.8277e-03]], requires_grad=True),\n",
       " 'layers.28.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-1.0339e-03, -4.7667e-03, -2.6204e-03,  ...,  4.0469e-03,\n",
       "          -1.7126e-04, -3.5475e-04],\n",
       "         [-2.8876e-04,  1.2522e-03, -2.5400e-03,  ...,  6.3565e-03,\n",
       "           3.3856e-03, -1.6846e-03],\n",
       "         [-1.7352e-03, -3.3023e-03, -2.1097e-03,  ...,  1.9757e-03,\n",
       "           1.2689e-03,  4.8349e-05],\n",
       "         ...,\n",
       "         [ 3.0480e-04,  1.8412e-03,  3.1646e-03,  ...,  5.5092e-03,\n",
       "          -1.2640e-03, -3.4634e-03],\n",
       "         [-6.3848e-04,  5.2766e-03, -1.6771e-03,  ..., -2.4634e-03,\n",
       "          -8.4657e-04,  1.1608e-03],\n",
       "         [-3.8431e-03, -4.2634e-03, -3.4744e-05,  ..., -2.4148e-03,\n",
       "          -6.4965e-04,  2.0871e-03]], requires_grad=True),\n",
       " 'layers.28.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.28.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.29.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0009,  0.0035, -0.0011,  ...,  0.0017, -0.0029,  0.0012],\n",
       "         [ 0.0011,  0.0031,  0.0009,  ..., -0.0033,  0.0026, -0.0015],\n",
       "         [-0.0021,  0.0056, -0.0013,  ...,  0.0025, -0.0033, -0.0017],\n",
       "         ...,\n",
       "         [-0.0012, -0.0033, -0.0017,  ...,  0.0022, -0.0030, -0.0010],\n",
       "         [ 0.0022,  0.0009,  0.0035,  ..., -0.0023, -0.0008,  0.0009],\n",
       "         [ 0.0051,  0.0049, -0.0003,  ..., -0.0010, -0.0011, -0.0007]],\n",
       "        requires_grad=True),\n",
       " 'layers.29.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-1.6827e-03, -2.2491e-03,  5.9512e-03,  ..., -2.9903e-03,\n",
       "           4.6004e-03,  9.9778e-04],\n",
       "         [ 2.0094e-03,  4.6707e-03, -1.1796e-03,  ...,  6.5353e-04,\n",
       "          -3.3379e-03,  6.7143e-04],\n",
       "         [-2.9809e-04, -3.5601e-03,  1.3230e-03,  ..., -6.0255e-04,\n",
       "           9.6810e-04, -3.1054e-03],\n",
       "         ...,\n",
       "         [-6.3959e-04, -1.1676e-03, -1.7211e-03,  ..., -2.0996e-04,\n",
       "           9.4167e-04,  4.1236e-06],\n",
       "         [ 1.5603e-03,  1.3545e-03,  3.1709e-03,  ..., -4.6940e-03,\n",
       "          -1.2750e-03,  4.2673e-03],\n",
       "         [ 1.4829e-03, -2.4881e-03, -4.9595e-03,  ..., -6.7767e-03,\n",
       "          -2.5263e-03,  1.6350e-04]], requires_grad=True),\n",
       " 'layers.29.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-3.9861e-05, -3.1199e-03,  3.6695e-03,  ..., -1.3773e-03,\n",
       "           4.9238e-04,  6.7095e-04],\n",
       "         [-4.3896e-03,  1.5278e-03,  3.1314e-03,  ..., -1.2326e-03,\n",
       "          -2.6613e-03, -7.9029e-04],\n",
       "         [-2.7141e-03,  6.0546e-04,  1.5610e-03,  ..., -5.7601e-03,\n",
       "           9.9364e-04,  4.9390e-05],\n",
       "         ...,\n",
       "         [-1.3068e-03,  3.2227e-03, -1.8131e-03,  ...,  2.7290e-03,\n",
       "          -3.9691e-03, -2.7633e-03],\n",
       "         [ 9.1552e-04, -4.0775e-03, -1.9127e-03,  ...,  1.3924e-03,\n",
       "          -4.7401e-03, -1.1418e-03],\n",
       "         [ 1.7330e-03, -8.8885e-04,  2.7164e-03,  ...,  3.2621e-03,\n",
       "          -3.6586e-04,  1.4611e-03]], requires_grad=True),\n",
       " 'layers.29.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[-0.0038, -0.0003, -0.0007,  ...,  0.0029,  0.0012,  0.0003],\n",
       "         [-0.0012, -0.0019, -0.0003,  ...,  0.0016, -0.0024, -0.0016],\n",
       "         [-0.0013,  0.0040, -0.0026,  ...,  0.0026,  0.0055,  0.0029],\n",
       "         ...,\n",
       "         [ 0.0073,  0.0004, -0.0026,  ..., -0.0032, -0.0026,  0.0004],\n",
       "         [ 0.0020, -0.0044, -0.0002,  ..., -0.0010,  0.0045, -0.0012],\n",
       "         [ 0.0057,  0.0014,  0.0009,  ...,  0.0025,  0.0013, -0.0039]],\n",
       "        requires_grad=True),\n",
       " 'layers.29.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-0.0003,  0.0009, -0.0011,  ..., -0.0029,  0.0035, -0.0009],\n",
       "         [-0.0015,  0.0029,  0.0024,  ..., -0.0034, -0.0014, -0.0005],\n",
       "         [ 0.0044, -0.0012, -0.0026,  ..., -0.0013, -0.0052,  0.0034],\n",
       "         ...,\n",
       "         [-0.0008,  0.0012,  0.0004,  ..., -0.0021, -0.0007, -0.0017],\n",
       "         [ 0.0015, -0.0015,  0.0010,  ..., -0.0006, -0.0013,  0.0007],\n",
       "         [ 0.0047,  0.0002, -0.0003,  ..., -0.0046, -0.0023, -0.0034]],\n",
       "        requires_grad=True),\n",
       " 'layers.29.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0010, -0.0010, -0.0012,  ...,  0.0015,  0.0011, -0.0039],\n",
       "         [ 0.0010, -0.0017, -0.0030,  ...,  0.0041, -0.0046,  0.0007],\n",
       "         [-0.0020,  0.0056, -0.0024,  ..., -0.0035,  0.0005,  0.0026],\n",
       "         ...,\n",
       "         [-0.0033, -0.0008,  0.0018,  ..., -0.0021, -0.0010,  0.0013],\n",
       "         [ 0.0011,  0.0022,  0.0041,  ...,  0.0019, -0.0009,  0.0024],\n",
       "         [-0.0006, -0.0029, -0.0010,  ..., -0.0007, -0.0015,  0.0065]],\n",
       "        requires_grad=True),\n",
       " 'layers.29.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[-0.0009,  0.0015, -0.0018,  ..., -0.0018, -0.0053, -0.0022],\n",
       "         [ 0.0001,  0.0025,  0.0040,  ...,  0.0009, -0.0032,  0.0049],\n",
       "         [ 0.0027,  0.0011, -0.0023,  ..., -0.0040,  0.0047,  0.0052],\n",
       "         ...,\n",
       "         [ 0.0023, -0.0030,  0.0024,  ..., -0.0063,  0.0003,  0.0013],\n",
       "         [-0.0027, -0.0015, -0.0054,  ...,  0.0018,  0.0003, -0.0065],\n",
       "         [ 0.0032,  0.0019,  0.0027,  ...,  0.0017, -0.0004,  0.0010]],\n",
       "        requires_grad=True),\n",
       " 'layers.29.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.29.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.30.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[ 1.7160e-03,  1.7463e-04,  4.5887e-04,  ...,  4.1264e-03,\n",
       "           1.3699e-03, -1.7209e-03],\n",
       "         [ 6.2472e-03, -1.1523e-03,  2.3988e-03,  ..., -1.8450e-03,\n",
       "          -1.7797e-03,  2.3643e-03],\n",
       "         [ 1.7959e-03,  2.7400e-03, -1.4352e-04,  ...,  3.6194e-05,\n",
       "          -4.0334e-03, -3.1550e-03],\n",
       "         ...,\n",
       "         [-8.2543e-04,  4.3480e-03,  7.9382e-04,  ...,  1.2793e-03,\n",
       "           1.9021e-03, -3.0846e-03],\n",
       "         [-2.2585e-03, -4.2777e-03, -6.3157e-04,  ..., -8.2422e-04,\n",
       "           1.3395e-03,  2.3127e-04],\n",
       "         [-8.7146e-04,  6.9035e-03,  2.7984e-03,  ...,  1.7482e-03,\n",
       "           5.1493e-04,  1.7413e-03]], requires_grad=True),\n",
       " 'layers.30.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-3.1434e-03, -3.5029e-04, -2.0915e-03,  ..., -3.0142e-03,\n",
       "           7.7015e-04,  4.5563e-04],\n",
       "         [-6.1383e-03, -3.4323e-04, -5.2004e-03,  ...,  1.3188e-03,\n",
       "          -1.4452e-03,  7.9772e-05],\n",
       "         [ 5.3555e-04, -2.1656e-03, -1.2194e-03,  ...,  2.9654e-03,\n",
       "          -1.5150e-04,  9.7441e-04],\n",
       "         ...,\n",
       "         [-4.1799e-04, -2.2168e-03,  2.6116e-03,  ...,  2.3008e-04,\n",
       "          -5.7743e-06,  1.7188e-03],\n",
       "         [-2.3977e-03,  1.0340e-04, -2.1023e-03,  ..., -3.5210e-04,\n",
       "           1.2003e-03,  1.2033e-03],\n",
       "         [-2.0236e-03,  1.0967e-03,  9.4425e-04,  ...,  7.7515e-04,\n",
       "          -9.8831e-04, -2.1701e-03]], requires_grad=True),\n",
       " 'layers.30.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[ 1.7093e-03, -9.6185e-04,  1.2157e-03,  ..., -1.1447e-03,\n",
       "           5.2127e-05,  7.1193e-04],\n",
       "         [-1.0559e-03,  3.5493e-03, -1.2516e-03,  ...,  2.8125e-03,\n",
       "          -3.4743e-03,  8.3233e-04],\n",
       "         [ 3.1237e-03, -1.1422e-03,  1.4514e-03,  ...,  1.5056e-03,\n",
       "          -5.1698e-03,  1.0325e-03],\n",
       "         ...,\n",
       "         [-1.5597e-04,  4.4443e-03,  2.3740e-03,  ...,  1.5355e-04,\n",
       "           1.1913e-03, -3.3930e-03],\n",
       "         [ 2.2220e-04, -2.7228e-04, -2.2865e-03,  ..., -4.0564e-03,\n",
       "           4.8015e-03,  1.3644e-03],\n",
       "         [-3.1507e-03,  1.5565e-03, -2.8892e-03,  ..., -6.8229e-04,\n",
       "          -2.6334e-04, -3.5787e-03]], requires_grad=True),\n",
       " 'layers.30.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0030,  0.0015,  0.0066,  ...,  0.0024,  0.0024,  0.0001],\n",
       "         [ 0.0036,  0.0015, -0.0027,  ...,  0.0015, -0.0038,  0.0042],\n",
       "         [-0.0040,  0.0017, -0.0021,  ..., -0.0010,  0.0018, -0.0016],\n",
       "         ...,\n",
       "         [ 0.0007, -0.0004, -0.0041,  ..., -0.0039, -0.0029, -0.0010],\n",
       "         [-0.0043, -0.0008,  0.0024,  ..., -0.0034, -0.0013,  0.0018],\n",
       "         [ 0.0049,  0.0006,  0.0004,  ..., -0.0021,  0.0034, -0.0006]],\n",
       "        requires_grad=True),\n",
       " 'layers.30.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-6.4663e-04,  3.9644e-04, -3.1800e-03,  ...,  4.4720e-03,\n",
       "           3.7692e-03, -1.2559e-03],\n",
       "         [-1.4237e-03, -2.5830e-03, -1.3243e-03,  ..., -1.6326e-03,\n",
       "          -7.3159e-04, -3.3026e-03],\n",
       "         [-1.8960e-04,  2.0613e-04, -8.6319e-04,  ..., -2.5593e-03,\n",
       "           1.2787e-03,  1.5536e-03],\n",
       "         ...,\n",
       "         [-2.0786e-03, -2.2326e-03,  9.9336e-04,  ..., -3.8510e-05,\n",
       "          -2.6299e-04, -1.7092e-03],\n",
       "         [ 5.5705e-04, -4.4696e-03, -1.6212e-03,  ..., -1.3071e-03,\n",
       "          -2.6542e-03,  1.6783e-03],\n",
       "         [ 1.5468e-03, -2.8290e-03, -2.7374e-04,  ...,  8.5594e-04,\n",
       "           9.7945e-04, -2.3549e-04]], requires_grad=True),\n",
       " 'layers.30.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[-1.2255e-03,  1.4395e-03, -1.0268e-03,  ...,  2.3245e-03,\n",
       "          -1.0389e-03,  1.5780e-03],\n",
       "         [-2.5480e-03, -1.3790e-03, -7.7528e-04,  ...,  3.7086e-05,\n",
       "           2.5560e-04,  1.0710e-03],\n",
       "         [ 2.5095e-03,  2.7945e-03,  2.0901e-03,  ..., -1.7261e-03,\n",
       "          -4.5332e-04, -1.9368e-03],\n",
       "         ...,\n",
       "         [-9.3267e-04, -2.2994e-03,  1.9998e-03,  ..., -4.9033e-03,\n",
       "           2.9588e-03, -2.8981e-03],\n",
       "         [ 1.1860e-04, -4.5367e-03, -2.8557e-03,  ...,  3.2187e-03,\n",
       "           7.3330e-03,  1.5764e-03],\n",
       "         [-9.0961e-04,  2.6084e-03, -1.2097e-03,  ..., -3.6146e-03,\n",
       "           2.7222e-03,  1.8625e-03]], requires_grad=True),\n",
       " 'layers.30.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0009,  0.0057,  0.0021,  ..., -0.0015, -0.0016,  0.0034],\n",
       "         [-0.0003,  0.0001, -0.0026,  ..., -0.0018, -0.0031, -0.0030],\n",
       "         [ 0.0008,  0.0032, -0.0045,  ..., -0.0028,  0.0028, -0.0022],\n",
       "         ...,\n",
       "         [ 0.0007, -0.0014,  0.0017,  ..., -0.0011, -0.0048, -0.0010],\n",
       "         [-0.0021,  0.0027,  0.0025,  ...,  0.0001,  0.0019,  0.0021],\n",
       "         [ 0.0027, -0.0016,  0.0008,  ..., -0.0040,  0.0043,  0.0014]],\n",
       "        requires_grad=True),\n",
       " 'layers.30.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.30.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.31.self_attn.q_proj.weight': Parameter containing:\n",
       " tensor([[-0.0019, -0.0021, -0.0007,  ...,  0.0010,  0.0024, -0.0007],\n",
       "         [-0.0014,  0.0036, -0.0022,  ...,  0.0024, -0.0011, -0.0061],\n",
       "         [ 0.0048,  0.0019,  0.0020,  ...,  0.0038, -0.0073,  0.0020],\n",
       "         ...,\n",
       "         [ 0.0059,  0.0010,  0.0010,  ...,  0.0011, -0.0016,  0.0019],\n",
       "         [ 0.0011, -0.0011,  0.0009,  ...,  0.0005, -0.0009, -0.0030],\n",
       "         [-0.0015, -0.0069, -0.0042,  ...,  0.0005, -0.0019,  0.0032]],\n",
       "        requires_grad=True),\n",
       " 'layers.31.self_attn.k_proj.weight': Parameter containing:\n",
       " tensor([[-3.8212e-06,  2.9088e-03,  5.1310e-03,  ..., -6.8538e-04,\n",
       "          -1.1936e-03,  3.5613e-04],\n",
       "         [ 2.4016e-04,  1.6976e-03,  1.4377e-03,  ..., -1.9751e-03,\n",
       "          -3.2750e-03,  6.8435e-04],\n",
       "         [-2.4387e-03,  1.2428e-04, -5.8987e-04,  ...,  3.1254e-03,\n",
       "           2.7742e-03,  3.9298e-03],\n",
       "         ...,\n",
       "         [ 2.8238e-03,  1.3523e-03,  2.7019e-03,  ..., -1.4857e-03,\n",
       "           4.0517e-03, -4.6059e-04],\n",
       "         [ 3.1712e-03,  1.3194e-04,  1.8287e-03,  ...,  8.0746e-04,\n",
       "          -1.1154e-03,  4.0771e-04],\n",
       "         [-3.0109e-03,  3.4992e-03,  1.0276e-04,  ..., -1.7868e-03,\n",
       "          -2.3115e-03,  5.4965e-04]], requires_grad=True),\n",
       " 'layers.31.self_attn.v_proj.weight': Parameter containing:\n",
       " tensor([[-7.8802e-04,  1.6784e-03,  4.0693e-04,  ..., -3.6786e-03,\n",
       "          -1.8389e-03,  2.0049e-03],\n",
       "         [-6.9000e-04, -7.3148e-04, -2.1319e-03,  ...,  3.6496e-05,\n",
       "           2.7987e-03, -2.3732e-03],\n",
       "         [-1.0444e-03,  3.3396e-03,  3.5557e-03,  ...,  1.4473e-03,\n",
       "          -2.5871e-04, -1.5023e-03],\n",
       "         ...,\n",
       "         [-1.3772e-03, -3.6772e-03,  1.7621e-03,  ...,  4.0805e-04,\n",
       "           3.0512e-03, -1.6980e-03],\n",
       "         [ 4.8788e-03, -1.4269e-03, -1.3839e-03,  ...,  1.7912e-03,\n",
       "          -1.6296e-03,  1.1215e-03],\n",
       "         [-1.2139e-03,  1.9303e-03,  7.4807e-04,  ...,  2.7399e-03,\n",
       "          -1.0283e-03, -1.8230e-03]], requires_grad=True),\n",
       " 'layers.31.self_attn.o_proj.weight': Parameter containing:\n",
       " tensor([[ 1.3477e-04, -4.6435e-03, -1.4592e-03,  ...,  7.6050e-04,\n",
       "           1.5031e-03, -3.7758e-03],\n",
       "         [ 3.6739e-03, -3.2922e-03, -1.3106e-03,  ...,  1.1575e-04,\n",
       "          -6.4738e-04,  1.0853e-03],\n",
       "         [-4.9473e-04,  3.0276e-04,  1.8909e-03,  ..., -1.4203e-03,\n",
       "           4.1921e-04,  2.2257e-03],\n",
       "         ...,\n",
       "         [-7.2917e-04,  5.3156e-04, -1.0453e-03,  ...,  2.4344e-03,\n",
       "          -2.4412e-04,  4.8940e-03],\n",
       "         [ 1.3385e-03,  8.2380e-05,  2.9128e-03,  ...,  9.3285e-04,\n",
       "           4.1275e-03, -1.0830e-03],\n",
       "         [ 2.6131e-03,  2.4726e-03, -2.5251e-04,  ..., -1.7452e-04,\n",
       "           6.0586e-04, -1.4832e-03]], requires_grad=True),\n",
       " 'layers.31.mlp.gate_proj.weight': Parameter containing:\n",
       " tensor([[-0.0066, -0.0007,  0.0018,  ...,  0.0021, -0.0022, -0.0054],\n",
       "         [-0.0009,  0.0002,  0.0003,  ..., -0.0027,  0.0021,  0.0007],\n",
       "         [-0.0034,  0.0003, -0.0010,  ..., -0.0024, -0.0047, -0.0012],\n",
       "         ...,\n",
       "         [ 0.0005, -0.0039,  0.0002,  ..., -0.0041,  0.0002, -0.0051],\n",
       "         [ 0.0018, -0.0019, -0.0017,  ...,  0.0012,  0.0022, -0.0055],\n",
       "         [ 0.0034,  0.0010,  0.0021,  ...,  0.0030,  0.0024,  0.0038]],\n",
       "        requires_grad=True),\n",
       " 'layers.31.mlp.up_proj.weight': Parameter containing:\n",
       " tensor([[ 3.2048e-03, -1.8086e-03, -5.8204e-04,  ...,  4.0733e-03,\n",
       "          -2.9951e-03,  1.5433e-03],\n",
       "         [-3.8915e-03,  4.9210e-04, -1.0395e-03,  ...,  6.4971e-04,\n",
       "          -3.5418e-03,  2.1478e-05],\n",
       "         [ 9.4340e-04,  3.7790e-04, -2.3446e-03,  ..., -2.8494e-03,\n",
       "          -2.8041e-03, -3.1950e-03],\n",
       "         ...,\n",
       "         [ 1.0518e-03,  1.3821e-03,  4.6136e-04,  ...,  2.5849e-03,\n",
       "           2.6420e-04, -2.7426e-03],\n",
       "         [ 1.7786e-03, -3.3229e-03,  5.1367e-04,  ...,  1.1242e-03,\n",
       "           2.8904e-04, -2.0223e-03],\n",
       "         [ 4.6281e-03,  1.2604e-03, -3.0812e-04,  ..., -1.7174e-03,\n",
       "           1.3146e-04, -3.1034e-03]], requires_grad=True),\n",
       " 'layers.31.mlp.down_proj.weight': Parameter containing:\n",
       " tensor([[ 0.0005, -0.0012,  0.0010,  ...,  0.0015,  0.0016,  0.0010],\n",
       "         [ 0.0025, -0.0018, -0.0016,  ...,  0.0029, -0.0025,  0.0014],\n",
       "         [ 0.0006, -0.0006,  0.0014,  ...,  0.0018,  0.0038,  0.0014],\n",
       "         ...,\n",
       "         [-0.0015, -0.0016,  0.0014,  ..., -0.0026,  0.0012, -0.0011],\n",
       "         [-0.0025,  0.0006, -0.0045,  ...,  0.0017,  0.0073, -0.0026],\n",
       "         [ 0.0037,  0.0006, -0.0006,  ..., -0.0024, -0.0011, -0.0025]],\n",
       "        requires_grad=True),\n",
       " 'layers.31.input_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'layers.31.post_attention_layernorm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " 'norm.weight': Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 225, with 8,309,964,800 parameters\n"
     ]
    }
   ],
   "source": [
    "num_decay_params = sum(p.numel() for p in decay_params)\n",
    "num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "time.time() - time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ADMIN\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "cache_dir = r'D:\\\\hugging-models\\\\llama3-meta-pragateesh'\n",
    "login(token='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')\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 35694]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hiii\", add_special_tokens=False)\n",
    "# tokenizer.decode([128000, 6151, 2967], skip_special_tokens= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi bro'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.encode(\"hi bro\")\n",
    "enc.decode([5303, 1379])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hiii'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Tokenizers import encoder, decoder \n",
    "decoder(encoder(\"Hiii\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8310231040\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 128256\n",
    "        self.dim = 4096\n",
    "        self.n_layers = 32\n",
    "        self.n_heads = 32\n",
    "        self.max_seq_len = 2048\n",
    "        self.norm_eps = 1e-6\n",
    "        self.hidden_dim = 14336\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = config.n_heads\n",
    "        self.n_local_heads = config.n_heads\n",
    "        self.n_local_kv_heads = self.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = config.dim // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            mask = torch.full((1, 1, config.max_seq_len, config.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            output = torch.matmul(scores, xv)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "        self.output.weight = self.embed_tokens.weight\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(config.dim // config.n_heads, config.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor = None):\n",
    "        batch_size, seqlen = tokens.shape\n",
    "        h = self.embed_tokens(tokens)\n",
    "        freqs_cos, freqs_sin = self.freqs_cos[:seqlen], self.freqs_sin[:seqlen]\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if targets is not None:\n",
    "            logits = output[:, :-1, :].contiguous()\n",
    "            targets = targets[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return output, loss\n",
    "        return output, None\n",
    "\n",
    "def compute_total_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "config = ModelConfig()\n",
    "model = LlamaModel(config)\n",
    "total_params = compute_total_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaForCausalLM(\n",
    "#   (model): LlamaModel(\n",
    "#     (embed_tokens): Embedding(128256, 4096)\n",
    "#     (layers): ModuleList(\n",
    "#       (0-31): 32 x LlamaDecoderLayer(\n",
    "#         (self_attn): LlamaSdpaAttention(\n",
    "#           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
    "#           (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
    "#           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (rotary_emb): LlamaRotaryEmbedding()\n",
    "#         )\n",
    "#         (mlp): LlamaMLP(\n",
    "#           (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
    "#           (act_fn): SiLU()\n",
    "#         )\n",
    "#         (input_layernorm): LlamaRMSNorm()\n",
    "#         (post_attention_layernorm): LlamaRMSNorm()\n",
    "#       )\n",
    "#     )\n",
    "#     (norm): LlamaRMSNorm()\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8835567616\n",
    "# LlamaForCausalLM(\n",
    "#   (model): LlamaModel(\n",
    "#     (embed_tokens): Embedding(128256, 4096)\n",
    "#     (layers): ModuleList(\n",
    "#       (0-31): 32 x LlamaDecoderLayer(\n",
    "#         (self_attn): LlamaSdpaAttention(\n",
    "#           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (rotary_emb): LlamaRotaryEmbedding()\n",
    "#         )\n",
    "#         (mlp): LlamaMLP(\n",
    "#           (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
    "#           (act_fn): SiLU()\n",
    "#         )\n",
    "#         (input_layernorm): LlamaRMSNorm()\n",
    "#         (post_attention_layernorm): LlamaRMSNorm()\n",
    "#       )\n",
    "#     )\n",
    "#     (norm): LlamaRMSNorm()\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import model \n",
    "Model, count = model.Model_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8835567616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ip address vanthu for a particular vm : 172.16.17.156\n",
    "# Password: Admin@123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "# device \n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model.to(device)\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ADMIN\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    " \n",
    "login(token='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')\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "cache_dir = r'D:\\\\hugging-models\\\\llama3-meta-pragateesh'\n",
    " \n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken \n",
    "enc = tiktoken.get_encoding(\"gpt2\") \n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "class Praga(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        a = torch.tensor(10)  # Convert integer to tensor\n",
    "        self.register_buffer('a', a)\n",
    "\n",
    "    def er(self):\n",
    "        print(self.a)\n",
    "\n",
    "Praga().er()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8835567616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 128256\n",
    "        self.dim = 4096\n",
    "        self.n_layers = 32\n",
    "        self.n_heads = 32\n",
    "        self.max_seq_len = 2048\n",
    "        self.norm_eps = 1e-6\n",
    "        self.hidden_dim = 14336\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\")):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1].unsqueeze(1).expand_as(logits)\n",
    "        logits = torch.where(logits < min_values, torch.full_like(logits, filter_value), logits)\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "    return logits\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = config.n_heads # 32 \n",
    "        self.n_local_heads = config.n_heads #32 \n",
    "        self.n_local_kv_heads = self.n_kv_heads # 32 \n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads #  1\n",
    "        self.head_dim = config.dim // config.n_heads # 128\n",
    "        self.q_proj = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n",
    "        # self.vocab_size = 128256\n",
    "        # self.dim = 4096\n",
    "        # self.n_layers = 32\n",
    "        # self.n_heads = 32\n",
    "        # self.max_seq_len = 2048\n",
    "        # self.norm_eps = 1e-6\n",
    "        # self.hidden_dim = 14336\n",
    "\n",
    "         \n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2) \n",
    "        output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "        self.freqs_cos, self.freqs_sin = precompute_freqs_cis(config.dim // 2, config.max_seq_len)\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        h = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, self.freqs_cos[:h.size(1)], self.freqs_sin[:h.size(1)])\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if targets is not None:\n",
    "            logits = output[:, :-1, :].contiguous()\n",
    "            targets = targets[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return output, loss\n",
    "        return output, None\n",
    "\n",
    "    def generate(self, input_ids, max_length, temperature=1.0, top_k=50, top_p=0.95):\n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            outputs, _ = self(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "            next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "def compute_total_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "config = ModelConfig()\n",
    "model = LlamaModel(config)\n",
    "total_params = compute_total_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
