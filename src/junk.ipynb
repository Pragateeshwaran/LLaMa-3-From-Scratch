{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem Statement: Design a NoSQL Data Model for “Railway Ticket Reservation System”\n",
    "\n",
    "### Step 1: Specify the Key Entities\n",
    "\n",
    "1. **Trains**\n",
    "2. **Stations**\n",
    "3. **Passengers**\n",
    "4. **Reservations**\n",
    "5. **Schedules**\n",
    "\n",
    "### Step 2: Mapping of Entities with Relationships\n",
    "\n",
    "- A train has multiple schedules (one-to-many).\n",
    "- A train has multiple reservations (one-to-many).\n",
    "- A station can have multiple trains passing through it (many-to-many).\n",
    "- A passenger can have multiple reservations (one-to-many).\n",
    "- Each reservation is linked to a passenger, a train, and a schedule.\n",
    "\n",
    "### Step 3: Draw ER- Diagram for Railway Reservation System\n",
    "\n",
    "Below is the textual representation of the ER diagram:\n",
    "\n",
    "```\n",
    "[Train] ---< [Schedule]\n",
    "    |\n",
    "    |---< [Reservation] >--- [Passenger]\n",
    "    |\n",
    "    |---< [Station] >---< [Train_Station]\n",
    "```\n",
    "\n",
    "### Step 4: Create JSON Document for Each Collection\n",
    "\n",
    "#### Collection 1: Train\n",
    "```json\n",
    "{\n",
    "    \"train_id\": \"T123\",\n",
    "    \"train_name\": \"Podhigai Express\",\n",
    "    \"train_type\": \"Express\",\n",
    "    \"capacity\": {\n",
    "        \"1AC\": 10,\n",
    "        \"2AC\": 20,\n",
    "        \"3AC\": 30,\n",
    "        \"Sleeper\": 100\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Collection 2: Station\n",
    "```json\n",
    "{\n",
    "    \"station_id\": \"STN01\",\n",
    "    \"station_name\": \"Chennai Central\",\n",
    "    \"location\": \"Chennai, Tamil Nadu\",\n",
    "    \"train_ids\": [\"T123\", \"T124\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Collection 3: Passenger\n",
    "```json\n",
    "{\n",
    "    \"passenger_id\": \"P123\",\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 30,\n",
    "    \"gender\": \"Male\",\n",
    "    \"contact\": {\n",
    "        \"phone\": \"1234567890\",\n",
    "        \"email\": \"johndoe@example.com\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Collection 4: Reservation\n",
    "```json\n",
    "{\n",
    "    \"reservation_id\": \"R123\",\n",
    "    \"train_id\": \"T123\",\n",
    "    \"passenger_id\": \"P123\",\n",
    "    \"schedule_id\": \"S123\",\n",
    "    \"class\": \"3AC\",\n",
    "    \"seat_number\": \"32\",\n",
    "    \"booking_date\": \"2024-07-01\",\n",
    "    \"journey_date\": \"2024-07-10\",\n",
    "    \"source_station\": \"Chennai Central\",\n",
    "    \"destination_station\": \"Madurai Junction\",\n",
    "    \"fare\": 1500\n",
    "}\n",
    "```\n",
    "\n",
    "#### Collection 5: Schedule\n",
    "```json\n",
    "{\n",
    "    \"schedule_id\": \"S123\",\n",
    "    \"train_id\": \"T123\",\n",
    "    \"departure_station\": \"Chennai Central\",\n",
    "    \"arrival_station\": \"Madurai Junction\",\n",
    "    \"departure_time\": \"2024-07-10T05:00:00\",\n",
    "    \"arrival_time\": \"2024-07-10T12:00:00\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 5: Insert at Least 10 Different Documents in Each Collection\n",
    "\n",
    "(Example: Insertions for Train collection)\n",
    "```json\n",
    "{\n",
    "    \"train_id\": \"T124\",\n",
    "    \"train_name\": \"Nellai Express\",\n",
    "    \"train_type\": \"Express\",\n",
    "    \"capacity\": {\n",
    "        \"1AC\": 10,\n",
    "        \"2AC\": 20,\n",
    "        \"3AC\": 30,\n",
    "        \"Sleeper\": 100\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 6: Evaluate the Data Model with the Following Queries\n",
    "\n",
    "a. **Display the schedule for train named “Podhigai Express”**\n",
    "```json\n",
    "db.schedules.find({\"train_id\": \"T123\"})\n",
    "```\n",
    "\n",
    "b. **Display the seat availability of seats in train “Nellai Express”**\n",
    "```json\n",
    "db.trains.find({\"train_id\": \"T124\"}, {\"capacity\": 1})\n",
    "```\n",
    "\n",
    "c. **Display the details of passengers booked under “3AC” in “Pandian Express”**\n",
    "```json\n",
    "db.reservations.find({\"train_id\": \"T125\", \"class\": \"3AC\"})\n",
    "```\n",
    "\n",
    "d. **Display all the reservations for the journey in “Podhigai Express”**\n",
    "```json\n",
    "db.reservations.find({\"train_id\": \"T123\"})\n",
    "```\n",
    "\n",
    "e. **Count the total number of stations available for station “Chennai”**\n",
    "```json\n",
    "db.stations.count({\"station_name\": \"Chennai Central\"})\n",
    "```\n",
    "\n",
    "f. **Count the total number of reservations of each train**\n",
    "```json\n",
    "db.reservations.aggregate([\n",
    "    { $group: { _id: \"$train_id\", total_reservations: { $sum: 1 } } }\n",
    "])\n",
    "```\n",
    "\n",
    "g. **Calculate the average age of passengers of each train**\n",
    "```json\n",
    "db.reservations.aggregate([\n",
    "    {\n",
    "        $lookup: {\n",
    "            from: \"passengers\",\n",
    "            localField: \"passenger_id\",\n",
    "            foreignField: \"passenger_id\",\n",
    "            as: \"passenger_details\"\n",
    "        }\n",
    "    },\n",
    "    { $unwind: \"$passenger_details\" },\n",
    "    {\n",
    "        $group: {\n",
    "            _id: \"$train_id\",\n",
    "            average_age: { $avg: \"$passenger_details.age\" }\n",
    "        }\n",
    "    }\n",
    "])\n",
    "```\n",
    "\n",
    "h. **Calculate the total revenue generated by each train**\n",
    "```json\n",
    "db.reservations.aggregate([\n",
    "    { $group: { _id: \"$train_id\", total_revenue: { $sum: \"$fare\" } } }\n",
    "])\n",
    "```\n",
    "\n",
    "i. **Count the total number of reservations of each station**\n",
    "```json\n",
    "db.reservations.aggregate([\n",
    "    { $group: { _id: \"$source_station\", total_reservations: { $sum: 1 } } }\n",
    "])\n",
    "```\n",
    "\n",
    "j. **Identify the top 5 stations with the greatest number of reservations**\n",
    "```json\n",
    "db.reservations.aggregate([\n",
    "    { $group: { _id: \"$source_station\", total_reservations: { $sum: 1 } } },\n",
    "    { $sort: { total_reservations: -1 } },\n",
    "    { $limit: 5 }\n",
    "])\n",
    "```\n",
    "\n",
    "### Result\n",
    "Thus, the NoSQL data model for a Railway Ticket Reservation System using a document-oriented database like MongoDB was designed and the output was verified successfully.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Sure, let's create 10 different documents for each collection specified. I'll provide example documents for each collection: Trains, Stations, Passengers, Reservations, and Schedules.\n",
    "\n",
    "### Trains Collection\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"train_id\": \"T123\",\n",
    "        \"train_name\": \"Podhigai Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 10,\n",
    "            \"2AC\": 20,\n",
    "            \"3AC\": 30,\n",
    "            \"Sleeper\": 100\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T124\",\n",
    "        \"train_name\": \"Nellai Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 12,\n",
    "            \"2AC\": 25,\n",
    "            \"3AC\": 35,\n",
    "            \"Sleeper\": 120\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T125\",\n",
    "        \"train_name\": \"Pandian Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 8,\n",
    "            \"2AC\": 18,\n",
    "            \"3AC\": 28,\n",
    "            \"Sleeper\": 90\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T126\",\n",
    "        \"train_name\": \"Vaigai Express\",\n",
    "        \"train_type\": \"Superfast\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 10,\n",
    "            \"2AC\": 20,\n",
    "            \"3AC\": 30,\n",
    "            \"Sleeper\": 100\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T127\",\n",
    "        \"train_name\": \"Cholan Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 9,\n",
    "            \"2AC\": 19,\n",
    "            \"3AC\": 29,\n",
    "            \"Sleeper\": 95\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T128\",\n",
    "        \"train_name\": \"Anandapuri Express\",\n",
    "        \"train_type\": \"Superfast\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 11,\n",
    "            \"2AC\": 21,\n",
    "            \"3AC\": 31,\n",
    "            \"Sleeper\": 105\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T129\",\n",
    "        \"train_name\": \"Palaruvi Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 10,\n",
    "            \"2AC\": 20,\n",
    "            \"3AC\": 30,\n",
    "            \"Sleeper\": 100\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T130\",\n",
    "        \"train_name\": \"Cheran Express\",\n",
    "        \"train_type\": \"Superfast\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 12,\n",
    "            \"2AC\": 22,\n",
    "            \"3AC\": 32,\n",
    "            \"Sleeper\": 110\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T131\",\n",
    "        \"train_name\": \"Tuticorin Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 8,\n",
    "            \"2AC\": 18,\n",
    "            \"3AC\": 28,\n",
    "            \"Sleeper\": 90\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"train_id\": \"T132\",\n",
    "        \"train_name\": \"Kanniyakumari Express\",\n",
    "        \"train_type\": \"Express\",\n",
    "        \"capacity\": {\n",
    "            \"1AC\": 10,\n",
    "            \"2AC\": 20,\n",
    "            \"3AC\": 30,\n",
    "            \"Sleeper\": 100\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Stations Collection\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"station_id\": \"STN01\",\n",
    "        \"station_name\": \"Chennai Central\",\n",
    "        \"location\": \"Chennai, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T123\", \"T124\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN02\",\n",
    "        \"station_name\": \"Madurai Junction\",\n",
    "        \"location\": \"Madurai, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T123\", \"T125\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN03\",\n",
    "        \"station_name\": \"Coimbatore Junction\",\n",
    "        \"location\": \"Coimbatore, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T126\", \"T130\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN04\",\n",
    "        \"station_name\": \"Tiruchirappalli Junction\",\n",
    "        \"location\": \"Tiruchirappalli, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T127\", \"T129\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN05\",\n",
    "        \"station_name\": \"Salem Junction\",\n",
    "        \"location\": \"Salem, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T128\", \"T131\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN06\",\n",
    "        \"station_name\": \"Erode Junction\",\n",
    "        \"location\": \"Erode, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T124\", \"T132\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN07\",\n",
    "        \"station_name\": \"Tirunelveli Junction\",\n",
    "        \"location\": \"Tirunelveli, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T125\", \"T127\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN08\",\n",
    "        \"station_name\": \"Dindigul Junction\",\n",
    "        \"location\": \"Dindigul, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T126\", \"T130\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN09\",\n",
    "        \"station_name\": \"Virudhunagar Junction\",\n",
    "        \"location\": \"Virudhunagar, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T128\", \"T129\"]\n",
    "    },\n",
    "    {\n",
    "        \"station_id\": \"STN10\",\n",
    "        \"station_name\": \"Kanniyakumari\",\n",
    "        \"location\": \"Kanniyakumari, Tamil Nadu\",\n",
    "        \"train_ids\": [\"T131\", \"T132\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Passengers Collection\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"passenger_id\": \"P123\",\n",
    "        \"name\": \"John Doe\",\n",
    "        \"age\": 30,\n",
    "        \"gender\": \"Male\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"1234567890\",\n",
    "            \"email\": \"johndoe@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P124\",\n",
    "        \"name\": \"Jane Smith\",\n",
    "        \"age\": 28,\n",
    "        \"gender\": \"Female\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"0987654321\",\n",
    "            \"email\": \"janesmith@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P125\",\n",
    "        \"name\": \"Robert Brown\",\n",
    "        \"age\": 45,\n",
    "        \"gender\": \"Male\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"1231231234\",\n",
    "            \"email\": \"robertbrown@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P126\",\n",
    "        \"name\": \"Emily Davis\",\n",
    "        \"age\": 35,\n",
    "        \"gender\": \"Female\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"3213213210\",\n",
    "            \"email\": \"emilydavis@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P127\",\n",
    "        \"name\": \"Michael Johnson\",\n",
    "        \"age\": 50,\n",
    "        \"gender\": \"Male\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"4564564567\",\n",
    "            \"email\": \"michaeljohnson@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P128\",\n",
    "        \"name\": \"Jessica Williams\",\n",
    "        \"age\": 32,\n",
    "        \"gender\": \"Female\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"6546546543\",\n",
    "            \"email\": \"jessicawilliams@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P129\",\n",
    "        \"name\": \"Daniel Miller\",\n",
    "        \"age\": 29,\n",
    "        \"gender\": \"Male\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"7897897890\",\n",
    "            \"email\": \"danielmiller@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P130\",\n",
    "        \"name\": \"Sarah Wilson\",\n",
    "        \"age\": 42,\n",
    "        \"gender\": \"Female\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"9879879876\",\n",
    "            \"email\": \"sarahwilson@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P131\",\n",
    "        \"name\": \"David Moore\",\n",
    "        \"age\": 27,\n",
    "        \"gender\": \"Male\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"1239876543\",\n",
    "            \"email\": \"davidmoore@example.com\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"passenger_id\": \"P132\",\n",
    "        \"name\": \"Sophia Taylor\",\n",
    "        \"age\": 37,\n",
    "        \"gender\": \"Female\",\n",
    "        \"contact\": {\n",
    "            \"phone\": \"3216549870\",\n",
    "            \"email\": \"sophiataylor@example.com\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Reservations Collection\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"reservation_id\": \"R123\",\n",
    "        \"train_id\": \"T123\",\n",
    "        \"passenger_id\": \"P123\",\n",
    "        \"schedule_id\": \"S123\",\n",
    "        \"class\": \"3AC\",\n",
    "        \"seat_number\": \"32\",\n",
    "        \"booking_date\": \"2024-07-01\",\n",
    "        \"journey_date\": \"2024-07-10\",\n",
    "        \"source_station\": \"Chennai Central\",\n",
    "        \"destination_station\": \"Mad\n",
    "\n",
    "urai Junction\",\n",
    "        \"fare\": 1500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R124\",\n",
    "        \"train_id\": \"T124\",\n",
    "        \"passenger_id\": \"P124\",\n",
    "        \"schedule_id\": \"S124\",\n",
    "        \"class\": \"2AC\",\n",
    "        \"seat_number\": \"12\",\n",
    "        \"booking_date\": \"2024-07-02\",\n",
    "        \"journey_date\": \"2024-07-11\",\n",
    "        \"source_station\": \"Salem Junction\",\n",
    "        \"destination_station\": \"Coimbatore Junction\",\n",
    "        \"fare\": 1200\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R125\",\n",
    "        \"train_id\": \"T125\",\n",
    "        \"passenger_id\": \"P125\",\n",
    "        \"schedule_id\": \"S125\",\n",
    "        \"class\": \"Sleeper\",\n",
    "        \"seat_number\": \"45\",\n",
    "        \"booking_date\": \"2024-07-03\",\n",
    "        \"journey_date\": \"2024-07-12\",\n",
    "        \"source_station\": \"Madurai Junction\",\n",
    "        \"destination_station\": \"Tiruchirappalli Junction\",\n",
    "        \"fare\": 500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R126\",\n",
    "        \"train_id\": \"T126\",\n",
    "        \"passenger_id\": \"P126\",\n",
    "        \"schedule_id\": \"S126\",\n",
    "        \"class\": \"1AC\",\n",
    "        \"seat_number\": \"05\",\n",
    "        \"booking_date\": \"2024-07-04\",\n",
    "        \"journey_date\": \"2024-07-13\",\n",
    "        \"source_station\": \"Chennai Central\",\n",
    "        \"destination_station\": \"Madurai Junction\",\n",
    "        \"fare\": 2500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R127\",\n",
    "        \"train_id\": \"T127\",\n",
    "        \"passenger_id\": \"P127\",\n",
    "        \"schedule_id\": \"S127\",\n",
    "        \"class\": \"3AC\",\n",
    "        \"seat_number\": \"22\",\n",
    "        \"booking_date\": \"2024-07-05\",\n",
    "        \"journey_date\": \"2024-07-14\",\n",
    "        \"source_station\": \"Coimbatore Junction\",\n",
    "        \"destination_station\": \"Madurai Junction\",\n",
    "        \"fare\": 1500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R128\",\n",
    "        \"train_id\": \"T128\",\n",
    "        \"passenger_id\": \"P128\",\n",
    "        \"schedule_id\": \"S128\",\n",
    "        \"class\": \"2AC\",\n",
    "        \"seat_number\": \"18\",\n",
    "        \"booking_date\": \"2024-07-06\",\n",
    "        \"journey_date\": \"2024-07-15\",\n",
    "        \"source_station\": \"Madurai Junction\",\n",
    "        \"destination_station\": \"Coimbatore Junction\",\n",
    "        \"fare\": 1200\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R129\",\n",
    "        \"train_id\": \"T129\",\n",
    "        \"passenger_id\": \"P129\",\n",
    "        \"schedule_id\": \"S129\",\n",
    "        \"class\": \"Sleeper\",\n",
    "        \"seat_number\": \"52\",\n",
    "        \"booking_date\": \"2024-07-07\",\n",
    "        \"journey_date\": \"2024-07-16\",\n",
    "        \"source_station\": \"Chennai Central\",\n",
    "        \"destination_station\": \"Madurai Junction\",\n",
    "        \"fare\": 500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R130\",\n",
    "        \"train_id\": \"T130\",\n",
    "        \"passenger_id\": \"P130\",\n",
    "        \"schedule_id\": \"S130\",\n",
    "        \"class\": \"1AC\",\n",
    "        \"seat_number\": \"07\",\n",
    "        \"booking_date\": \"2024-07-08\",\n",
    "        \"journey_date\": \"2024-07-17\",\n",
    "        \"source_station\": \"Chennai Central\",\n",
    "        \"destination_station\": \"Madurai Junction\",\n",
    "        \"fare\": 2500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R131\",\n",
    "        \"train_id\": \"T131\",\n",
    "        \"passenger_id\": \"P131\",\n",
    "        \"schedule_id\": \"S131\",\n",
    "        \"class\": \"3AC\",\n",
    "        \"seat_number\": \"28\",\n",
    "        \"booking_date\": \"2024-07-09\",\n",
    "        \"journey_date\": \"2024-07-18\",\n",
    "        \"source_station\": \"Coimbatore Junction\",\n",
    "        \"destination_station\": \"Madurai Junction\",\n",
    "        \"fare\": 1500\n",
    "    },\n",
    "    {\n",
    "        \"reservation_id\": \"R132\",\n",
    "        \"train_id\": \"T132\",\n",
    "        \"passenger_id\": \"P132\",\n",
    "        \"schedule_id\": \"S132\",\n",
    "        \"class\": \"2AC\",\n",
    "        \"seat_number\": \"20\",\n",
    "        \"booking_date\": \"2024-07-10\",\n",
    "        \"journey_date\": \"2024-07-19\",\n",
    "        \"source_station\": \"Madurai Junction\",\n",
    "        \"destination_station\": \"Coimbatore Junction\",\n",
    "        \"fare\": 1200\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Schedules Collection\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"schedule_id\": \"S123\",\n",
    "        \"train_id\": \"T123\",\n",
    "        \"departure_station\": \"Chennai Central\",\n",
    "        \"arrival_station\": \"Madurai Junction\",\n",
    "        \"departure_time\": \"2024-07-10T05:00:00\",\n",
    "        \"arrival_time\": \"2024-07-10T12:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S124\",\n",
    "        \"train_id\": \"T124\",\n",
    "        \"departure_station\": \"Salem Junction\",\n",
    "        \"arrival_station\": \"Coimbatore Junction\",\n",
    "        \"departure_time\": \"2024-07-11T06:00:00\",\n",
    "        \"arrival_time\": \"2024-07-11T08:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S125\",\n",
    "        \"train_id\": \"T125\",\n",
    "        \"departure_station\": \"Madurai Junction\",\n",
    "        \"arrival_station\": \"Tiruchirappalli Junction\",\n",
    "        \"departure_time\": \"2024-07-12T07:00:00\",\n",
    "        \"arrival_time\": \"2024-07-12T09:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S126\",\n",
    "        \"train_id\": \"T126\",\n",
    "        \"departure_station\": \"Chennai Central\",\n",
    "        \"arrival_station\": \"Madurai Junction\",\n",
    "        \"departure_time\": \"2024-07-13T05:00:00\",\n",
    "        \"arrival_time\": \"2024-07-13T12:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S127\",\n",
    "        \"train_id\": \"T127\",\n",
    "        \"departure_station\": \"Coimbatore Junction\",\n",
    "        \"arrival_station\": \"Madurai Junction\",\n",
    "        \"departure_time\": \"2024-07-14T06:00:00\",\n",
    "        \"arrival_time\": \"2024-07-14T13:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S128\",\n",
    "        \"train_id\": \"T128\",\n",
    "        \"departure_station\": \"Madurai Junction\",\n",
    "        \"arrival_station\": \"Coimbatore Junction\",\n",
    "        \"departure_time\": \"2024-07-15T07:00:00\",\n",
    "        \"arrival_time\": \"2024-07-15T14:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S129\",\n",
    "        \"train_id\": \"T129\",\n",
    "        \"departure_station\": \"Chennai Central\",\n",
    "        \"arrival_station\": \"Madurai Junction\",\n",
    "        \"departure_time\": \"2024-07-16T05:00:00\",\n",
    "        \"arrival_time\": \"2024-07-16T12:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S130\",\n",
    "        \"train_id\": \"T130\",\n",
    "        \"departure_station\": \"Chennai Central\",\n",
    "        \"arrival_station\": \"Madurai Junction\",\n",
    "        \"departure_time\": \"2024-07-17T05:00:00\",\n",
    "        \"arrival_time\": \"2024-07-17T12:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S131\",\n",
    "        \"train_id\": \"T131\",\n",
    "        \"departure_station\": \"Coimbatore Junction\",\n",
    "        \"arrival_station\": \"Madurai Junction\",\n",
    "        \"departure_time\": \"2024-07-18T06:00:00\",\n",
    "        \"arrival_time\": \"2024-07-18T13:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"schedule_id\": \"S132\",\n",
    "        \"train_id\": \"T132\",\n",
    "        \"departure_station\": \"Madurai Junction\",\n",
    "        \"arrival_station\": \"Coimbatore Junction\",\n",
    "        \"departure_time\": \"2024-07-19T07:00:00\",\n",
    "        \"arrival_time\": \"2024-07-19T14:00:00\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "With these documents in place, you can populate each collection in your NoSQL database to simulate the Railway Ticket Reservation System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8310231040\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 128256\n",
    "        self.dim = 4096\n",
    "        self.n_layers = 32\n",
    "        self.n_heads = 32\n",
    "        self.max_seq_len = 2048\n",
    "        self.norm_eps = 1e-6\n",
    "        self.hidden_dim = 14336\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = config.n_heads\n",
    "        self.n_local_heads = config.n_heads\n",
    "        self.n_local_kv_heads = self.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = config.dim // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            mask = torch.full((1, 1, config.max_seq_len, config.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            output = torch.matmul(scores, xv)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "        self.output.weight = self.embed_tokens.weight\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(config.dim // config.n_heads, config.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor = None):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.embed_tokens(tokens)\n",
    "        freqs_cos, freqs_sin = self.freqs_cos[:seqlen], self.freqs_sin[:seqlen]\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if targets is not None:\n",
    "            logits = output[:, :-1, :].contiguous()\n",
    "            targets = targets[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return output, loss\n",
    "        return output, None\n",
    "\n",
    "def compute_total_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "config = ModelConfig()\n",
    "model = LlamaModel(config)\n",
    "total_params = compute_total_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaForCausalLM(\n",
    "#   (model): LlamaModel(\n",
    "#     (embed_tokens): Embedding(128256, 4096)\n",
    "#     (layers): ModuleList(\n",
    "#       (0-31): 32 x LlamaDecoderLayer(\n",
    "#         (self_attn): LlamaSdpaAttention(\n",
    "#           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
    "#           (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
    "#           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (rotary_emb): LlamaRotaryEmbedding()\n",
    "#         )\n",
    "#         (mlp): LlamaMLP(\n",
    "#           (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
    "#           (act_fn): SiLU()\n",
    "#         )\n",
    "#         (input_layernorm): LlamaRMSNorm()\n",
    "#         (post_attention_layernorm): LlamaRMSNorm()\n",
    "#       )\n",
    "#     )\n",
    "#     (norm): LlamaRMSNorm()\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8835567616\n",
    "# LlamaForCausalLM(\n",
    "#   (model): LlamaModel(\n",
    "#     (embed_tokens): Embedding(128256, 4096)\n",
    "#     (layers): ModuleList(\n",
    "#       (0-31): 32 x LlamaDecoderLayer(\n",
    "#         (self_attn): LlamaSdpaAttention(\n",
    "#           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "#           (rotary_emb): LlamaRotaryEmbedding()\n",
    "#         )\n",
    "#         (mlp): LlamaMLP(\n",
    "#           (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "#           (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
    "#           (act_fn): SiLU()\n",
    "#         )\n",
    "#         (input_layernorm): LlamaRMSNorm()\n",
    "#         (post_attention_layernorm): LlamaRMSNorm()\n",
    "#       )\n",
    "#     )\n",
    "#     (norm): LlamaRMSNorm()\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import model \n",
    "Model, count = model.Model_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8835567616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ip address vanthu for a particular vm : 172.16.17.156\n",
    "# Password: Admin@123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "# device \n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model.to(device)\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ADMIN\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    " \n",
    "login(token='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')\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "cache_dir = r'D:\\\\hugging-models\\\\llama3-meta-pragateesh'\n",
    " \n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken \n",
    "enc = tiktoken.get_encoding(\"gpt2\") \n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "class Praga(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        a = torch.tensor(10)  # Convert integer to tensor\n",
    "        self.register_buffer('a', a)\n",
    "\n",
    "    def er(self):\n",
    "        print(self.a)\n",
    "\n",
    "Praga().er()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8835567616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 128256\n",
    "        self.dim = 4096\n",
    "        self.n_layers = 32\n",
    "        self.n_heads = 32\n",
    "        self.max_seq_len = 2048\n",
    "        self.norm_eps = 1e-6\n",
    "        self.hidden_dim = 14336\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\")):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1].unsqueeze(1).expand_as(logits)\n",
    "        logits = torch.where(logits < min_values, torch.full_like(logits, filter_value), logits)\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "    return logits\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = config.n_heads # 32 \n",
    "        self.n_local_heads = config.n_heads #32 \n",
    "        self.n_local_kv_heads = self.n_kv_heads # 32 \n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads #  1\n",
    "        self.head_dim = config.dim // config.n_heads # 128\n",
    "        self.q_proj = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n",
    "        # self.vocab_size = 128256\n",
    "        # self.dim = 4096\n",
    "        # self.n_layers = 32\n",
    "        # self.n_heads = 32\n",
    "        # self.max_seq_len = 2048\n",
    "        # self.norm_eps = 1e-6\n",
    "        # self.hidden_dim = 14336\n",
    "\n",
    "         \n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2) \n",
    "        output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "        self.freqs_cos, self.freqs_sin = precompute_freqs_cis(config.dim // 2, config.max_seq_len)\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        h = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, self.freqs_cos[:h.size(1)], self.freqs_sin[:h.size(1)])\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if targets is not None:\n",
    "            logits = output[:, :-1, :].contiguous()\n",
    "            targets = targets[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return output, loss\n",
    "        return output, None\n",
    "\n",
    "    def generate(self, input_ids, max_length, temperature=1.0, top_k=50, top_p=0.95):\n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            outputs, _ = self(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "            next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "def compute_total_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "config = ModelConfig()\n",
    "model = LlamaModel(config)\n",
    "total_params = compute_total_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
